"""
FastAPI with v6_rag integration
âœ… MVP1: ì‚¬ìš©ì ì…ë ¥ í¼ ìë™ ìƒì„±
"""

from fastapi import FastAPI, File, Form, UploadFile
from fastapi.responses import JSONResponse
from fastapi.concurrency import run_in_threadpool
from typing import List
from pathlib import Path

from v11_generator.ai_generator import generate_proposal

# ì„¤ì • import
from config import get_settings
import os

# v6_rag_real ëª¨ë“ˆ import (í”„ë¡œë•ì…˜ ì „ìš©)
from v6_rag_real import create_batch_graph

from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import openai
import json

# ì„¤ì • ë¡œë“œ
settings = get_settings()

class ChatRequest(BaseModel):
    userMessage: str
    userIdx: int | None = None
    projectIdx: int | None = None

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(
    title=settings.API_TITLE,
    version=settings.API_VERSION,
    description=settings.API_DESCRIPTION
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ê°œë°œìš©ìœ¼ë¡œ ëª¨ë“  ë„ë©”ì¸ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],  # OPTIONS, POST, GET ë“± ëª¨ë‘ í—ˆìš©
    allow_headers=["*"],  # ëª¨ë“  í—¤ë” í—ˆìš©
)


# ì•± ì‹œì‘ ì‹œ ê·¸ë˜í”„ í•œ ë²ˆë§Œ ìƒì„±
batch_app = create_batch_graph()

# ========================================
# API ì—”ë“œí¬ì¸íŠ¸
# ========================================
@app.post("/analyze")
async def analyze_documents(
    files: List[UploadFile] = File(...),
    folders: List[str] = Form(...),
    userid: str = Form(...),
    projectidx: int = Form(...)
):
    """
    âœ… MVP1: ê³µê³  + ì²¨ë¶€ì„œë¥˜ ë¶„ì„ ë° ì‚¬ìš©ì ì…ë ¥ í¼ ìë™ ìƒì„±

    ë””ë²„ê¹…: 422 ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ë°›ì€ íŒŒë¼ë¯¸í„°ë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥

    Backendì—ì„œ ë°›ëŠ” ë°ì´í„° êµ¬ì¡°:
    - files: ì—…ë¡œë“œëœ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ (UploadFile ê°ì²´, ì‹¤ì œ íŒŒì¼ ë°”ì´ë„ˆë¦¬ í¬í•¨)
    - folders: ê° íŒŒì¼ì´ ì†í•œ í´ë” ID ë¦¬ìŠ¤íŠ¸ (filesì™€ 1:1 ë§¤ì¹­)
    - userid: ì‚¬ìš©ì ID
    - projectidx: í”„ë¡œì íŠ¸ ID

    ì˜ˆì‹œ:
    files[0] = UploadFile("2024_ì‚¬ì—…ê³µê³ .pdf")  â†’ folders[0] = "1" (ê³µê³  í´ë”)
    files[1] = UploadFile("ë¶™ì„1_ì‹ ì²­ì„œ.hwp")   â†’ folders[1] = "2" (ì²¨ë¶€ì„œë¥˜ í´ë”)
    files[2] = UploadFile("ë¶™ì„2_ì–‘ì‹.xlsx")    â†’ folders[2] = "2" (ì²¨ë¶€ì„œë¥˜ í´ë”)

    Returns:
    - form_source: 'TEMPLATE' (ì²¨ë¶€ ì–‘ì‹) or 'TOC' (ê³µê³  ëª©ì°¨)
    - user_form: ì‚¬ìš©ì ì…ë ¥ í¼ ìŠ¤í‚¤ë§ˆ
    - documents: ë¶„ì„ëœ ë¬¸ì„œ ì •ë³´
    """
    try:
        # ========================================
        # 1ë‹¨ê³„: Backendì—ì„œ ë°›ì€ ë°ì´í„° ê²€ì¦
        # ========================================
        if len(files) != len(folders):
            raise ValueError(f"íŒŒì¼ ê°œìˆ˜({len(files)})ì™€ í´ë” ê°œìˆ˜({len(folders)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        print(f"ğŸ“¥ ìˆ˜ì‹  ë°ì´í„°: userid={userid}, projectidx={projectidx}")
        print(f"ğŸ“ íŒŒì¼ ê°œìˆ˜: {len(files)}ê°œ")

        # ========================================
        # 2ë‹¨ê³„: íŒŒì¼ ë°”ì´íŠ¸ ë³€í™˜ (ë””ìŠ¤í¬ ì €ì¥ ì—†ì´ ë©”ëª¨ë¦¬ì—ì„œ ì²˜ë¦¬)
        # ========================================
        # Backendê°€ ì´ë¯¸ íŒŒì¼ì„ ì €ì¥í–ˆìœ¼ë¯€ë¡œ, FastAPIëŠ” ì €ì¥í•˜ì§€ ì•Šê³ 
        # ë°”ì´íŠ¸ ë°ì´í„°ë§Œ ì¶”ì¶œí•˜ì—¬ LangGraphë¡œ ì „ë‹¬
        #
        # Backendê°€ ë³´ë‚¸ files[i]ì™€ folders[i]ëŠ” 1:1 ë§¤ì¹­ë¨
        # ì˜ˆì‹œ:
        #   files[0] = UploadFile("ê³µê³ .pdf")      folders[0] = "1"
        #   files[1] = UploadFile("ë¶™ì„1.hwp")     folders[1] = "2"
        #   files[2] = UploadFile("ë¶™ì„2.xlsx")    folders[2] = "2"

        saved_files = []
        for i, file in enumerate(files):
            folder_id = int(folders[i])  # "1" â†’ 1, "2" â†’ 2

            # UploadFile â†’ ë°”ì´íŠ¸ ë°ì´í„° ë³€í™˜ (ë””ìŠ¤í¬ ì €ì¥ ì•ˆí•¨!)
            file_bytes = await file.read()

            saved_files.append({
                "bytes": file_bytes,         # íŒŒì¼ ë°”ì´ë„ˆë¦¬ ë°ì´í„°
                "filename": file.filename,   # ì›ë³¸ íŒŒì¼ëª…
                "folder": folder_id          # 1=ê³µê³ , 2=ì²¨ë¶€ì„œë¥˜
            })

            # ë””ë²„ê¹… ë¡œê·¸
            folder_type = "ê³µê³ " if folder_id == 1 else "ì²¨ë¶€ì„œë¥˜"
            file_size_kb = len(file_bytes) / 1024
            print(f"  [{i}] {file.filename} â†’ í´ë” {folder_id} ({folder_type}) - {file_size_kb:.1f}KB")

        print(f"âœ… íŒŒì¼ ë³€í™˜ ì™„ë£Œ: {len(saved_files)}ê°œ")

        # ========================================
        # 3ë‹¨ê³„: AI ë¶„ì„ì„ ìœ„í•œ State ìƒì„±
        # ========================================
        # saved_files êµ¬ì¡°:
        # [
        #   {"bytes": b"PDF binary...", "filename": "ê³µê³ .pdf", "folder": 1},
        #   {"bytes": b"HWP binary...", "filename": "ë¶™ì„1.hwp", "folder": 2},
        #   {"bytes": b"XLSX binary...", "filename": "ë¶™ì„2.xlsx", "folder": 2}
        # ]

        state = {
            "files": saved_files,
            "user_id": userid,
            "project_idx": projectidx,
            "documents": [],
            "all_chunks": [],
            "all_embeddings": None,
            "embedding_model": None,
            "chroma_client": None,
            "chroma_collection": None,
            "vector_db_path": "",
            "extracted_features": [],
            # "cross_references": [],  # ğŸ”– MVP2ì—ì„œ ì¬êµ¬í˜„ ì˜ˆì • (í˜„ì¬ ë¯¸ì‚¬ìš©)
            "attachment_templates": [],  # âœ¨ MVP1
            "csv_paths": None,
            "oracle_ids": None,
            "response_data": {},
            "status": "initialized",
            "errors": []
        }

        # ========================================
        # 4ë‹¨ê³„: LangGraph AI ë¶„ì„ ì‹¤í–‰
        # ========================================
        # v6_ragì˜ batch_appì´ saved_filesë¥¼ ë¶„ì„í•˜ì—¬:
        # 1. folder=1 íŒŒì¼ë“¤ â†’ ê³µê³  ë¶„ì„ (TOC ì¶”ì¶œ)
        # 2. folder=2 íŒŒì¼ë“¤ â†’ ì²¨ë¶€ì„œë¥˜ ë¶„ì„ (ì–‘ì‹ ì¶”ì¶œ)
        # 3. ì‚¬ìš©ì ì…ë ¥ í¼ ìë™ ìƒì„±
        print(f"ğŸš€ LangGraph ë¶„ì„ ì‹œì‘: project_idx={projectidx}")
        result = await run_in_threadpool(batch_app.invoke, state)
        print(f"âœ… LangGraph ë¶„ì„ ì™„ë£Œ")

        # ========================================
        # 5ë‹¨ê³„ LLM í˜¸ì¶œ â†’ JSON Plan ìƒì„± [ë¶„ë¦¬í•¨]
        # ========================================
       
        # ========================================
        # 6ë‹¨ê³„: ë¶„ì„ ê²°ê³¼ ë°˜í™˜
        # ========================================
        return JSONResponse(
            status_code=200,
            content=result['response_data']
        )

    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={
                "status": "error",
                "message": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "detail": str(e)
            }
        )


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {"status": "ok", "message": "Alice Consultant API is running"}


@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": settings.API_TITLE,
        "version": settings.API_VERSION,
        "mvp1": "ì‚¬ìš©ì ì…ë ¥ í¼ ìë™ ìƒì„±",
        "endpoints": {
            "POST /analyze": "ê³µê³  ë° ì²¨ë¶€ì„œë¥˜ ë¶„ì„",
            "GET /health": "í—¬ìŠ¤ ì²´í¬"
        }
    }


@app.post("/chat")
async def chat(request: ChatRequest):
    try:
        print("ğŸ“¢ Chat ìš”ì²­ ìˆ˜ì‹ :", request.userMessage)
        print("ğŸ“¢ OpenAI í˜¸ì¶œ í‚¤:", os.getenv("OPENAI_API_KEY") is not None)

        # ë¶„ì„ ë‹¨ê³„ ì—†ì´ ë°”ë¡œ LLM í˜¸ì¶œ
        response_data = await generate_proposal(
            request.userMessage,
            request.userIdx,
            request.projectIdx,
            os.getenv("OPENAI_API_KEY")
        )


        return response_data

    except Exception as e:
        return {"error": str(e)}
    

# ========================================
# ì‹¤í–‰ (ê°œë°œìš©)
# ========================================
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app,
        host=settings.HOST,
        port=settings.PORT,
        reload=settings.RELOAD
    )
