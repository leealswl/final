# ì´ˆì•ˆì„ ì‘ì„±í•´ì£¼ëŠ” ì‘ê°€ í•¨ìˆ˜

from ..state_types import ProposalGenerationState
from typing import Dict, Any
# from langchain_core.prompts import PromptTemplate  # [ì£¼ì„ ì²˜ë¦¬]
# from langchain_openai import ChatOpenAI            # [ì£¼ì„ ì²˜ë¦¬]
import logging
# import json                                        # [ì£¼ì„ ì²˜ë¦¬]

# [ì£¼ì„ ì²˜ë¦¬] ê³ ê¸‰ ì¶”ë¡ ì„ ìœ„í•´ GPT-4o ì‚¬ìš© ê¶Œì¥
# LLM_CLIENT = ChatOpenAI(temperature=0.3, model="gpt-4o")

def generate_proposal_draft(state: ProposalGenerationState) -> Dict[str, Any]:
    """
    [ì‘ê°€ ë…¸ë“œ - ë¹„í™œì„±í™” ìƒíƒœ]
    í˜„ì¬ëŠ” ì´ˆì•ˆ ìƒì„± ë¡œì§ì„ ì£¼ì„ ì²˜ë¦¬í•˜ì—¬ ì‹¤í–‰ë˜ì§€ ì•Šë„ë¡ ë§‰ì•„ë‘ì—ˆìŠµë‹ˆë‹¤.
    í…ŒìŠ¤íŠ¸ ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ë”ë¯¸(Dummy) ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print("--- ë…¸ë“œ ì‹¤í–‰: generate_proposal_draft (í˜„ì¬ ë¹„í™œì„±í™”ë¨) ---")
    logging.info(f"ğŸ“ generate_draft ë…¸ë“œ ì‹¤í–‰ (Skipped)")
    
    # -------------------------------------------------------------------------
    # [ì£¼ì„ ì²˜ë¦¬ ì‹œì‘] - ë‚˜ì¤‘ì— í™œì„±í™” ì‹œ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.
    # -------------------------------------------------------------------------
    # # 1. ì…ë ¥ ë°ì´í„° ì¤€ë¹„
    # # (1) ëŒ€í™” ê¸°ë¡ (User Domain Knowledge)
    # collected_data = state.get("collected_data", "")
    # accumulated_data = state.get("accumulated_data", "")
    # full_user_context = f"{accumulated_data}\n{collected_data}"
    # if len(full_user_context) < 10:
    #     full_user_context = "ì‚¬ìš©ìë¡œë¶€í„° ìˆ˜ì§‘ëœ êµ¬ì²´ì ì¸ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì¼ë°˜ì ì¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."

    # # (2) ê³µê³ ë¬¸ ë¶„ì„ ë°ì´í„° (Guide & Strategy)
    # # state['fetched_context']ì— anal.json ë‚´ìš©ì´ ìˆë‹¤ê³  ê°€ì •
    # fetched_context = state.get("fetched_context", {})
    # # anal_guideê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¼ë©´ í˜„ì¬ ì±•í„°ì™€ ê´€ë ¨ëœ ì „ëµì„ ì°¾ì•„ì•¼ í•¨ (ì—¬ê¸°ì„œëŠ” ì „ì²´ë¥¼ ë¬¸ìì—´ë¡œ ìš”ì•½ ê°€ì •)
    # # ì‹¤ì œë¡œëŠ” anal.json êµ¬ì¡°ì— ë§ì¶° í•„í„°ë§ ë¡œì§ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # anal_guide_summary = "ê³µê³ ë¬¸ì—ì„œ ìš”êµ¬í•˜ëŠ” 'í˜ì‹ ì„±'ê³¼ 'ê¸€ë¡œë²Œ ì§„ì¶œ ê°€ëŠ¥ì„±'ì„ ê°•ì¡°í•´ì•¼ í•©ë‹ˆë‹¤." 
    # 
    # # (3) ì‘ì„± ëª©í‘œ (Current Chapter)
    # target_chapter = state.get("target_chapter", "ì „ì²´ ê¸°íšì„œ")
    # toc_structure = state.get("draft_toc_structure", [])
    # 
    # # í˜„ì¬ ì‘ì„±í•´ì•¼ í•  ì±•í„°ì˜ í•˜ìœ„ ëª©ì°¨ ìƒì„¸ ì •ë³´ êµ¬ì„±
    # current_toc_detail = ""
    # current_idx = state.get("current_chapter_index", 0)
    # if toc_structure and current_idx < len(toc_structure):
    #     section = toc_structure[current_idx]
    #     current_toc_detail = f"ì±•í„°ëª…: {section.get('title')}\nì„¤ëª…: {section.get('description')}"

    # # 2. proposal.py ìŠ¤íƒ€ì¼ì˜ ê°•ë ¥í•œ í”„ë¡¬í”„íŠ¸ ì •ì˜
    # SYSTEM_PROMPT = """
    # ë‹¹ì‹ ì€ ì •ë¶€ ì§€ì›ì‚¬ì—… ì œì•ˆì„œ(RFP) ì‘ì„± ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.
    # ì£¼ì–´ì§„ [ê³µê³ ë¬¸ ê°€ì´ë“œ], [ì‚¬ìš©ì ì¸í„°ë·° ë‚´ìš©], [ì‘ì„± ëª©í‘œ]ë¥¼ ì™„ë²½í•˜ê²Œ ìˆ™ì§€í•˜ê³ ,
    # í‰ê°€ìœ„ì›ì´ ë†’ì€ ì ìˆ˜ë¥¼ ì¤„ ìˆ˜ë°–ì— ì—†ëŠ” **ì „ë¬¸ì ì´ê³  ë…¼ë¦¬ì ì¸ ì œì•ˆì„œ ì´ˆì•ˆ**ì„ ì‘ì„±í•˜ì„¸ìš”.

    # <ì‘ì„± ì›ì¹™>
    # 1. **ë‘ê´„ì‹ ì‘ì„±**: í•µì‹¬ ì£¼ì¥ì„ ë¬¸ë‹¨ ì²˜ìŒì— ë°°ì¹˜í•˜ì‹­ì‹œì˜¤.
    # 2. **ê·¼ê±° ì œì‹œ**: ì‚¬ìš©ìì˜ ì¸í„°ë·° ë‚´ìš©ì— ìˆëŠ” êµ¬ì²´ì  ìˆ˜ì¹˜ë‚˜ ì‚¬ì‹¤ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì‹­ì‹œì˜¤.
    # 3. **ê°€ì´ë“œ ì¤€ìˆ˜**: ê³µê³ ë¬¸ ë¶„ì„ ì „ëµì—ì„œ ê°•ì¡°í•˜ëŠ” í‚¤ì›Œë“œ(ì˜ˆ: ê¸€ë¡œë²Œ, í˜ì‹  ë“±)ë¥¼ ë…¹ì—¬ë‚´ì‹­ì‹œì˜¤.
    # 4. **ëª…í™•í•œ ì–´ì¡°**: "~í•  ê²ƒì„", "~ë¡œ ì‚¬ë£Œë¨" ë³´ë‹¤ëŠ” "~í•¨", "~ë¥¼ ì¶”ì§„í•¨" ë“±ì˜ ê°œì¡°ì‹ í˜¹ì€ ëª…í™•í•œ í•´ìš”ì²´ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤. (Markdown í¬ë§· ì‚¬ìš©)
    # """

    # USER_PROMPT_TEMPLATE = """
    # ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **[{target_chapter}]** ì±•í„°ì˜ ì´ˆì•ˆì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.

    # ### 1. ê³µê³ ë¬¸ ë¶„ì„ ë° ì‘ì„± ì „ëµ (Guide)
    # {anal_guide_summary}

    # ### 2. ì‚¬ìš©ì ì¸í„°ë·° ë‚´ìš© (Domain Context)
    # {full_user_context}

    # ### 3. ì‘ì„± ëª©í‘œ (Target Section)
    # {current_toc_detail}

    # ---
    # **[ìš”ì²­ì‚¬í•­]**
    # - ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ì±•í„°ì— ë“¤ì–´ê°ˆ ë³¸ë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”.
    # - ì†Œì œëª©(##)ì„ ì ì ˆíˆ í™œìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”.
    # - ë‚´ìš©ì€ ë„ˆë¬´ ì§§ì§€ ì•Šê²Œ, ì „ë¬¸ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ í’ì„±í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
    # """

    # prompt = PromptTemplate(
    #     template=SYSTEM_PROMPT + "\n\n" + USER_PROMPT_TEMPLATE,
    #     input_variables=["target_chapter", "anal_guide_summary", "full_user_context", "current_toc_detail"]
    # )

    # # 3. LLM ì‹¤í–‰
    # chain = prompt | LLM_CLIENT
    # 
    # try:
    #     response = chain.invoke({
    #         "target_chapter": target_chapter,
    #         "anal_guide_summary": anal_guide_summary,
    #         "full_user_context": full_user_context,
    #         "current_toc_detail": current_toc_detail
    #     })
    #     draft_content = response.content.strip()
    #     print(f"âœ… [{target_chapter}] ì´ˆì•ˆ ìƒì„± ì™„ë£Œ")

    # except Exception as e:
    #     draft_content = f"âŒ ì´ˆì•ˆ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
    #     logging.error(f"GENERATE_DRAFT ì˜¤ë¥˜: {e}")
    # -------------------------------------------------------------------------
    # [ì£¼ì„ ì²˜ë¦¬ ë]
    # -------------------------------------------------------------------------

    # ì„ì‹œ ë°˜í™˜ê°’ (ì˜¤ë¥˜ ë°©ì§€ìš©)
    draft_content = "(í˜„ì¬ ì´ˆì•ˆ ìƒì„± ê¸°ëŠ¥ì€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.)"

    # 4. ìƒíƒœ ë°˜í™˜
    return {
        "current_draft": draft_content,
        "generated_text": draft_content,
        # ë‹¤ìŒ ìŠ¤í…ì€ ì‚¬ìš©ìê°€ ê²€í† í•˜ê±°ë‚˜, ë‹¤ìŒ ì±•í„°ë¡œ ë„˜ì–´ê°€ëŠ” ë¡œì§ìœ¼ë¡œ ì—°ê²°ë¨
        "next_step": "REVIEW_OR_NEXT" 
    }