from ..state_types import ProposalGenerationState
# from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
import logging
from typing import Dict, Any, Optional
import re
import json
from json_repair import repair_json
from pathlib import Path
import os

def get_json_file_path() -> Path:
    """
    JSON íŒŒì¼ ì €ì¥ ê²½ë¡œë¥¼ ë°˜í™˜
    Returns:
        Path: backend/uploads/admin/1/1/234.json ê²½ë¡œ (Spring Bootê°€ ì„œë¹™í•˜ëŠ” ê²½ë¡œ)
    """
    current_file = Path(__file__).resolve()
    # alice/fastAPI/src/v11_generator/nodes/generate_draft.py
    # â†’ alice/fastAPI/src/v11_generator/ â†’ alice/fastAPI/src/ â†’ alice/fastAPI/ â†’ alice/ â†’ final/ (í”„ë¡œì íŠ¸ ë£¨íŠ¸)
    project_root = current_file.parent.parent.parent.parent.parent.parent
    # Spring Bootê°€ ì„œë¹™í•˜ëŠ” backend/uploads/ ê²½ë¡œì— ì €ì¥
    save_dir = project_root / "backend" / "documents"
    return save_dir


def load_existing_json() -> Optional[Dict[str, Any]]:
    """
    ê¸°ì¡´ JSON íŒŒì¼ì„ ì½ì–´ì„œ ë°˜í™˜
    Phase 1: íŒŒì¼ ì½ê¸° ê¸°ëŠ¥
    
    Returns:
        Optional[Dict[str, Any]]: ProseMirror JSON êµ¬ì¡°, íŒŒì¼ì´ ì—†ìœ¼ë©´ None
    """
    file_path = get_json_file_path()
    
    try:
        if not file_path.exists():
            print(f"ğŸ“„ [Phase 1] íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file_path}")
            return None
        
        print(f"ğŸ“– [Phase 1] íŒŒì¼ ì½ê¸° ì‹œì‘: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = json.load(f)
        
        # JSON êµ¬ì¡° ê²€ì¦
        if not isinstance(content, dict):
            print(f"âš ï¸ [Phase 1] JSONì´ dict í˜•ì‹ì´ ì•„ë‹˜: {type(content)}")
            return None
        
        if content.get("type") != "doc":
            print(f"âš ï¸ [Phase 1] ProseMirror JSON í˜•ì‹ì´ ì•„ë‹˜: type={content.get('type')}")
            return None
        
        paragraph_count = len(content.get("content", []))
        print(f"âœ… [Phase 1] íŒŒì¼ ì½ê¸° ì™„ë£Œ: {paragraph_count}ê°œ paragraph")
        
        return content
        
    except json.JSONDecodeError as e:
        print(f"âŒ [Phase 1] JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None
    except Exception as e:
        print(f"âŒ [Phase 1] íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        import traceback
        print(f"ğŸ” [Phase 1] ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return None


def text_to_prosemirror_json(text: str) -> Dict[str, Any]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ProseMirror JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    í•œê¸€ ë¬¸ì„œ/DOCX ìŠ¤íƒ€ì¼: ê° ì¤„ì„ ë³„ë„ paragraphë¡œ, ë¹ˆ ì¤„ë„ ë¹ˆ paragraphë¡œ ì²˜ë¦¬
    """
    if not text or not text.strip():
        return {
            "type": "doc",
            "content": []
        }
    
    # í”„ë¡¬í”„íŠ¸ì—ì„œ ë°˜í™˜ëœ í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ ë³¸ë¬¸ë§Œ ì¶”ì¶œ
    # "----------------------------------------------------------------------" êµ¬ë¶„ì ì œê±°
    lines = text.split('\n')
    content_lines = []
    in_content = False
    
    for line in lines:
        line_stripped = line.strip()
        # êµ¬ë¶„ì ë¼ì¸ ë¬´ì‹œ
        if line_stripped.startswith('---') or line_stripped.startswith('==='):
            continue
        # <ì‘ì„±ëœ ì œì•ˆì„œ ë³¸ë¬¸> ê°™ì€ íƒœê·¸ ì œê±°
        if '<ì‘ì„±ëœ ì œì•ˆì„œ ë³¸ë¬¸>' in line_stripped or '</ì‘ì„±ëœ ì œì•ˆì„œ ë³¸ë¬¸>' in line_stripped:
            in_content = True
            continue
        # ëª¨ë“  ì¤„ì„ í¬í•¨ (ë¹ˆ ì¤„ë„ í¬í•¨)
        content_lines.append(line_stripped)
    
    # ê° ì¤„ì„ ë³„ë„ paragraphë¡œ ì²˜ë¦¬ (í•œê¸€ ë¬¸ì„œ ìŠ¤íƒ€ì¼)
    paragraphs = []
    
    for line in content_lines:
        # ë¹ˆ ì¤„ë„ ë¹ˆ paragraphë¡œ ì²˜ë¦¬
        if not line:
            paragraphs.append({
                "type": "paragraph",
                "attrs": {"textAlign": "left"},
                "content": [{"type": "text", "text": "â€‹"}]  # ë¹ˆ í…ìŠ¤íŠ¸ (zero-width space)
            })
        else:
            # ê° ì¤„ì„ ë³„ë„ paragraphë¡œ
            paragraphs.append({
                "type": "paragraph",
                "attrs": {"textAlign": "left"},
                "content": [{"type": "text", "text": line}]
            })
    
    # ë¹ˆ ê²½ìš° ê¸°ë³¸ paragraph ì¶”ê°€
    if not paragraphs:
        paragraphs.append({
            "type": "paragraph",
            "attrs": {"textAlign": None},
            "content": [{"type": "text", "text": ""}]
        })
    
    return {
        "type": "doc",
        "content": paragraphs
    }
import json
import os
from pathlib import Path


def _extract_relevant_guide(guide_data: dict, chapter_number: str, chapter_title: str) -> str:
    """
    guide_claude.jsonì—ì„œ ëª©ì°¨ ë²ˆí˜¸/ì œëª©ì— ë§ëŠ” ê°€ì´ë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        guide_data: ë¡œë“œëœ guide JSON ë°ì´í„°
        chapter_number: ëª©ì°¨ ë²ˆí˜¸ (ì˜ˆ: "1", "2", "2.1")
        chapter_title: ëª©ì°¨ ì œëª©

    Returns:
        í•´ë‹¹ ëª©ì°¨ì— ëŒ€í•œ ê°€ì´ë“œ í…ìŠ¤íŠ¸
    """
    try:
        # integrated_business_proposal_guideì—ì„œ ì„¹ì…˜ ì°¾ê¸°
        guide_root = guide_data.get("integrated_business_proposal_guide", {})

        # ëª©ì°¨ ë²ˆí˜¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¹ì…˜ í‚¤ ë§¤í•‘
        section_mapping = {
            "1": "section_01_basic_info",
            "2": "section_02_current_status",
            "3": "section_03_preparation_plan",
            "4": "section_04_goals_and_requirements",
            "5": "section_05_business_feasibility",
            "6": "section_06_budget",
            "7": "section_07_evaluation_criteria"
        }

        # ì£¼ìš” ì„¹ì…˜ ë²ˆí˜¸ ì¶”ì¶œ (2.1 -> 2)
        main_section_num = chapter_number.split('.')[0] if '.' in chapter_number else chapter_number
        section_key = section_mapping.get(main_section_num)

        if section_key and section_key in guide_root:
            section_data = guide_root[section_key]

            # ê°€ì´ë“œ í…ìŠ¤íŠ¸ êµ¬ì„±
            guide_text = f"## {section_data.get('section_name', '')}\n\n"

            # R&D ê³„íšì„œ ì°¸ì¡°
            if 'rd_plan_reference' in section_data:
                guide_text += f"**R&D ì°¸ì¡°**: {section_data['rd_plan_reference']}\n"

            # SW RFP ì°¸ì¡°
            if 'sw_rfp_reference' in section_data:
                guide_text += f"**SW RFP ì°¸ì¡°**: {section_data['sw_rfp_reference']}\n\n"

            # ì£¼ìš” í‚¤ì›Œë“œ
            if 'common_keywords' in section_data:
                guide_text += f"**í•µì‹¬ í‚¤ì›Œë“œ**: {', '.join(section_data['common_keywords'])}\n\n"

            # ë‚˜ë¨¸ì§€ ì„¹ì…˜ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì¶”ê°€
            guide_text += "### ìƒì„¸ ê°€ì´ë“œ\n"
            guide_text += json.dumps(section_data, ensure_ascii=False, indent=2)

            return guide_text
        else:
            # ë§¤ì¹­ë˜ëŠ” ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ì¼ë°˜ ì‘ì„± íŒ ë°˜í™˜
            tips = guide_root.get("writing_tips_and_warnings", {}).get("common_tips", [])
            if tips:
                return f"ì¼ë°˜ ì‘ì„± ì§€ì¹¨:\n" + "\n".join(f"- {item}" for item in tips)
            else:
                return "í•´ë‹¹ ëª©ì°¨ì— ëŒ€í•œ ê°€ì´ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    except Exception as e:
        print(f"âš ï¸ ê°€ì´ë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return "ê°€ì´ë“œ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


def generate_proposal_draft(state: ProposalGenerationState) -> ProposalGenerationState:
    import os
    """
    [ì‘ê°€ ë…¸ë“œ - ë¹„í™œì„±í™” ìƒíƒœ]
    í˜„ì¬ëŠ” ì´ˆì•ˆ ìƒì„± ë¡œì§ì„ ì£¼ì„ ì²˜ë¦¬í•˜ì—¬ ì‹¤í–‰ë˜ì§€ ì•Šë„ë¡ ë§‰ì•„ë‘ì—ˆìŠµë‹ˆë‹¤.
    í…ŒìŠ¤íŠ¸ ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ë”ë¯¸(Dummy) ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print("ğŸ” [ë””ë²„ê¹…] --- ë…¸ë“œ ì‹¤í–‰: generate_proposal_draft ---")
    print(f"ğŸ” [ë””ë²„ê¹…] state.keys(): {list(state.keys())}")
    print(f"ğŸ” [ë””ë²„ê¹…] collected_data ê¸¸ì´: {len(state.get('collected_data', ''))}")
    logging.info(f"ğŸ“ generate_draft ë…¸ë“œ ì‹¤í–‰")

    DRAFT_PROMPT = """
        ë‹¹ì‹ ì€ í•œêµ­ ì •ë¶€ RFP(ì œì•ˆìš”ì²­ì„œ)Â·ì…ì°°Â·ì§€ì›ì‚¬ì—… ì œì•ˆì„œ ì‘ì„± ì „ë¬¸ê°€ì´ë©°,
        ì‹¤ì œ í‰ê°€ ì‹¬ì‚¬ìœ„ì›ì´ ì½ëŠ” ìˆ˜ì¤€ìœ¼ë¡œ ê³µì‹ì ì´ê³  ì„¤ë“ë ¥ ìˆëŠ” ë¬¸ì²´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

        ======================================================================
        ğŸ“Œ <ì…ë ¥ ì •ë³´>
        1. ì‘ì„± ëŒ€ìƒ ëª©ì°¨ (Target Section)
        - "{target_chapter_info}"

        2. ê³µê³ ë¬¸ í•µì‹¬ ë¶„ì„ ìš”ì•½ (Key Guidelines Summary)
        - "{anal_guide_summary}"

        3. í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ì‚¬ìš©ì ì •ë³´ (Collected Data)
        - {collected_data}

        4. ìµœê·¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ (Recent Chat History)
        - {recent_history}

        5. ì œì•ˆì„œ ì‘ì„± ê°€ì´ë“œ (Writing Guide Reference)
        - {guide_reference}
        ======================================================================

        âœï¸ <ì‘ì„± ì§€ì¹¨>
        - ìœ„ ë‹¤ì„¯ ê°€ì§€ ì…ë ¥ ì •ë³´ë¥¼ ëª¨ë‘ ë°˜ì˜í•˜ì—¬ **ì •ë¶€ ì œì•ˆì„œ ê³µì‹ ë¬¸ì²´ë¡œ í•´ë‹¹ ëª©ì°¨ì˜ ì™„ì„±ëœ ë‹¨ë½**ì„ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
        - **ì œì•ˆì„œ ì‘ì„± ê°€ì´ë“œ**ì— ëª…ì‹œëœ í•´ë‹¹ ëª©ì°¨ì˜ ì‘ì„± ìš”ë ¹, í•µì‹¬ í¬ì¸íŠ¸, í•„ìˆ˜ í¬í•¨ ë‚´ìš©ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì‹­ì‹œì˜¤.
        - ê°€ì´ë“œì— ì œì‹œëœ í‘œ í˜•ì‹, ì¸¡ì • ë°©ë²•, ì •ëŸ‰ì  ì§€í‘œ, ì˜ˆì‹œ ë“±ì˜ ìš”êµ¬ì‚¬í•­ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì‹­ì‹œì˜¤.
        - ë¬¸ë‹¨ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ê°œì¡°ì‹ ë‚˜ì—´ì´ í•„ìš”í•œ ê²½ìš° ì ì ˆíˆ í˜¼í•©í•˜ì‹­ì‹œì˜¤.
        - ì‚¬ìš©ìê°€ ì œê³µí•œ ì •ë³´ê°€ ë¶ˆì¶©ë¶„í•œ ì˜ì—­ì´ ìˆì–´ë„ ì¶”ë¡  ê°€ëŠ¥í•œ ë²”ìœ„ ë‚´ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë³´ì™„í•˜ì‹­ì‹œì˜¤.
        - ë‹¨ìˆœ ìš”ì•½ì´ë‚˜ ë‚˜ì—´ì´ ì•„ë‹Œ **ë…¼ë¦¬ì  êµ¬ì¡°(ë°°ê²½ â†’ í•„ìš”ì„± â†’ ëª©ì  â†’ ê·¼ê±° â†’ ê¸°ëŒ€íš¨ê³¼ ë“±)**ë¡œ ì„¤ë“ë ¥ ìˆê²Œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
        - ê³µê³ ë¬¸ ìš”êµ¬ì‚¬í•­ê³¼ì˜ ì í•©ì„±ì„ ëª…í™•í•˜ê²Œ ë“œëŸ¬ë‚´ì‹­ì‹œì˜¤.
        - í‰ê°€ìœ„ì›ì´ ì½ì„ ë•Œ **ì‚¬ì—…ì˜ íƒ€ë‹¹ì„±, ì‹¤í˜„ ê°€ëŠ¥ì„±, ê³µê³µì„±, í˜ì‹ ì„±, ê¸°ëŒ€ ì„±ê³¼**ê°€ ê°•ì¡°ë˜ë„ë¡ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
        - 'ìš°ë¦¬ëŠ”', 'ì €í¬ëŠ”' ê°™ì€ í‘œí˜„ ëŒ€ì‹  **ê¸°ì—…ëª… ë˜ëŠ” ì‚¬ì—… ì£¼ì²´ë¥¼ 3ì¸ì¹­ìœ¼ë¡œ ê¸°ìˆ **í•˜ì‹­ì‹œì˜¤.

        ğŸ“Œ <ì¶œë ¥ í˜•ì‹>
        ì•„ë˜ ProseMirror JSON í˜•ì‹ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì—¬ ì¶œë ¥í•˜ì‹­ì‹œì˜¤. ì½”ë“œ ë¸”ë¡ ì—†ì´ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.

        âš ï¸ ì¤‘ìš”: ë°˜ë“œì‹œ **ì œëª©(heading)ì„ ì²« ë²ˆì§¸ ìš”ì†Œë¡œ** ìƒì„±í•˜ê³ , ê·¸ ë‹¤ìŒì— ë‚´ìš©(paragraph)ì„ ìƒì„±í•˜ì„¸ìš”.
        - ì œëª© í…ìŠ¤íŠ¸: "{chapter_title}" í˜•ì‹ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš” (ì˜ˆ: "1. ê¸°ì—…í˜„í™©")
        - ì œëª©ì˜ levelì€ 1ì„ ì‚¬ìš©í•˜ì„¸ìš”

        {{
          "type": "doc",
          "content": [
            {{
              "type": "heading",
              "attrs": {{
                "level": 1
              }},
              "content": [
                {{
                  "type": "text",
                  "text": "{chapter_title}"
                }}
              ]
            }},
            {{
              "type": "paragraph",
              "attrs": {{
                "textAlign": "left",
                "paragraphIndex": 0
              }},
              "content": [
                {{
                  "type": "text",
                  "text": "ì²« ë²ˆì§¸ ë¬¸ë‹¨ ë‚´ìš©"
                }}
              ]
            }},
            {{
              "type": "paragraph",
              "attrs": {{
                "textAlign": "left",
                "paragraphIndex": 1
              }},
              "content": [
                {{
                  "type": "text",
                  "text": "â€‹"
                }}
              ]
            }},
            {{
              "type": "paragraph",
              "attrs": {{
                "textAlign": "left",
                "paragraphIndex": 2
              }},
              "content": [
                {{
                  "type": "text",
                  "text": "ë‘ ë²ˆì§¸ ë¬¸ë‹¨ ë‚´ìš©"
                }}
              ]
            }}
          ]
        }}

        âš ï¸ ì¤‘ìš”:
        - **ë°˜ë“œì‹œ ì²« ë²ˆì§¸ ìš”ì†Œë¡œ headingì„ ìƒì„±í•˜ì„¸ìš”** (ì œëª© í…ìŠ¤íŠ¸: "{chapter_title}")
        - ê° paragraphëŠ” paragraphIndexë¥¼ 0ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ ë¶€ì—¬í•˜ì„¸ìš”
        - ë¹ˆ ì¤„ì€ text: "â€‹" (zero-width space)ë¡œ ì²˜ë¦¬í•˜ì„¸ìš”
        - ì½”ë“œ ë¸”ë¡ ë§ˆì»¤(```)ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
        - ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”
        - ê° ë¬¸ë‹¨ì€ ë³„ë„ì˜ paragraphë¡œ êµ¬ë¶„í•˜ì„¸ìš”
        """
    
    # 1. guide_claude.json ë¡œë“œ
    guide_data = {}
    guide_reference = "ê°€ì´ë“œ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    try:
        # í˜„ì¬ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ guide í´ë” ê²½ë¡œ ì°¾ê¸°
        current_dir = Path(__file__).parent
        guide_path = current_dir.parent / "guide" / "guide_claude.json"

        if guide_path.exists():
            with open(guide_path, 'r', encoding='utf-8') as f:
                guide_data = json.load(f)
                print(f"âœ… guide_claude.json ë¡œë“œ ì„±ê³µ: {guide_path}")
        else:
            print(f"âš ï¸ guide_claude.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {guide_path}")
    except Exception as e:
        print(f"âš ï¸ guide_claude.json ë¡œë“œ ì˜¤ë¥˜: {e}")

    # 2. í˜„ì¬ ëª©í‘œ ì„¹ì…˜ ì •ë³´ ì„¤ì • (history_checkerì˜ ê²°ì • ë°˜ì˜ ë¡œì§)
    collected_data = state.get("collected_data", "")
    # print('collected_data: ', collected_data)
    # print(f"--- ğŸ“Š ASSESS_INFO ìˆ˜ì‹  ë°ì´í„° ê¸¸ì´: {len(collected_data)}ì ---")

    toc_structure = state.get("draft_toc_structure", [])
    target_title = state.get("target_chapter", "")
    current_idx = state.get("current_chapter_index", 0)

    fetched_context = state.get("fetched_context", {})
    anal_guide_summary = str(fetched_context.get("anal_guide", "ì „ëµ ì •ë³´ ì—†ìŒ"))

    if toc_structure and current_idx < len(toc_structure):
        major_chapter_item = toc_structure[current_idx]
        major_chapter_number = major_chapter_item.get("number", "0")
        major_chapter_title = major_chapter_item.get("title", "ì œëª© ì—†ìŒ")

        # 2-1. LLM í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©ë  ì£¼ ì±•í„° ì •ë³´ êµ¬ì„±
        chapter_display = f"{major_chapter_item.get('number')} {major_chapter_item.get('title')}"
        target_info_full = f"[{chapter_display}]\nì„¤ëª…: {major_chapter_item.get('description')}"

        print('target_info_full: ', target_info_full)

        # 2-2. ê°€ì´ë“œì—ì„œ í•´ë‹¹ ëª©ì°¨ì— ë§ëŠ” ì„¹ì…˜ ì°¾ê¸°
        if guide_data:
            guide_reference = _extract_relevant_guide(guide_data, major_chapter_number, major_chapter_title)
            print(f"ğŸ“š ì¶”ì¶œëœ ê°€ì´ë“œ ê¸¸ì´: {len(guide_reference)}ì")

    msgs = state.get("messages", [])
    recent_history = ""
    if msgs:
        for msg in msgs:
            role = "ğŸ‘¤" if msg.get("role") == "user" else "ğŸ¤–"
            content = msg.get("content", "")
            recent_history += f"{role}: {content}\n"


    prompt = PromptTemplate.from_template(DRAFT_PROMPT)

    llm = None
    try:
        llm = ChatAnthropic(
            model="claude-sonnet-4-5-20250929",
            temperature=0,
            max_tokens=8000
        )
    except Exception as e:
        print(f"âš ï¸ LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")

    chain = prompt | llm | StrOutputParser()

    result = chain.invoke({
        'target_chapter_info': target_info_full,
        'chapter_title': chapter_display,  # ì œëª©ì„ ë³„ë„ë¡œ ì „ë‹¬
        'anal_guide_summary': anal_guide_summary,
        'collected_data': collected_data,
        'recent_history': recent_history,
        'guide_reference': guide_reference
        })
    
    # ë§Œì•½ accumulated_dataê°€ ë¬¸ìì—´ì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    accumulated_data = state.get('accumulated_data', [])
    if isinstance(accumulated_data, str):
        accumulated_data = [accumulated_data]

    accumulated_data.append(target_title)

    print('accumulated_data: ', accumulated_data)
    
    history = state.get("messages", [])
    history.append({"role": "assistant", "content": result})
    
    # LLMì´ ìƒì„±í•œ JSON íŒŒì‹± ë° íŒŒì¼ ì €ì¥
    completed_content = None
    try:
        print(f"ğŸ” [ë””ë²„ê¹…] JSON íŒŒì‹± ì‹œì‘, result ê¸¸ì´: {len(result) if result else 0}")
        
        # ì½”ë“œ ë¸”ë¡ ë§ˆì»¤ ì œê±° (```json ... ``` í˜•ì‹ì¼ ê²½ìš°)
        json_text = result.strip()
        if json_text.startswith('```'):
            lines = json_text.split('\n')
            # ì²« ì¤„ê³¼ ë§ˆì§€ë§‰ ì¤„ ì œê±° (```json, ```)
            json_text = '\n'.join(lines[1:-1]) if len(lines) > 2 and lines[-1].strip() == '```' else '\n'.join(lines[1:])
        
        # JSON íŒŒì‹±
        # completed_content = json.loads(json_text)
        try:
            completed_content = repair_json(json_text, return_objects=True)
        except Exception as e:
            print(f"JSON ë³µêµ¬ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë¡œê·¸ì— ë‚¨ê²¨ í™•ì¸ í•„ìš”
            print(json_text) 
            raise e
        print(f"âœ… JSON íŒŒì‹± ì™„ë£Œ: {len(completed_content.get('content', []))}ê°œ ë¬¸ë‹¨")
        
        # íŒŒì¼ ì €ì¥ ê²½ë¡œ ì„¤ì • (get_json_file_path í•¨ìˆ˜ ì¬ì‚¬ìš©)
        save_path = Path(get_json_file_path()) / str(state.get("user_id")) / str(state.get("project_idx")) / "ì´ˆì•ˆ.json"
        
        # ì ˆëŒ€ ê²½ë¡œ ëª…í™•íˆ ì¶œë ¥
        absolute_path = save_path.resolve()
        print(f"ğŸ’¾ [ì €ì¥ ì „] íŒŒì¼ ê²½ë¡œ (ìƒëŒ€): {save_path}")
        print(f"ğŸ’¾ [ì €ì¥ ì „] íŒŒì¼ ê²½ë¡œ (ì ˆëŒ€): {absolute_path}")
        print(f"ğŸ’¾ [ì €ì¥ ì „] íŒŒì¼ ì¡´ì¬: {save_path.exists()}")
        
        # ê¸°ì¡´ íŒŒì¼ ì½ê¸° (ìˆìœ¼ë©´)
        existing_content = None
        if save_path.exists():
            with open(save_path, 'r', encoding='utf-8') as f:
                existing_content = json.load(f)
            old_para_count = len(existing_content.get('content', []))
            print(f"ğŸ’¾ [ì €ì¥ ì „] ê¸°ì¡´ paragraph ê°œìˆ˜: {old_para_count} (ì¶”ê°€ ì˜ˆì •)")
        else:
            print(f"ğŸ’¾ [ì €ì¥ ì „] íŒŒì¼ ì—†ìŒ (ìƒˆë¡œ ìƒì„±)")
        
        # ê¸°ì¡´ contentê°€ ìˆìœ¼ë©´ ìƒˆ contentë¥¼ ë’¤ì— ì¶”ê°€, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if existing_content and existing_content.get('content'):
            # ê¸°ì¡´ contentì— ìƒˆ content ì¶”ê°€
            existing_content_list = existing_content.get('content', [])
            new_content_list = completed_content.get('content', [])
            
            # ê¸°ì¡´ paragraphì˜ ìµœëŒ€ paragraphIndex ì°¾ê¸°
            max_paragraph_index = -1
            for para in existing_content_list:
                if para.get('type') == 'paragraph' and para.get('attrs', {}).get('paragraphIndex') is not None:
                    max_paragraph_index = max(max_paragraph_index, para.get('attrs', {}).get('paragraphIndex', -1))
            
            # ìƒˆ paragraphë“¤ì˜ paragraphIndexë¥¼ ê¸°ì¡´ ì¸ë±ìŠ¤ ë‹¤ìŒë¶€í„° ì‹œì‘
            current_index = max_paragraph_index + 1
            for para in new_content_list:
                if para.get('type') == 'paragraph':
                    if 'attrs' not in para:
                        para['attrs'] = {}
                    para['attrs']['paragraphIndex'] = current_index
                    if 'textAlign' not in para['attrs']:
                        para['attrs']['textAlign'] = 'left'
                    current_index += 1
                elif para.get('type') == 'heading':
                    # headingì€ paragraphIndexê°€ í•„ìš” ì—†ì§€ë§Œ, attrsëŠ” ìœ ì§€
                    if 'attrs' not in para:
                        para['attrs'] = {}
            
            # ê¸°ì¡´ content ë’¤ì— ìƒˆ content ì¶”ê°€
            existing_content_list.extend(new_content_list)
            final_content = {
                "type": "doc",
                "content": existing_content_list
            }
            print(f"ğŸ’¾ [ì¶”ê°€ ëª¨ë“œ] ê¸°ì¡´ {len(existing_content_list) - len(new_content_list)}ê°œ + ìƒˆ {len(new_content_list)}ê°œ = ì´ {len(existing_content_list)}ê°œ")
        else:
            # ê¸°ì¡´ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            final_content = completed_content
            # ìƒˆ paragraphë“¤ì˜ paragraphIndexê°€ 0ë¶€í„° ì‹œì‘í•˜ë„ë¡ ë³´ì¥
            for i, para in enumerate(final_content.get('content', [])):
                if para.get('type') == 'paragraph':
                    if 'attrs' not in para:
                        para['attrs'] = {}
                    if 'paragraphIndex' not in para['attrs']:
                        para['attrs']['paragraphIndex'] = i
                    if 'textAlign' not in para['attrs']:
                        para['attrs']['textAlign'] = 'left'
            print(f"ğŸ’¾ [ìƒˆë¡œ ìƒì„±] paragraph ê°œìˆ˜: {len(final_content.get('content', []))}")
        
        # JSON íŒŒì¼ ì €ì¥ (ê¸°ì¡´ ë‚´ìš© + ìƒˆ ë‚´ìš©)
        print(f"ğŸ’¾ [ì €ì¥ ì‹œë„] ì´ paragraph ê°œìˆ˜: {len(final_content.get('content', []))}")
        if final_content.get('content'):
            first_para_text = final_content['content'][0].get('content', [{}])[0].get('text', '')[:50] if final_content['content'][0].get('content') else ''
            print(f"ğŸ’¾ [ì €ì¥ ì‹œë„] ì²« ë²ˆì§¸ paragraph í…ìŠ¤íŠ¸: {first_para_text}...")
        
        try:
            # íŒŒì¼ ì €ì¥ (ê¸°ì¡´ + ìƒˆ ë‚´ìš©)
            print(f"ğŸ’¾ [ì €ì¥ ì‹œì‘] ê²½ë¡œ: {absolute_path}")
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(final_content, f, ensure_ascii=False, indent=2)
                # íŒŒì¼ ë²„í¼ í”ŒëŸ¬ì‹œ ê°•ì œ
                f.flush()
                import os
                if hasattr(f, 'fileno'):
                    try:
                        os.fsync(f.fileno())
                    except:
                        pass
            print(f"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {absolute_path}")
        except Exception as save_error:
            print(f"âŒ [ì €ì¥ ì‹¤íŒ¨] ì˜¤ë¥˜: {save_error}")
            print(f"âŒ [ì €ì¥ ì‹¤íŒ¨] ê²½ë¡œ: {absolute_path}")
            import traceback
            print(f"ğŸ” [ì €ì¥ ì‹¤íŒ¨ ìƒì„¸] {traceback.format_exc()}")
            raise
        
        # ì €ì¥ í›„ íŒŒì¼ ë‚´ìš© ê²€ì¦
        import time
        time.sleep(0.1)  # íŒŒì¼ ì‹œìŠ¤í…œ ë™ê¸°í™” ëŒ€ê¸°
        
        if save_path.exists():
            with open(save_path, 'r', encoding='utf-8') as f:
                saved_content = json.load(f)
            saved_para_count = len(saved_content.get('content', []))
            saved_first_text = ''
            if saved_content.get('content') and saved_content['content'][0].get('content'):
                saved_first_text = saved_content['content'][0]['content'][0].get('text', '')[:50]
            
            print(f"âœ… [ì €ì¥ í›„ ê²€ì¦] íŒŒì¼ ê²½ë¡œ: {absolute_path}")
            print(f"âœ… [ì €ì¥ í›„ ê²€ì¦] íŒŒì¼ paragraph ê°œìˆ˜: {saved_para_count}")
            print(f"âœ… [ì €ì¥ í›„ ê²€ì¦] ì²« ë²ˆì§¸ paragraph í…ìŠ¤íŠ¸: {saved_first_text}...")
            print(f"âœ… [ì €ì¥ í›„ ê²€ì¦] íŒŒì¼ í¬ê¸°: {save_path.stat().st_size} bytes")
            0
            # ë‚´ìš© ì¼ì¹˜ í™•ì¸
            expected_count = len(final_content.get('content', []))
            if saved_para_count == expected_count:
                print(f"âœ… [ì €ì¥ í›„ ê²€ì¦] paragraph ê°œìˆ˜ ì¼ì¹˜! (ì˜ˆìƒ: {expected_count}ê°œ, ì‹¤ì œ: {saved_para_count}ê°œ)")
                print(f"âœ… [ì €ì¥ í›„ ê²€ì¦] âœ…âœ…âœ… íŒŒì¼ ì €ì¥ ì„±ê³µ! âœ…âœ…âœ…")
            else:
                print(f"âš ï¸ [ì €ì¥ í›„ ê²€ì¦] paragraph ê°œìˆ˜ ë¶ˆì¼ì¹˜: ì €ì¥ë¨={saved_para_count}, ì˜ˆìƒ={expected_count}")
        else:
            print(f"âŒ [ì €ì¥ í›„ ê²€ì¦] íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ! ì €ì¥ ì‹¤íŒ¨!")
            print(f"âŒ [ì €ì¥ í›„ ê²€ì¦] ì˜ˆìƒ ê²½ë¡œ: {absolute_path}")
        
        # completed_contentë¥¼ final_contentë¡œ ì—…ë°ì´íŠ¸ (ë°˜í™˜ê°’ì— ì‚¬ìš©)
        completed_content = final_content
        
        print(f"ğŸ” [ë””ë²„ê¹…] completedContent êµ¬ì¡°: {json.dumps(completed_content, ensure_ascii=False, indent=2)[:500]}...")
        
    except json.JSONDecodeError as e:
        print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        print(f"ğŸ” [ë””ë²„ê¹…] íŒŒì‹± ì‹¤íŒ¨í•œ í…ìŠ¤íŠ¸: {result[:500]}...")
        import traceback
        print(f"ğŸ” [ë””ë²„ê¹…] íŒŒì‹± ì‹¤íŒ¨ ìƒì„¸: {traceback.format_exc()}")
        completed_content = None
    except Exception as e:
        print(f"âš ï¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        import traceback
        print(f"ğŸ” [ë””ë²„ê¹…] ì €ì¥ ì‹¤íŒ¨ ìƒì„¸: {traceback.format_exc()}")
        # íŒŒì‹±ì€ ì„±ê³µí–ˆì§€ë§Œ ì €ì¥ ì‹¤íŒ¨í•œ ê²½ìš° completed_contentëŠ” ìœ ì§€
    
    # 4. ìƒíƒœ ë°˜í™˜
    # ë‹¤ìŒ ì±•í„° ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ëª©ì°¨ì˜ description í•„ë“œ ì‚¬ìš©)
    current_chapter_info = chapter_display if 'chapter_display' in locals() else "í˜„ì¬ ì„¹ì…˜"
    
    # ë‹¤ìŒ ì±•í„° ì •ë³´ ë° description ê°€ì ¸ì˜¤ê¸°
    next_chapter_info = ""
    next_chapter_description = ""
    
    if toc_structure and current_idx + 1 < len(toc_structure):
        next_chapter = toc_structure[current_idx + 1]
        next_chapter_number = next_chapter.get('number', '')
        next_chapter_title = next_chapter.get('title', '')
        next_chapter_description = next_chapter.get('description', '')
        next_chapter_info = f"{next_chapter_number} {next_chapter_title}"
    else:
        next_chapter_info = "ëª¨ë“  ì„¹ì…˜ ì‘ì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
        next_chapter_description = ""
    
    # ì½”ë©˜íŠ¸ ìƒì„± (ìƒì„± ì™„ë£Œ ë©”ì‹œì§€ + ë‹¤ìŒ ì±•í„° ì„¤ëª…)
    if next_chapter_description:
        comment = f"""âœ… '{current_chapter_info}' ì„¹ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

ğŸ“ ë‹¤ìŒ ë‹¨ê³„:
ë‹¤ìŒìœ¼ë¡œ ì‘ì„±í•  ì„¹ì…˜ì€ '{next_chapter_info}'ì…ë‹ˆë‹¤.
{next_chapter_description}

í•´ë‹¹ ì„¹ì…˜ì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ê³„ì† ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤."""
    else:
        comment = f"""âœ… '{current_chapter_info}' ì„¹ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

ğŸ“ ë‹¤ìŒ ë‹¨ê³„:
{next_chapter_info}"""
    
    return_value = {
        "current_query": comment,  # ì½”ë©˜íŠ¸ + ë‹¤ìŒ ì±•í„° description
        "completedContent": completed_content,
        "messages": history
    }
    print(f"ğŸ” [ë””ë²„ê¹…] generate_draft ë°˜í™˜ê°’ - current_query ì¡´ì¬: {return_value.get('current_query') is not None}")
    print(f"ğŸ” [ë””ë²„ê¹…] generate_draft ë°˜í™˜ê°’ - completedContent ì¡´ì¬: {return_value.get('completedContent') is not None}")
    print(f"ğŸ” [ë””ë²„ê¹…] generate_draft ë°˜í™˜ê°’ - keys: {list(return_value.keys())}")
    
    return return_value