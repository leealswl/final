# ìˆ˜ì§‘ëœ ì»¨í…ìŠ¤íŠ¸ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ ê²°í•©í•˜ì—¬ LLMì„ í†µí•´ ê¸°íšì„œ ì´ˆì•ˆ(ì„œë¡  ë“±)ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

# íŒŒì¼: v11_generator.py (ë˜ëŠ” ë…¸ë“œ êµ¬í˜„ íŒŒì¼)

from ..state_types import ProposalGenerationState
from typing import Dict, Any
from langchain_core.prompts import PromptTemplate
from langchain_community.chat_models import ChatOpenAI 
import logging

LLM_CLIENT = ChatOpenAI(temperature=0.3, model="gpt-4o") # LLM í´ë¼ì´ì–¸íŠ¸ ê°€ì •

def generate_proposal_draft(state: ProposalGenerationState) -> ProposalGenerationState:
    """
    ìˆ˜ì§‘ëœ ì •ë³´(collected_data)ì™€ ëª©ì°¨ êµ¬ì¡°/ë¶„ì„ ì „ëµì„ ê¸°ë°˜ìœ¼ë¡œ ê¸°íšì„œ ì´ˆì•ˆì„ ìƒì„±í•©ë‹ˆë‹¤.
    """

    print("ë…¸ë“œ ì‹¤í–‰: generate_proposal_draft")
    logging.info(f"ğŸ“ generate_draft ë…¸ë“œ ì‹¤í–‰ (ì‹œë„: {state.get('attempt_count', 0) + 1})")
    
    # --- 1. ìƒíƒœì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ ---
    
    # ğŸ“š ëª©ì°¨ êµ¬ì¡° (ì‘ì„±í•  ë‚´ìš©ì˜ ë¼ˆëŒ€)
    toc_structure = state.get("draft_toc_structure", [])

    print('toc_structure: ', toc_structure)
    toc_text = "\n".join([f"- {item.get('title', 'ì œëª© ì—†ìŒ')}: {item.get('description', 'ì„¤ëª… ì—†ìŒ')}" 
                          for item in toc_structure])
    
    print('toc_text: ', toc_text)

    # ğŸ’¡ ë¶„ì„ ì „ëµ (ì‘ì„± í†¤ ë° ê°•ì¡°ì )
    strategy = state.get("draft_strategy", "ëª…í™•í•˜ê³  ë…¼ë¦¬ì ì¸ í‘œì¤€ ë³´ê³ ì„œ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.")
    
    # ğŸ’¬ ìˆ˜ì§‘ëœ ì‚¬ìš©ì ì •ë³´ (ì´ˆì•ˆì„ ì±„ìš¸ ë°ì´í„°)
    collected_data = state.get("collected_data", "ì‚¬ìš©ìë¡œë¶€í„° ì¶©ë¶„í•œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    
    # --- 2. LLM í˜¸ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ---
    
    # ìˆ˜ì§‘ëœ ì •ë³´ê°€ 'collected_data'ì— ëª¨ë‘ ëˆ„ì ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
    DRAFT_PROMPT = f"""
    ë‹¹ì‹ ì€ ì „ë¬¸ ì œì•ˆì„œ ì‘ì„±ìì…ë‹ˆë‹¤. ë‹¤ìŒì˜ [ëª©í‘œ ëª©ì°¨], [ë¶„ì„ ì „ëµ], [ìˆ˜ì§‘ëœ ì •ë³´]ë¥¼ ì¢…í•©í•˜ì—¬
    ì™„ë²½í•˜ê²Œ ë¬¸ì¥ì´ ì—°ê²°ë˜ê³  íë¦„ì´ ìì—°ìŠ¤ëŸ¬ìš´ **ê¸°íšì„œ ì´ˆì•ˆ ì „ì²´ ë‚´ìš©**ì„ Markdown í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
    
    # ğŸ¯ ëª©í‘œ ëª©ì°¨ êµ¬ì¡°:
    {toc_text}
    
    # ğŸ’¡ ê³µê³ ë¬¸ ë¶„ì„ ì „ëµ:
    {strategy}
    
    # ğŸ’¬ ìˆ˜ì§‘ëœ ì‚¬ìš©ì ì •ë³´ (ì´ˆì•ˆ ì‘ì„±ì˜ ê·¼ê±°):
    ---
    {collected_data}
    ---
    
    [ìš”ì²­ ì‚¬í•­]
    1. ëª©ì°¨ ìˆœì„œëŒ€ë¡œ ë‚´ìš©ì„ ì‘ì„±í•˜ë˜, ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ê° ëª©ì°¨ì— ì ì ˆíˆ ë°°ì¹˜í•˜ê³  ìƒì„¸í•˜ê²Œ ì„œìˆ í•˜ì‹­ì‹œì˜¤.
    2. ìƒì„±ëœ ê²°ê³¼ë¬¼ì€ **ì´ˆì•ˆ ë‚´ìš© ìì²´**ë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    
    # 3. LLM Chain ì •ì˜ ë° ì‹¤í–‰
    prompt_template = PromptTemplate.from_template(DRAFT_PROMPT)
    chain = prompt_template | LLM_CLIENT
    
    try:
        draft_content = chain.invoke({}).content.strip()
    except Exception as e:
        draft_content = f"âŒ ì´ˆì•ˆ ìƒì„± ì¤‘ LLM í˜¸ì¶œ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logging.error(f"GENERATE_DRAFT LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}")

    # 4. ìƒíƒœ ì—…ë°ì´íŠ¸
    return {
        "current_draft": draft_content,
        "generated_text": draft_content, # generated_textëŠ” ì£¼ë¡œ ìµœì¢… ê²°ê³¼ë¬¼ í•„ë“œë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆìŒ
        "attempt_count": state.get('attempt_count', 0) + 1,
        # ë‹¤ìŒ ë‹¨ê³„ëŠ” ì´ˆì•ˆì„ ê²€í† í•˜ê³  ìˆ˜ì •í• ì§€ ê²°ì •í•˜ëŠ” ë…¸ë“œë¡œ ì´ë™
        "next_step": "REVIEW_AND_DECIDE" 
    }