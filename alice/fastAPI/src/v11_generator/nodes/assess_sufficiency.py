from typing import Dict, Any, List
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from ..state_types import ProposalGenerationState
from dotenv import load_dotenv
import json
import re

load_dotenv()

def assess_info(state: ProposalGenerationState) -> Dict[str, Any]:
    """
    [íŒì‚¬ ë…¸ë“œ]
    í˜„ì¬ ëª©í‘œ ì„¹ì…˜(ì˜ˆ: 1.1)ì˜ ì •ë³´ ì¶©ì¡±ë¥ (í•„ìš”ì •ë³´)ì„ í‰ê°€í•©ë‹ˆë‹¤.
    70ì  ì´ìƒì´ë©´ ë‹¤ìŒ ì„¹ì…˜ìœ¼ë¡œ ì§„í–‰í•˜ë„ë¡ Trueë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. (ì„¹ì…˜ ë‹¨ìœ„ ì§„í–‰ ë¡œì§)
    """
    print("--- ë…¸ë“œ ì‹¤í–‰: assess_sufficiency (Section Scoring) ---")

    # [ì´ˆê¸° ë°©ì–´ë²½ - LLM ì´ˆê¸°í™” ì—ëŸ¬ë§Œ ë°©ì–´]
    try:
        llm = ChatOpenAI(temperature=0, model="gpt-4o")
    except Exception:
        # LLM ì˜¤ë¥˜ ì‹œì—ë„ ì¼ë‹¨ ë‹¤ìŒ ë…¸ë“œë¡œ ì´ë™í•˜ì—¬ ì§ˆë¬¸ ìƒì„±ì€ ì‹œë„ (ë˜ëŠ” ê¸°ë³¸ê°’ ë°˜í™˜)
        return {"sufficiency": False, "completeness_score": 0, "grading_reason": "LLM ì´ˆê¸°í™” ì˜¤ë¥˜ë¡œ í‰ê°€ ë¶ˆê°€", "next_step": "GENERATE_QUERY"}

    # 1. ë°ì´í„° í™•ì¸ (ìˆ˜ì²©ì´ ë¹„ì—ˆëŠ”ì§€ í™•ì¸)
    collected_data = state.get("collected_data", "")
    print(f"--- ğŸ“Š ASSESS_INFO ìˆ˜ì‹  ë°ì´í„° ê¸¸ì´: {len(collected_data)}ì ---")

    if not collected_data.strip():
        return {
            "sufficiency": False, "completeness_score": 0, "grading_reason": "ì•„ì§ ìˆ˜ì§‘ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.",
            "missing_subsections": ["ê¸°ì´ˆ ì•„ì´ë””ì–´"], "next_step": "GENERATE_QUERY"
        }

    # 2. í˜„ì¬ ëª©í‘œ ì„¹ì…˜ ì •ë³´ ì„¤ì • (ë‹¨ì¼ í•­ëª© ì¶”ì¶œ)
    toc_structure = state.get("draft_toc_structure", [])
    current_idx = state.get("current_chapter_index", 0)
    
    # ëª©ì°¨ ëì— ë„ë‹¬í–ˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì€ ì¸ë±ìŠ¤ì¸ ê²½ìš° ì™„ë£Œ ì²˜ë¦¬
    if not toc_structure or current_idx >= len(toc_structure):
        return {"sufficiency": True, "completeness_score": 100, "grading_reason": "ëª¨ë“  ëª©ì°¨ í•­ëª© ì™„ë£Œ", "next_step": "FINISH"}

    current_section_item = toc_structure[current_idx]
    
    # í˜„ì¬ ëª©í‘œ ì„¹ì…˜ì˜ ì •ë³´ë§Œ ì¶”ì¶œ
    current_number = current_section_item.get("number", "0")
    current_title = current_section_item.get("title", "ì œëª© ì—†ìŒ")
    current_desc = current_section_item.get("description", "ì„¤ëª… ì—†ìŒ")

    # LLMì´ ì±„ì í•  ëª©í‘œ ëª©ë¡ (ë‹¨ì¼ í•­ëª© ìš”ì²­)
    scoring_targets = f"[{current_number} {current_title}] - {current_desc}" 

    # 4. ì „ëµ ê°€ì ¸ì˜¤ê¸°
    fetched_context = state.get("fetched_context", {})
    anal_guide = str(fetched_context.get("anal_guide", "íŠ¹ë³„í•œ ì „ëµ ì—†ìŒ."))

    # 5. [í•µì‹¬] ë‹¨ì¼ í•­ëª© ì±„ì  í”„ë¡¬í”„íŠ¸ (JSON Only, ì´ìŠ¤ì¼€ì´í”„ ì ìš©)
    JUDGE_PROMPT = """
    ë‹¹ì‹ ì€ ì •ë¶€ ì§€ì›ì‚¬ì—… ê¸°íšì„œì˜ **ê³µì •ì„± ë° í•„ìš”ì •ë³´ë¥¼ í‰ê°€í•˜ëŠ” ì „ë¬¸ ì‹¬ì‚¬ìœ„ì›**ì…ë‹ˆë‹¤.
    **[ìˆ˜ì§‘ëœ ì •ë³´]**ë¥¼ ë°”íƒ•ìœ¼ë¡œ **[ê³µê³ ë¬¸ ì „ëµ]**ê³¼ **[í˜„ì¬ ëª©í‘œ í•­ëª©]**ì„ ë¶„ì„í•˜ì—¬, **í•´ë‹¹ í•­ëª©ì˜ í•„ìš”ì •ë³´ ì¶©ì¡±ë¥ **ì„ **0~100ì **ìœ¼ë¡œ ì±„ì í•˜ì„¸ìš”.

    <í‰ê°€ ì„ë¬´ ë° ê¸°ì¤€>
    1. **í‰ê°€ ëŒ€ìƒ:** ì˜¤ì§ **[í˜„ì¬ ëª©í‘œ í•­ëª©]** í•˜ë‚˜ë¿ì…ë‹ˆë‹¤.
    2. **í•„ìš”ì •ë³´ (ì¶©ì¡±) ê¸°ì¤€:** ì ìˆ˜ **70ì  ì´ìƒ**ì¼ ë•Œ, **ë‹¤ìŒ í•­ëª©ìœ¼ë¡œ ì§„í–‰**í•  ìˆ˜ ìˆëŠ” ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆ˜ì§‘ëœ ê²ƒìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤. (ì§ˆë¬¸ì˜ ì •ë³´ëŸ‰ì„ í‰ê°€í•˜ì„¸ìš”.)
    3. **ì±„ì  ì›ì¹™:** ìƒíƒœì— ì €ì¥ëœ **ê¸°ì¡´ ì ìˆ˜ë³´ë‹¤ ë‚®ì€ ì ìˆ˜ë¥¼ ì ˆëŒ€ ì£¼ì§€ ë§ˆì„¸ìš”.**

    <ì±„ì  ê¸°ì¤€í‘œ 1: ê³µê³ ë¬¸ í•µì‹¬ ì „ëµ>
    {anal_guide}
    (ì´ ì „ëµì— ëª…ì‹œëœ í‚¤ì›Œë“œ(ì˜ˆ: ê¸€ë¡œë²Œ, AI ê¸°ìˆ  ë“±)ê°€ ë‹µë³€ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.)

    <í˜„ì¬ ëª©í‘œ í•­ëª© (ë‹¨ì¼ í‰ê°€ ëŒ€ìƒ)> 
    {scoring_targets} 
    
    <ìˆ˜ì§‘ëœ ì •ë³´ (í˜„ì¬ ì„¹ì…˜ ê´€ë ¨ ëˆ„ì )>
    {collected_data}

    <ì¶œë ¥ í˜•ì‹ (JSON Only)>
    **ìš”êµ¬ì‚¬í•­**: ë°˜ë“œì‹œ [JSON ê°ì²´] í˜•íƒœë¡œ ì‘ë‹µí•˜ì„¸ìš”.
    {{
        "number": "{current_number}",
        "title": "{current_title}",
        "score": (0~100ì ),
        "reason": "í•„ìš”ì •ë³´ë¥¼ ì¶©ì¡±í•˜ëŠ”ì§€ì— ëŒ€í•œ êµ¬ì²´ì ì¸ í‰ê°€ ì‚¬ìœ ",
        "missing_points": ["ë¶€ì¡±í•œ êµ¬ì²´ì  ì •ë³´ ëª©ë¡ (ì˜ˆ: ì •ëŸ‰ì  ëª©í‘œ ìˆ˜ì¹˜)"] 
    }}
    """
    
    # 6. LLM í˜¸ì¶œ ë° JSON íŒŒì‹±
    
    # [ë³€ìˆ˜ ì„¤ëª…] LLM í˜¸ì¶œ ë° íŒŒì‹±ì— í•„ìš”í•œ ë³€ìˆ˜ë“¤
    prompt = PromptTemplate.from_template(JUDGE_PROMPT)
    chain = prompt | llm
    
    response_text = chain.invoke({
        "anal_guide": anal_guide,
        "scoring_targets": scoring_targets,
        "collected_data": collected_data,
        # ğŸ”‘ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì‚¬ìš©ëœ ëª¨ë“  ë³€ìˆ˜ë¥¼ ì „ë‹¬ (ì˜¤ë¥˜ í•´ê²°)
        "current_number": current_number, 
        "current_title": current_title 
    }).content.strip()

    # 7. LLM ê²°ê³¼ í†µí•© ë° ë¶„ì„ (ë‹¨ì¼ ê°ì²´ íŒŒì‹±)
    cleaned_json = re.sub(r"```json|```", "", response_text, flags=re.DOTALL).strip()
    
    try:
        parsed_score: Dict[str, Any] = json.loads(cleaned_json) 
    except json.JSONDecodeError as e:
        # LLMì´ JSON í˜•ì‹ì„ ì§€í‚¤ì§€ ì•Šì•˜ì„ ë•Œ
        print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}. Raw Text: {cleaned_json[:200]}")
        return {
            "sufficiency": False, 
            "completeness_score": 0, 
            "grading_reason": "í‰ê°€ ì‹œìŠ¤í…œ ì˜¤ë¥˜ (LLMì´ JSON í˜•ì‹ì„ ì§€í‚¤ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.)", 
            "missing_subsections": ["ì‹œìŠ¤í…œ ì˜¤ë¥˜"], 
            "next_step": "GENERATE_QUERY"
        }
    
    # 8. ë‹¨ì¼ í•­ëª© ì ìˆ˜ ê³„ì‚° ë° ëˆ„ì 
    num = parsed_score.get("number", current_number)
    score = parsed_score.get("score", 0)
    missing = parsed_score.get("missing_points", [])

    final_section_scores = state.get("section_scores", {})
    previous_score = final_section_scores.get(num, 0)
    
    # ì ìˆ˜ í•˜ë½ ë°©ì§€ (ê¸°ì¡´ ì ìˆ˜ì™€ ìƒˆ ì ìˆ˜ ì¤‘ ë†’ì€ ê°’ ì„ íƒ)
    final_score_for_item = max(previous_score, score)
    final_section_scores[num] = final_score_for_item
    
    # 9. [í•µì‹¬] ì§„í–‰ ì—¬ë¶€ íŒë‹¨ (ë‹¨ì¼ ì„¹ì…˜ 70ì  ê¸°ì¤€)
    is_sufficient = final_score_for_item >= 70
    
    # 10. ë°˜í™˜ê°’ êµ¬ì„±
    representative_score = final_score_for_item
    representative_reason = parsed_score.get("reason", "í‰ê°€ ì‚¬ìœ  ëˆ„ë½")
        
    print(f"âœ… í‰ê°€ ì™„ë£Œ: [{current_number} {current_title}] í•„ìš”ì •ë³´: {representative_score}%")

    return {
        "sufficiency": is_sufficient,
        "completeness_score": representative_score, # ì´ì œ ë‹¨ì¼ ì„¹ì…˜ ì ìˆ˜
        "grading_reason": representative_reason,
        "missing_subsections": list(set(missing)), # ì¬ì§ˆë¬¸ì„ ìœ„í•´ ë¶€ì¡± í•­ëª© ë°˜í™˜
        "section_scores": final_section_scores,
        "next_step": "GENERATE_QUERY"
    }