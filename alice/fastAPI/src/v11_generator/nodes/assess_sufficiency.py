from typing import Dict, Any
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from ..state_types import ProposalGenerationState
from dotenv import load_dotenv
import json
import re

load_dotenv()

def assess_info(state: ProposalGenerationState) -> Dict[str, Any]:
    """
    [íŒì‚¬ ë…¸ë“œ - ì ìˆ˜ ëˆ„ì  ë³´ì¥ ëª¨ë“œ]
    ì •ë³´ê°€ ëˆ„ì ë¨ì— ë”°ë¼ ì ìˆ˜ê°€ 'ìœ ì§€'ë˜ê±°ë‚˜ 'ìƒìŠ¹'í•  ë¿, ì ˆëŒ€ ë–¨ì–´ì§€ì§€ ì•Šë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.
    """
    print("--- ë…¸ë“œ ì‹¤í–‰: assess_sufficiency (Accumulative Scoring) ---")

    try:
        llm = ChatOpenAI(temperature=0, model="gpt-4o")
    except Exception:
        return {"sufficiency": False, "completeness_score": 0, "next_step": "GENERATE_QUERY"}

    # 1. ë°ì´í„° í™•ì¸
    collected_data = state.get("collected_data", "")
    if not collected_data.strip():
        return {
            "sufficiency": False, 
            "completeness_score": 0,
            "grading_reason": "ì•„ì§ ìˆ˜ì§‘ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.", # ğŸ‘ˆ ì´ìœ  ì €ì¥
            "missing_subsections": ["ê¸°ì´ˆ ì•„ì´ë””ì–´"],
            "next_step": "GENERATE_QUERY"
        }
    
    # 2. ëª©í‘œ ì±•í„° ì„¤ì •
    toc_structure = state.get("draft_toc_structure", [])
    current_idx = state.get("current_chapter_index", 0)
    
    target_number = "" 
    target_title = "ì œëª© ì—†ìŒ" # ë¹ˆ í•„ë“œ
    target_desc = ""
    
    # ë°ì´í„°ê°€ ì •ìƒì´ë©´ ë¹ˆí•„ë“œì— ì§„ì§œë‚´ìš© ì±„ì›Œë„£ìŒ
    if toc_structure and current_idx < len(toc_structure):
            item = toc_structure[current_idx]
            target_number = item.get('number', '')
            target_title = item.get('title', '')
            target_desc = item.get('description', '')

    # íŒì‚¬ì—ê²Œ "ë„ˆëŠ” ì§€ê¸ˆ íƒ€ê²Ÿë§Œ ì±„ì í•´ì•¼ í•´"ë¼ê³  ì•Œë ¤ì¤Œ
    target_info = f"[{target_number}] {target_title}\n(ì„¤ëª…: {target_desc})"

    # 3. ì „ëµ ê°€ì ¸ì˜¤ê¸°
    fetched_context = state.get("fetched_context", {})
    anal_guide = str(fetched_context.get("anal_guide", 
        "í˜„ì¬ëŠ” íŠ¹ë³„í•œ ì „ëµ ì—†ìŒ. ì¼ë°˜ì ì¸ ë…¼ë¦¬ì  ì™„ê²°ì„±ë§Œ í‰ê°€í•  ê²ƒ."))

    # 4. ì±„ì  í”„ë¡¬í”„íŠ¸
    JUDGE_PROMPT = """
    ë‹¹ì‹ ì€ ê¸°íšì„œ í‰ê°€ ìœ„ì›ì…ë‹ˆë‹¤.
    **ëˆ„ì ëœ ì •ë³´**ê°€ **[ê³µê³ ë¬¸ ì „ëµ]**ê³¼ **[ëª©í‘œ ì±•í„°]**ë¥¼ ì–¼ë§ˆë‚˜ ì¶©ì¡±í•˜ëŠ”ì§€ **0~100ì **ìœ¼ë¡œ ì±„ì í•˜ì„¸ìš”.

    <í˜„ì¬ ëª©í‘œ ì±•í„° (ì±„ì  ê¸°ì¤€ì´ ë˜ëŠ”)>
    {target_info}

    <ì±„ì  ê¸°ì¤€í‘œ 1: ê³µê³ ë¬¸ ì „ëµ>
    {anal_guide}

    <ì±„ì  ê¸°ì¤€í‘œ 2: ëª©í‘œ ì±•í„°>
    {target_info}

    <ìˆ˜ì§‘ëœ ì •ë³´ (ëˆ„ì )>
    {collected_data}

    <ì±„ì  ê°€ì´ë“œ>
    - ì •ë³´ê°€ ì¶”ê°€ë˜ì—ˆë”ë¼ë„ í•µì‹¬ ì „ëµê³¼ ë¬´ê´€í•˜ë©´ ì ìˆ˜ë¥¼ ì˜¬ë¦¬ì§€ ë§ˆì„¸ìš”. (ìœ ì§€)
    - ê¸°ì¡´ì— ì¶©ì¡±ëœ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì ìˆ˜ë¥¼ ê¹ì§€ ë§ˆì„¸ìš”. (ëˆ„ì  í‰ê°€)
    - 0~100ì  ì‚¬ì´ ì ìˆ˜ ë¶€ì—¬.

    <ì¶œë ¥ í˜•ì‹ (JSON Only)>
    {{
        "score": 60,
        "reason": "ê¸°ë³¸ ë‚´ìš©ì€ ì¶©ì¡±ë˜ë‚˜, ì „ëµì—ì„œ ìš”êµ¬í•˜ëŠ” 'ê¸€ë¡œë²Œ ì§„ì¶œ' ë‚´ìš©ì´ ì—†ìŒ.",
        "missing_points": ["ê¸€ë¡œë²Œ ì§„ì¶œ ì „ëµ"]
    }}
    """
    try:
            prompt = PromptTemplate.from_template(JUDGE_PROMPT)
            chain = prompt | llm
            response_text = chain.invoke({
                "anal_guide": anal_guide,
                "target_info": target_info,
                "collected_data": collected_data
            }).content.strip()

            cleaned_json = re.sub(r"```json|```", "", response_text).strip()
            result_json = json.loads(cleaned_json)
            
            # 1) LLMì´ ê³„ì‚°í•œ ì´ë²ˆ í„´ ì ìˆ˜
            calculated_score = result_json.get("score", 0)
            reason = result_json.get("reason", "í‰ê°€ ë‚´ìš© ì—†ìŒ")
            missing_points = result_json.get("missing_points", [])

            # 2) ì ìˆ˜ í•˜ë½ ë°©ì§€ ë¡œì§ (Max Logic)
            # -------------------------------------------------------
            previous_score = state.get("completeness_score", 0)
            # ì´ì „ ì ìˆ˜ì™€ ì´ë²ˆ ì ìˆ˜ ì¤‘ ë†’ì€ ê±¸ ì„ íƒ
            final_score = max(previous_score, calculated_score)
            
            # ì ìˆ˜ê°€ ë–¨ì–´ì§ˆ ë»”í–ˆë‹¤ë©´ ë¡œê·¸ë¡œ ì•Œë ¤ì¤Œ (ë””ë²„ê¹…ìš©)
            if calculated_score < previous_score:
                print(f"ğŸ“‰ ì ìˆ˜ ë°©ì–´ ë°œë™! (LLMíŒì •: {calculated_score} -> ìœ ì§€: {final_score})")
            # -------------------------------------------------------

            print(f"ğŸ“Š [{target_number} {target_title}] ìµœì¢… ì ìˆ˜: {final_score}ì  | ì´ìœ : {reason}")

            return {
                "sufficiency": final_score >= 85,
                "completeness_score": final_score,  # ë°©ì–´ëœ ìµœì¢… ì ìˆ˜ ì €ì¥
                "grading_reason": reason,           # ì´ìœ  ì €ì¥
                "missing_subsections": missing_points,
                "next_step": "GENERATE_QUERY"
            }

    except Exception as e:
        print(f"âš ï¸ ì±„ì  ì˜¤ë¥˜: {e}")
        # ì—ëŸ¬ ë‚˜ë„ ì´ì „ ì ìˆ˜ëŠ” ìœ ì§€í•´ì¤Œ
        prev_score = state.get("completeness_score", 0)
        return {
            "sufficiency": False, 
            "completeness_score": prev_score, 
            "grading_reason": "ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë°œìƒ", 
            "next_step": "GENERATE_QUERY"
        }