# íŒŒì¼: generate_query.py (ì „ì²´ êµì²´)

from typing import Dict, Any
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from ..state_types import ProposalGenerationState
from dotenv import load_dotenv

load_dotenv()


# 

PROMPT_TEMPLATE_CONSULTANT = """
ë‹¹ì‹ ì€ ì •ë¶€ ì§€ì›ì‚¬ì—… ê¸°íšì„œ ì‘ì„±ì„ ë•ëŠ” AI ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ì œê³µí•œ ì •ë³´ê°€ í•´ë‹¹ ëª©ì°¨ì˜ **'ì‘ì„± ê°€ì´ë“œ(ì„¤ëª…)'**ì— ë¶€í•©í•˜ëŠ”ì§€ ë¶„ì„í•˜ê³ , ë¶€ì¡±í•œ í•µì‹¬ ë‚´ìš©ì„ í™•ë³´í•˜ê¸° ìœ„í•œ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

======================================================================
ğŸ“Œ <ì…ë ¥ ì •ë³´>
1. ì‘ì„± ëŒ€ìƒ ëª©ì°¨ ì •ë³´: "{target_chapter_info}"
   (â€» ìœ„ ì •ë³´ ë‚´ì— í¬í•¨ëœ 'ì„¤ëª…', 'ê°€ì´ë“œ', 'ì‘ì„± ìš”ë ¹' ë“±ì˜ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„ì˜ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ìœ¼ì‹­ì‹œì˜¤.)

2. í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ì •ë³´: {collected_data}
======================================================================

ğŸ¯ <ì—­í•  ë° ì˜ì‚¬ê²°ì • ì›ì¹™>
1. **ê¸°ì¤€ í™•ì¸**: "{target_chapter_info}"ì— ëª…ì‹œëœ í•´ë‹¹ ëª©ì°¨ì˜ **ì‘ì„± ì˜ë„ì™€ í•„ìˆ˜ í¬í•¨ ìš”ì†Œ(ì„¤ëª… ë¶€ë¶„)**ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
2. **ê°­(Gap) ë¶„ì„**: ìœ„ ê¸°ì¤€ê³¼ ëŒ€ì¡°í•˜ì—¬, í˜„ì¬ ìˆ˜ì§‘ëœ ì •ë³´({collected_data})ì—ì„œ **ë¹ ì ¸ìˆê±°ë‚˜ êµ¬ì²´í™”ê°€ í•„ìš”í•œ í•­ëª©**ì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
3. **ì¤‘ë³µ ë°©ì§€**: ì´ë¯¸ ìˆ˜ì§‘ëœ ì •ë³´ì— ì¶©ë¶„íˆ í¬í•¨ëœ ë‚´ìš©ì€ ì ˆëŒ€ ë‹¤ì‹œ ë¬»ì§€ ì•ŠìŠµë‹ˆë‹¤.
4. **ì§ˆë¬¸ ìƒì„±**: ì‚¬ìš©ìê°€ ê³ ë¯¼ ì—†ì´ ì¦‰ì‹œ ë‹µë³€í•  ìˆ˜ ìˆë„ë¡, ì¶”ìƒì ì¸ ì§ˆë¬¸ ëŒ€ì‹  **êµ¬ì²´ì ì¸ ë°ì´í„°ë‚˜ ê³„íš**ì„ ë¬»ëŠ” í•­ëª©ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë‚˜ì—´í•©ë‹ˆë‹¤.

======================================================================
ğŸ“ <ì¶œë ¥ í˜•ì‹>
ì•„ë˜ í˜•ì‹ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì‹­ì‹œì˜¤. í˜•ì‹ ë³€ê²½ ê¸ˆì§€.

[ì¶”ê°€ ì •ë³´ ìš”ì²­]

ì‘ì„± ê°€ì´ë“œë¥¼ ë³´ë‹ˆ, ì•„ë˜ ë‚´ìš©ì´ ì¡°ê¸ˆ ë” ë³´ì™„ë˜ì–´ì•¼ í•´ìš”!

- (ë¶€ì¡±í•œ í•­ëª© 1: ëª©ì°¨ ì„¤ëª…ì— ëª…ì‹œë˜ì–´ ìˆìœ¼ë‚˜ ëˆ„ë½ëœ ë‚´ìš© êµ¬ì²´ì  ëª…ì‹œ)
- (ë¶€ì¡±í•œ í•­ëª© 2: ì‘ì„± ì˜ë„ë¥¼ ì‚´ë¦¬ê¸° ìœ„í•´ êµ¬ì²´í™”ê°€ í•„ìš”í•œ ë°ì´í„°ë‚˜ ê³„íš)
- (ë¶€ì¡±í•œ í•­ëª© 3: í•„ìš”ì‹œ ë” ì¶”ê°€, ìµœëŒ€ 3-5ê°œ)

ğŸ“Œ ì˜ˆì‹œ
(ìœ„ ë¶€ì¡±í•œ í•­ëª©ë“¤ì— ëŒ€í•´ ì‚¬ìš©ìê°€ ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ì‹¤ì œ ë‹µë³€ ì˜ˆì‹œë¥¼ 2-3ì¤„ë¡œ ì‘ì„±.
ì˜ˆ: "2024ë…„ 10ì›” ì‹œì œí’ˆ ì œì‘ ì™„ë£Œ, 11ì›” í•„ë“œ í…ŒìŠ¤íŠ¸ ì§„í–‰ (ì˜ˆì‚° 5ì²œë§Œ ì›)
ê²½ìŸì‚¬ A ëŒ€ë¹„ ì²˜ë¦¬ ì†ë„ 2ë°° í–¥ìƒ, ê°€ê²© 30% ì ˆê° ëª©í‘œ")

ğŸ™Œ ìœ„ì²˜ëŸ¼ ì•Œë ¤ì£¼ì‹œë©´ ê°€ì´ë“œì— ë”± ë§ëŠ” ë‚´ìš©ì„ ì‘ì„±í•´ ë“œë¦´ê²Œìš”!

âš ï¸ ì¤‘ìš”:
- ì§ˆë¬¸ì€ ë°˜ë“œì‹œ **ì…ë ¥ëœ ëª©ì°¨ ì •ë³´ì˜ 'ì„¤ëª…(Description)'**ì„ ê·¼ê±°ë¡œ ë„ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.
- ë‹¨ìˆœíˆ "ë‚´ìš©ì„ ë³´ê°•í•´ì£¼ì„¸ìš”"ê°€ ì•„ë‹ˆë¼, "ì–´ë–¤ ë°ì´í„°/ìˆ˜ì¹˜/ê³„íšì´ í•„ìš”í•œì§€" êµ¬ì²´ì ìœ¼ë¡œ ë¬»ìŠµë‹ˆë‹¤.
- ì‹¬ì‚¬ ê¸°ì¤€, ì ìˆ˜, í‰ê°€ ë‚´ìš© ë“± ë‚´ë¶€ ì •ë³´ëŠ” ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.
- ì§ˆë¬¸ í•­ëª©ì€ 3~5ê°œ ì´ë‚´ë¡œ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ì¶”ë¦½ë‹ˆë‹¤.
======================================================================
"""

def generate_query(state: ProposalGenerationState) -> ProposalGenerationState:
    print("--- ë…¸ë“œ ì‹¤í–‰: generate_query (Score Display / Fix Error) ---")
    
    # ğŸŒŸ [ì˜¤ë¥˜ í•´ê²°] generated_response ë³€ìˆ˜ë¥¼ ë¯¸ë¦¬ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    generated_response = ""
    
    try:
        llm = ChatOpenAI(temperature=0.1, model="gpt-4o-mini")
    except Exception:
        return {"current_query": "LLM ì´ˆê¸°í™” ì˜¤ë¥˜ ë°œìƒ"}

    # 1. ìƒíƒœ ë³€ìˆ˜ ì¶”ì¶œ ë° ì´ˆê¸°ê°’ ì„¤ì •
    user_prompt = state.get("user_prompt", "")
    collected_data = state.get("collected_data", "")
    if not collected_data:
        collected_data = "(ì—†ìŒ)"
    
    current_avg_score = state.get("completeness_score", 0) 
    grading_reason = state.get("grading_reason", "")
    # missing_list = state.get("missing_subsections", [])
    section_scores = state.get("section_scores", {}) 
    # missing_points = ", ".join(missing_list) if missing_list else "(ì—†ìŒ)"

    print('-' * 50)
    print('grading_reason: ', grading_reason)
    print('-' * 50)
    
    fetched_context = state.get("fetched_context", {})
    anal_guide_summary = str(fetched_context.get("anal_guide", "ì „ëµ ì •ë³´ ì—†ìŒ"))

    toc_structure = state.get("draft_toc_structure", [])
    current_idx = state.get("current_chapter_index", 0)
    
    # 2. [í•µì‹¬] ì§„í–‰ë¥  í‘œì‹œ ë³€ìˆ˜ ì´ˆê¸°í™” ë° ê³„ì‚°
    major_chapter_title = "ì±•í„° ì œëª© ì—†ìŒ"
    focused_subchapter_display = "ì´ˆê¸° ì§ˆë¬¸"
    focused_subchapter_score = current_avg_score #í˜„ì¬ ASSESS_INFOì˜ ê²°ê³¼ ì ìˆ˜
    all_sub_section_numbers = []
    # avg_score_description = "(ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜ ë˜ëŠ” ì´ˆê¸° ì§„ì…)"
    target_info_full = "ì •ë³´ ìˆ˜ì§‘"
    chapter_display = "ì „ì²´ ê°œìš”"

    if toc_structure and current_idx < len(toc_structure):
        major_chapter_item = toc_structure[current_idx]
        major_chapter_number = major_chapter_item.get("number", "0") 
        major_chapter_title = major_chapter_item.get("title", "ì œëª© ì—†ìŒ") 

        # 2-1. LLM í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©ë  ì£¼ ì±•í„° ì •ë³´ êµ¬ì„±
        chapter_display = f"{major_chapter_item.get('number')} {major_chapter_item.get('title')}"
        target_info_full = f"[{chapter_display}]\nì„¤ëª…: {major_chapter_item.get('description')}" 

        print('target_info_full: ', target_info_full)
        
        # 2-2. í•˜ìœ„ í•­ëª© ë°ì´í„° ì¶”ì¶œ
        for item in toc_structure:
            num = item.get("number", "")
            if num.startswith(major_chapter_number + '.') and '.' in num:
                all_sub_section_numbers.append(num)
        
        # 2-3. í¬ì»¤ìŠ¤ ëŒ€ìƒ (1.1 í•­ëª©) ë° ì ìˆ˜ ì„¤ì •
        if all_sub_section_numbers:
            first_subchapter_num = all_sub_section_numbers[0]
            first_subchapter_item = next((item for item in toc_structure if item.get("number") == first_subchapter_num), None)
            
            if first_subchapter_item:
                focused_subchapter_display = f"{first_subchapter_item.get('number')} {first_subchapter_item.get('title')}"
                # ê°œë³„ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸° 
                focused_subchapter_score = section_scores.get(first_subchapter_num, 0)
        
        # 2-4. ì „ì²´ ì§„í–‰ë¥  ì„¤ëª… ë¬¸ìì—´ ìƒì„±
        subchapter_list_str = ", ".join(all_sub_section_numbers)
        if all_sub_section_numbers:
            avg_score_description = f"({subchapter_list_str} í‰ê· , {major_chapter_title} ë‚´ {len(all_sub_section_numbers)}ê°œ í•­ëª©)"
        else:
            avg_score_description = f"({major_chapter_title} ìì²´ ì§„í–‰ë¥ )"

    # 3. ìµœê·¼ ëŒ€í™” ê¸°ë¡ ì¶”ì¶œ
    msgs = state.get("messages", [])
    recent_history = ""
    if msgs:
        for msg in msgs:
            role = "ğŸ‘¤" if msg.get("role") == "user" else "ğŸ¤–"
            content = msg.get("content", "")
            recent_history += f"{role}: {content}\n"

    # 4. LLM í˜¸ì¶œ ë° ì‘ë‹µ ìƒì„±
    prompt = PromptTemplate.from_template(PROMPT_TEMPLATE_CONSULTANT)
    chain = prompt | llm
    
    try:
        generated_response = chain.invoke({
            "target_chapter_info": target_info_full,
            "collected_data": collected_data,
            "grading_reason": grading_reason,
        }).content.strip()
    except Exception as e:
        print(f"âŒ í”„ë¡¬í”„íŠ¸ ì…ë ¥ ì˜¤ë¥˜: {e}")
        generated_response = "ì§ˆë¬¸ ìƒì„± ì¤‘ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
    
    # 5. ìµœì¢… ì¶œë ¥ (ì‹¬ì‚¬ ê¸°ì¤€ ì½”ë©˜íŠ¸ëŠ” ì œê±°, ì‚¬ìš©ìì—ê²ŒëŠ” ì§ˆë¬¸ë§Œ í‘œì‹œ)
    final_response = generated_response

    history = state.get("messages", [])
    history.append({"role": "assistant", "content": final_response})

    # ğŸ“Œ [ë””ë²„ê·¸] â€” scoreê°€ ì •ìƒì ìœ¼ë¡œ ë„˜ì–´ì˜¤ëŠ”ì§€ í™•ì¸
    # print("DEBUG >>> generate_query received state keys:", state.keys())
    # print("DEBUG >>> generate_query completeness_score:", state.get("completeness_score"))
    # print("DEBUG >>> generate_query section_scores:", section_scores)
    # print("DEBUG >>> generate_query focused score:", focused_subchapter_score)

    return {
        "current_query": final_response,
        "messages": history,
    }