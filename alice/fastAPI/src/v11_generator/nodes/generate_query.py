# íŒŒì¼: generate_query.py (ì „ì²´ êµì²´)

from typing import Dict, Any
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from ..state_types import ProposalGenerationState
from dotenv import load_dotenv

load_dotenv()


PROMPT_TEMPLATE_CONSULTANT = """
ë‹¹ì‹ ì€ ì •ë¶€ ì§€ì›ì‚¬ì—… í•©ê²©ì„ ë•ëŠ” 'ì „ëµê¸°íš íŒŒíŠ¸ë„ˆ'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì™€ ëŒ€í™”í•˜ê³  ìˆì§€ë§Œ, ë‹¹ì‹ ì˜ ìµœìš°ì„  ëª©í‘œëŠ” [íŒì‚¬ì˜ í‰ê°€]ë¥¼ ë°˜ì˜í•˜ì—¬ 
ì ìˆ˜ë¥¼ 70ì  ì´ìƒ(í†µê³¼)ìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.

<ì…ë ¥ ì •ë³´>
1. ì‘ì„± ëª©í‘œ: "{target_chapter_info}"
2. ê³µê³ ë¬¸ í•µì‹¬: "{anal_guide_summary}"
3. ëˆ„ì ëœ ì •ë³´: {collected_data} [ê°•ì¡°] ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— ëª…í™•íˆ í¬í•¨
4. ì‚¬ìš©ì ë°œì–¸: "{user_prompt}"
5. ìµœê·¼ ëŒ€í™”: {recent_history}

6. íŒì‚¬ì˜ í‰ê°€ (Judge's Feedback)
- í˜„ì¬ ì ìˆ˜: {current_score}ì 
- í‰ê°€ ì‚¬ìœ : {grading_reason}
- ë¶€ì¡±í•œ í•­ëª©(Missing Points): {missing_points}

<ì‚¬ê³  ê³¼ì •>
1. ìƒíƒœ ì ê²€:
- **í˜„ì¬ê¹Œì§€ ëˆ„ì ëœ ì •ë³´({collected_data} ë‚´ìš©)**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŒì‚¬ì˜ í‰ê°€ë¥¼ í•´ì„.
- ì‚¬ìš©ì ë°œì–¸ì´ ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì£¼ì§€ ì•ŠëŠ”ë‹¤ë©´, **ì´ì „ì˜ ë§¥ë½(ì“°ë ˆê¸° ì‚¬ì—…)**ì„ ìœ ì§€í•˜ë©° ì§ˆë¬¸ì„ ìƒì„±í•´ì•¼ í•¨.
2. ë°˜ì‘ ë° ì „í™˜:
- ì‚¬ìš©ì ë§ì— ì§§ê²Œ í˜¸ì‘ í›„, "í•˜ì§€ë§Œ í•©ê²©ì„ ìœ„í•´ì„œëŠ” ~ê°€ ë³´ì™„ë˜ì–´ì•¼ í•©ë‹ˆë‹¤"ë¡œ í™”ì œë¥¼ ì „í™˜.
- ë¬´ì¡°ê±´ ë¶€ì¡±í•œ í•­ëª©ì— ëŒ€í•œ ì§ˆë¬¸ì„ ë˜ì§.
3. ì§ˆë¬¸ ì „ëµ:
- ì§ˆë¬¸ì€ êµ¬ì²´ì ìœ¼ë¡œ. ì˜ˆ: "ìˆ˜ìµì„±ì€ ì¦ëª…ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ ì‹¬ì‚¬ìœ„ì›ì€ 'ì‚¬ì—…ì˜ í•„ìš”ì„±'ì„ ë´…ë‹ˆë‹¤. ì™œ ì§€ê¸ˆ ì´ ì‹œì ì— ì´ ê¸°ìˆ ì´ í•„ìš”í•œê°€ìš”?"

ì¶œë ¥ ê°€ì´ë“œ
- ì ˆëŒ€ ê¸ˆì§€: ë°˜ë³µ ì§ˆë¬¸, ì¡ë‹´.
- ë§íˆ¬: ì „ë¬¸ê°€ë‹¤ìš´ ìì—°ìŠ¤ëŸ¬ìš´ íšŒí™”ì²´.
"""

def generate_query(state: ProposalGenerationState) -> Dict[str, Any]:
    print("--- ë…¸ë“œ ì‹¤í–‰: generate_query (Score Display / Fix Error) ---")
    
    # ğŸŒŸ [ì˜¤ë¥˜ í•´ê²°] generated_response ë³€ìˆ˜ë¥¼ ë¯¸ë¦¬ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    generated_response = ""
    
    try:
        llm = ChatOpenAI(temperature=0.1, model="gpt-4o")
    except Exception:
        return {"current_query": "LLM ì´ˆê¸°í™” ì˜¤ë¥˜ ë°œìƒ"}

    # 1. ìƒíƒœ ë³€ìˆ˜ ì¶”ì¶œ ë° ì´ˆê¸°ê°’ ì„¤ì •
    user_prompt = state.get("user_prompt", "")
    collected_data = state.get("collected_data", "")
    if not collected_data:
        collected_data = "(ì—†ìŒ)"
    
    current_avg_score = state.get("completeness_score", 0) 
    grading_reason = state.get("grading_reason", "")
    missing_list = state.get("missing_subsections", [])
    section_scores = state.get("section_scores", {}) 
    missing_points = ", ".join(missing_list) if missing_list else "(ì—†ìŒ)"
    
    fetched_context = state.get("fetched_context", {})
    anal_guide_summary = str(fetched_context.get("anal_guide", "ì „ëµ ì •ë³´ ì—†ìŒ"))

    toc_structure = state.get("draft_toc_structure", [])
    current_idx = state.get("current_chapter_index", 0)
    
    # 2. [í•µì‹¬] ì§„í–‰ë¥  í‘œì‹œ ë³€ìˆ˜ ì´ˆê¸°í™” ë° ê³„ì‚°
    major_chapter_title = "ì±•í„° ì œëª© ì—†ìŒ"
    focused_subchapter_display = "ì´ˆê¸° ì§ˆë¬¸"
    focused_subchapter_score = current_avg_score #í˜„ì¬ ASSESS_INFOì˜ ê²°ê³¼ ì ìˆ˜
    all_sub_section_numbers = []
    avg_score_description = "(ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜ ë˜ëŠ” ì´ˆê¸° ì§„ì…)"
    target_info_full = "ì •ë³´ ìˆ˜ì§‘"
    chapter_display = "ì „ì²´ ê°œìš”"

    if toc_structure and current_idx < len(toc_structure):
        major_chapter_item = toc_structure[current_idx]
        major_chapter_number = major_chapter_item.get("number", "0") 
        major_chapter_title = major_chapter_item.get("title", "ì œëª© ì—†ìŒ") 

        # 2-1. LLM í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©ë  ì£¼ ì±•í„° ì •ë³´ êµ¬ì„±
        chapter_display = f"{major_chapter_item.get('number')} {major_chapter_item.get('title')}"
        target_info_full = f"[{chapter_display}]\nì„¤ëª…: {major_chapter_item.get('description')}" 
        
        # 2-2. í•˜ìœ„ í•­ëª© ë°ì´í„° ì¶”ì¶œ
        for item in toc_structure:
            num = item.get("number", "")
            if num.startswith(major_chapter_number + '.') and '.' in num:
                all_sub_section_numbers.append(num)
        
        # 2-3. í¬ì»¤ìŠ¤ ëŒ€ìƒ (1.1 í•­ëª©) ë° ì ìˆ˜ ì„¤ì •
        if all_sub_section_numbers:
            first_subchapter_num = all_sub_section_numbers[0]
            first_subchapter_item = next((item for item in toc_structure if item.get("number") == first_subchapter_num), None)
            
            if first_subchapter_item:
                focused_subchapter_display = f"{first_subchapter_item.get('number')} {first_subchapter_item.get('title')}"
                # ê°œë³„ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸° 
                focused_subchapter_score = section_scores.get(first_subchapter_num, 0)
        
        # 2-4. ì „ì²´ ì§„í–‰ë¥  ì„¤ëª… ë¬¸ìì—´ ìƒì„±
        subchapter_list_str = ", ".join(all_sub_section_numbers)
        if all_sub_section_numbers:
            avg_score_description = f"({subchapter_list_str} í‰ê· , {major_chapter_title} ë‚´ {len(all_sub_section_numbers)}ê°œ í•­ëª©)"
        else:
            avg_score_description = f"({major_chapter_title} ìì²´ ì§„í–‰ë¥ )"

    # 3. ìµœê·¼ ëŒ€í™” ê¸°ë¡ ì¶”ì¶œ
    msgs = state.get("messages", [])
    recent_history = ""
    if msgs:
        for msg in msgs[-4:]:
            role = "ğŸ‘¤" if msg.get("role") == "user" else "ğŸ¤–"
            content = msg.get("content", "")
            recent_history += f"{role}: {content}\n"

    # 4. LLM í˜¸ì¶œ ë° ì‘ë‹µ ìƒì„±
    prompt = PromptTemplate.from_template(PROMPT_TEMPLATE_CONSULTANT)
    chain = prompt | llm
    
    try:
        generated_response = chain.invoke({
            "anal_guide_summary": anal_guide_summary,
            "target_chapter_info": target_info_full,
            "user_prompt": user_prompt,
            "collected_data": collected_data,
            "recent_history": recent_history,
            "current_score": current_avg_score,
            "grading_reason": grading_reason,
            "missing_points": missing_points
        }).content.strip()
    except Exception as e:
        print(f"âŒ í”„ë¡¬í”„íŠ¸ ì…ë ¥ ì˜¤ë¥˜: {e}")
        generated_response = "ì§ˆë¬¸ ìƒì„± ì¤‘ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
    
    # 5. ìµœì¢… ì¶œë ¥ í¬ë§· êµ¬ì„± (ì‚¬ìš©ì ìš”ì²­ ë°˜ì˜)
    feedback_text = f" | ğŸ’¡ {grading_reason}" if grading_reason else ""
    
    final_response = (
        f"{generated_response}\n\n"
        # f"**(ğŸ“Œ ì „ì²´ì™„ì„±ë„: {current_avg_score}% {avg_score_description}) "
        f"(í˜„ì¬ ì§„í–‰ì¤‘: [{focused_subchapter_display}] ì •ë³´ìˆ˜ì§‘ë„: {focused_subchapter_score}%{feedback_text})**"
    )

    history = state.get("messages", [])
    history.append({"role": "assistant", "content": final_response})

    # ğŸ“Œ [ë””ë²„ê·¸] â€” scoreê°€ ì •ìƒì ìœ¼ë¡œ ë„˜ì–´ì˜¤ëŠ”ì§€ í™•ì¸
    print("DEBUG >>> generate_query received state keys:", state.keys())
    print("DEBUG >>> generate_query completeness_score:", state.get("completeness_score"))
    print("DEBUG >>> generate_query section_scores:", section_scores)
    print("DEBUG >>> generate_query focused score:", focused_subchapter_score)

    return {
        **state,
        "current_query": final_response,
        "messages": history,
    }