from typing import Dict, Any, List
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from ..state_types import ProposalGenerationState
from dotenv import load_dotenv

load_dotenv()

def manage_progression(state: ProposalGenerationState) -> ProposalGenerationState:
    """
    [í›„ì²˜ë¦¬ ë…¸ë“œ]
    ì´ˆì•ˆ ìƒì„±ì´ ì™„ë£Œëœ í›„ ì‹¤í–‰ë©ë‹ˆë‹¤.
    1. í˜„ì¬ ì±•í„° ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ accumulated_dataì— ì €ì¥í•©ë‹ˆë‹¤.
    2. collected_dataë¥¼ ì´ˆê¸°í™”í•˜ì—¬ ë‹¤ìŒ ì±•í„° ì‘ì„±ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
    3. ì¸ë±ìŠ¤ë¥¼ ì¦ê°€ì‹œí‚µë‹ˆë‹¤.
    """
    print("--- ë…¸ë“œ ì‹¤í–‰: manage_progression (Post-Processing) ---")
    
    # 1. ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
    current_idx = state.get("current_chapter_index", 0)
    toc = state.get("draft_toc_structure", [])
    collected_data = state.get("collected_data", "")
    
    # ì˜ˆì™¸ ì²˜ë¦¬
    if current_idx >= len(toc):
        return {}

    current_item = toc[current_idx]
    current_number = current_item.get("number", "0")
    current_title = current_item.get("title", "ì œëª© ì—†ìŒ")
    
    # 2. ëˆ„ì  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    raw_data = state.get("accumulated_data")
    accumulated_data_list: List[str] = []
    if isinstance(raw_data, list):
        accumulated_data_list = raw_data
    elif isinstance(raw_data, str):
        accumulated_data_list = [raw_data] if raw_data else []
    
    # 3. ìš”ì•½ ìˆ˜í–‰ (ì¤‘ë³µ í™•ì¸)
    header_check = f"### [{current_number} {current_title} ìš”ì•½]"
    is_already_saved = any(header_check in content for content in accumulated_data_list)
    
    new_accumulated_list = accumulated_data_list
    
    if not is_already_saved:
        print(f"âš¡ [manage_progression] ìš”ì•½ ë° ì •ë¦¬ ì‹œì‘: {current_title}")
        llm = None
        try:
            llm = ChatOpenAI(temperature=0, model="gpt-4o-mini")
        except Exception as e:
            print(f"âš ï¸ LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")

        summary_text = collected_data
        if llm and collected_data.strip():
            SUMMARY_PROMPT = f"""
            ë‹¹ì‹ ì€ ê¸°íšì„œ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            ì•„ë˜ [ëŒ€í™” ë‚´ìš©]ì—ì„œ **[{current_title}]** ì‘ì„±ì— í•„ìš”í•œ í•µì‹¬ ì •ë³´ë§Œ ìš”ì•½í•˜ì„¸ìš”.
            
            <ëŒ€í™” ë‚´ìš©>
            {{current_data}}
            """
            try:
                prompt = PromptTemplate.from_template(SUMMARY_PROMPT)
                chain = prompt | llm
                summary_text = chain.invoke({"current_data": collected_data}).content.strip()
            except Exception as e:
                print(f"âš ï¸ ìš”ì•½ ì˜¤ë¥˜: {e}")
        
        # ìš”ì•½ë³¸ ì €ì¥
        summary_content = f"### [{current_number} {current_title} ìš”ì•½]\n{summary_text}\n----------------------------------------"
        new_accumulated_list = accumulated_data_list + [summary_content]
    
    # 4. ë‹¤ìŒ ë‹¨ê³„ ì¤€ë¹„ (ë§¤ìš° ì¤‘ìš”!)
    # ì´ˆì•ˆ ì‘ì„±ì´ ëë‚¬ìœ¼ë¯€ë¡œ ë°ì´í„°ë¥¼ ë¹„ìš°ê³  ì¸ë±ìŠ¤ë¥¼ ì˜¬ë¦½ë‹ˆë‹¤.
    next_idx = current_idx + 1
    
    print(f"âœ… ì„¹ì…˜ ì™„ë£Œ ì²˜ë¦¬: [{current_title}] -> ë‹¤ìŒ ì¸ë±ìŠ¤ë¡œ ì´ë™")
    
    return {
        "accumulated_data": new_accumulated_list,
        "collected_data": "",       # ğŸ§¹ ë°ì´í„° ë¹„ìš°ê¸° (ë‹¤ìŒ ì±•í„°ë¥¼ ìœ„í•´ í•„ìˆ˜)
        "current_chapter_index": next_idx, # â© ì¸ë±ìŠ¤ ì¦ê°€
        "sufficiency": False        # ìƒíƒœ ì´ˆê¸°í™”
    }