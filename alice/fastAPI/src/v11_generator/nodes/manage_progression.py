from typing import Dict, Any, List
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from ..state_types import ProposalGenerationState
from dotenv import load_dotenv

load_dotenv()

def manage_progression(state: ProposalGenerationState) -> Dict[str, Any]:
    print("--- ë…¸ë“œ ì‹¤í–‰: manage_progression (Section Progression) ---")
    
    is_sufficient = state.get("sufficiency", False)
    current_idx = state.get("current_chapter_index", 0)
    toc = state.get("draft_toc_structure", [])
    
    if not is_sufficient or current_idx >= len(toc):
        # ì¶©ë¶„í•˜ì§€ ì•Šê±°ë‚˜ ëª©ì°¨ ëì´ë©´ ì§„í–‰ ê´€ë¦¬ ë…¸ë“œê°€ í˜¸ì¶œë  ì¼ ì—†ìŒ (ë°©ì–´ì  ì½”ë“œ)
        # Note: ì´ í•¨ìˆ˜ëŠ” sufficiency=Trueì¼ ë•Œë§Œ í˜¸ì¶œë˜ì–´ì•¼ í•¨
        return {} 

    # 1. í˜„ì¬ ì™„ë£Œëœ ì„¹ì…˜ ì •ë³´ ë° ë°ì´í„°
    current_item = toc[current_idx]
    current_number = current_item.get("number", "0")
    current_title = current_item.get("title", "ì œëª© ì—†ìŒ")
    current_data = state.get("collected_data", "")
    
    # ğŸ”‘ [í•µì‹¬ 1] ì¤‘ë³µ ì €ì¥ ë°©ì§€ë¥¼ ìœ„í•œ ê²€ì‚¬
    # stateì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ List[str]ë¡œ ê°•ì œ ë³€í™˜í•©ë‹ˆë‹¤.
    raw_data = state.get("accumulated_data")
    
    if isinstance(raw_data, str) or raw_data is None:
        # ì´ì „ì— ì˜ëª»ëœ íƒ€ì…ì´ ì €ì¥ë˜ì—ˆê±°ë‚˜, Noneì¸ ê²½ìš°, ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì‹œì‘
        accumulated_data_list: List[str] = []
    else:
        # ì˜¬ë°”ë¥¸ List[str] íƒ€ì…ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
        accumulated_data_list: List[str] = raw_data
    
    # ìš”ì•½ í—¤ë”ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ ê²€ì‚¬ (ì˜ˆ: "### [1.1 ì‚¬ì—… ë°°ê²½ ë° í•„ìš”ì„± ìš”ì•½]")
    header_check = f"### [{current_number} {current_title} ìš”ì•½]"
    is_already_saved = any(header_check in content for content in accumulated_data_list)
    
    if is_already_saved:
        print(f"âœ… ë°ì´í„° ì¬ì²˜ë¦¬ ìŠ¤í‚µ: [{current_number} {current_title}]ì€ ì´ë¯¸ ìµœì¢… ì €ì¥ì†Œì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        # ì´ë¯¸ ì €ì¥ë˜ì–´ ìˆë‹¤ë©´, ìš”ì•½/ì €ì¥ ê³¼ì •ì„ ê±´ë„ˆë›°ê³  ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
        new_accumulated_list = accumulated_data_list
    
    else:
        # --- [ìµœì´ˆ ì €ì¥: ìš”ì•½ ë° ì••ì¶• ë¡œì§] ---
        llm = None
        try:
            llm = ChatOpenAI(temperature=0, model="gpt-4o")
        except Exception as e:
            print(f"âš ï¸ LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")

        summary_text = current_data
        if llm and current_data.strip():
            SUMMARY_PROMPT = f"""
            ë‹¹ì‹ ì€ ê¸°íšì„œ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            ì•„ë˜ [ëŒ€í™” ë‚´ìš©]ì—ì„œ **[{current_title}]** ì‘ì„±ì— í•„ìš”í•œ **í•µì‹¬ ì •ë³´(ì‚¬ì‹¤, ìˆ˜ì¹˜, ì „ëµ)**ë§Œ ì¶”ì¶œí•˜ì—¬ ìš”ì•½í•˜ì„¸ìš”.
            ì¡ë‹´ì´ë‚˜ ë¶ˆí•„ìš”í•œ ë¬¸ì¥ì€ ëª¨ë‘ ì œê±°í•˜ì„¸ìš”. ê°œì¡°ì‹ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
            
            <ëŒ€í™” ë‚´ìš©>
            {{current_data}}
            """
            try:
                prompt = PromptTemplate.from_template(SUMMARY_PROMPT)
                chain = prompt | llm
                summary_result = chain.invoke({"current_data": current_data}).content.strip()
                summary_text = summary_result
                print(f"âš¡ ë°ì´í„° ì••ì¶• ì™„ë£Œ: {current_number} - {len(current_data)}ì -> {len(summary_text)}ì")
            except Exception as e:
                print(f"âš ï¸ ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # ìš”ì•½ëœ ë‚´ìš©ì„ Listì— ì¶”ê°€í•  í¬ë§·ì„ ë§Œë“­ë‹ˆë‹¤.
        summary_content = f"### [{current_number} {current_title} ìš”ì•½]\n{summary_text}\n----------------------------------------"
        
        # ğŸ’¡ [í•µì‹¬ ìˆ˜ì •] ë¦¬ìŠ¤íŠ¸ ê²°í•©ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        new_accumulated_list = accumulated_data_list + [summary_content]
        
        print(f"âš¡ ë°ì´í„° ì••ì¶• ë° ì €ì¥ ì™„ë£Œ: [{current_number} {current_title}]")
    
    
    # 2. [í•µì‹¬ 2] ë‹¤ìŒ ì„¹ì…˜ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ (ì¤‘ë³µ ì—¬ë¶€ì™€ ìƒê´€ì—†ì´ ë‹¤ìŒìœ¼ë¡œ ì§„í–‰)
    next_idx = current_idx + 1 # ì™„ë£Œëœ ì¸ë±ìŠ¤ ë‹¤ìŒ ìˆœì„œë¡œ ì—…ë°ì´íŠ¸
    
    next_chapter_info = ""
    if next_idx < len(toc):
        next_chapter = toc[next_idx]
        next_chapter_info = next_chapter.get('title')
        # print ë¬¸ì€ ì‹¤ì œ ì €ì¥ ì—¬ë¶€ì™€ ìƒê´€ì—†ì´ ë‹¤ìŒ ì¸ë±ìŠ¤ ì •ë³´ë¥¼ ì¶œë ¥
        print(f"â© ì„¹ì…˜ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸: [{current_title}] -> ë‹¤ìŒ ì¸ë±ìŠ¤ [{next_chapter_info}]")
    
    # ğŸ”‘ [í•µì‹¬ ìˆ˜ì •] ë‹¤ìŒ ë…¸ë“œ(GENERATE_QUERY)ì—ê²Œ ì™„ë£Œ ì •ë³´ë¥¼ ì „ë‹¬
    just_completed_chapter = f"{current_number} {current_title}"
    
    if next_idx < len(toc):
        return {
            "current_chapter_index": next_idx,
            "target_chapter": next_chapter.get("title", "ëª©í‘œ ì—†ìŒ"),
            # "accumulated_data": new_accumulated,
            "collected_data": "", # ë‹¤ìŒ ì±•í„°ë¥¼ ìœ„í•´ ë°ì´í„° ì´ˆê¸°í™”
            "completeness_score": 0, # ë‹¤ìŒ ì±•í„° ì ìˆ˜ ì´ˆê¸°í™”
            "next_step": "GENERATE_QUERY"
        }
    else:
        # ë” ì´ìƒ í•˜ìœ„ ì„¹ì…˜ì´ ì—†ë‹¤ë©´ ëª¨ë“  ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œë¡œ ê°„ì£¼
        print("ğŸ‰ ëª¨ë“  ì„¹ì…˜ ì™„ë£Œ: ìµœì¢… ì´ˆì•ˆ ìƒì„± ë‹¨ê³„ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        return {
            "next_step": "FINISH_DRAFT", # (ì¶”í›„ generate_draft ë…¸ë“œë¡œ ì—°ê²°)
            # "accumulated_data": new_accumulated,
            "collected_data": "", 
            "current_draft": f"ìµœì¢… ì´ˆì•ˆì„ ìƒì„±í•˜ê¸° ìœ„í•œ ì •ë³´ê°€ ëª¨ë‘ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ì§‘ëœ ì´ ì •ë³´ ê¸¸ì´: {len(new_accumulated)}ì",
            "completeness_score": 100
        }
