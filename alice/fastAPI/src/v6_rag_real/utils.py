"""
ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
"""

import re
from typing import Optional, List, Dict, Any


# ========================================
# ğŸ”– MVP2: ë¶„ì„ ëŒ€ì‹œë³´ë“œ (ê·¼ê±° ì¶”ì )
# ========================================
# ëª©ì : ê³µê³ ë¬¸ì—ì„œ "ë¶™ì„ 1 ì°¸ì¡°" ë“±ì˜ ì–¸ê¸‰ì´ ìˆì„ ë•Œ,
#       í•´ë‹¹ ì²¨ë¶€ ë¬¸ì„œë¥¼ ìë™ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ ë¶„ì„ ëŒ€ì‹œë³´ë“œì—ì„œ
#       ê·¼ê±°ë¡œ í‘œì‹œí•˜ê¸° ìœ„í•œ ì²¨ë¶€ë²ˆí˜¸ ì¶”ì¶œ
# ========================================

def extract_attachment_number(filename: str) -> Optional[int]:
    """
    [MVP2] íŒŒì¼ëª…ì—ì„œ ì²¨ë¶€ë²ˆí˜¸ ì¶”ì¶œ (ë¶„ì„ ëŒ€ì‹œë³´ë“œ ê·¼ê±° ì¶”ì ìš©)

    Args:
        filename: íŒŒì¼ëª…

    Returns:
        ì²¨ë¶€ë²ˆí˜¸ (ì˜ˆ: "ë¶™ì„1" â†’ 1) ë˜ëŠ” None

    Examples:
        >>> extract_attachment_number("ë¶™ì„1_ì—°êµ¬ê³„íšì„œ.hwp")
        1
        >>> extract_attachment_number("ë³„ì²¨2_ë™ì˜ì„œ.pdf")
        2
        >>> extract_attachment_number("ê³µê³ ë¬¸.pdf")
        None
    """
    patterns = [
        r'ë¶™ì„\s*(\d+)',
        r'ë³„ì²¨\s*(\d+)',
        r'ì²¨ë¶€\s*(\d+)',
        r'attachment\s*(\d+)',
        r'ë¶€ë¡\s*(\d+)',
    ]

    for pattern in patterns:
        match = re.search(pattern, filename, re.IGNORECASE)
        if match:
            return int(match.group(1))

    return None


def detect_section_headers(text: str) -> List[Dict[str, Any]]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ì„¹ì…˜ í—¤ë” ê°ì§€

    Args:
        text: ë¶„ì„í•  í…ìŠ¤íŠ¸

    Returns:
        ì„¹ì…˜ í—¤ë” ëª©ë¡ [{"level": 1, "title": "...", "position": ...}]
    """
    headers = []
    lines = text.split('\n')

    # íŒ¨í„´: "1. ", "ê°€. ", "1) ", "(1) " ë“±
    patterns = [
        (r'^([0-9]+)\.\s+(.+)$', 1),           # 1. ì œëª©
        (r'^([ê°€-í£])\.\s+(.+)$', 2),          # ê°€. ì œëª©
        (r'^([0-9]+)\)\s+(.+)$', 2),           # 1) ì œëª©
        (r'^\(([0-9]+)\)\s+(.+)$', 3),         # (1) ì œëª©
    ]

    for i, line in enumerate(lines):
        line = line.strip()
        if not line:
            continue

        for pattern, level in patterns:
            match = re.match(pattern, line)
            if match:
                headers.append({
                    'level': level,
                    'title': match.group(2).strip(),
                    'position': i,
                    'raw': line
                })
                break

    return headers


def chunk_by_sections(text: str, page_num: int, max_chunk_size: int = 1000, overlap_size: int = 200) -> List[Dict[str, Any]]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì„¹ì…˜ ê¸°ë°˜ìœ¼ë¡œ ì²­í‚¹ (ì˜¤ë²„ë© í¬í•¨)

    Args:
        text: ì²­í‚¹í•  í…ìŠ¤íŠ¸
        page_num: í˜ì´ì§€ ë²ˆí˜¸
        max_chunk_size: ìµœëŒ€ ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)
        overlap_size: ì˜¤ë²„ë© í¬ê¸° (ë¬¸ì ìˆ˜, ë¬¸ë§¥ ë³´ì¡´ìš©)

    Returns:
        ì²­í¬ ë¦¬ìŠ¤íŠ¸ [{"text": "...", "section": "...", "page": ..., "is_sectioned": bool}]
    """
    # ì„¹ì…˜ í—¤ë” ê°ì§€
    headers = detect_section_headers(text)

    chunks = []

    if not headers:
        # ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì²­í‚¹ (ì˜¤ë²„ë© í¬í•¨)
        # ë¬¸ì¥ ë¶„ë¦¬ (., !, ? ê¸°ì¤€)
        sentences = re.split(r'([.!?]\s+)', text)
        # êµ¬ë¶„ìë¥¼ ë¬¸ì¥ì— ë‹¤ì‹œ ë¶™ì„
        full_sentences = []
        for i in range(0, len(sentences) - 1, 2):
            if i + 1 < len(sentences):
                full_sentences.append(sentences[i] + sentences[i + 1])
            else:
                full_sentences.append(sentences[i])
        if len(sentences) % 2 == 1:
            full_sentences.append(sentences[-1])

        current_chunk = []
        current_length = 0
        previous_overlap = ""

        for sentence in full_sentences:
            sentence = sentence.strip()
            if not sentence:
                continue

            sentence_length = len(sentence)

            if current_length + sentence_length > max_chunk_size and current_chunk:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                chunk_text = ' '.join(current_chunk).strip()
                if chunk_text:
                    chunks.append({
                        'text': chunk_text,
                        'section': f'í˜ì´ì§€ {page_num}',
                        'page': page_num,
                        'is_sectioned': False
                    })

                    # ì˜¤ë²„ë© ì¤€ë¹„: ë§ˆì§€ë§‰ ëª‡ ë¬¸ì¥ ì €ì¥
                    overlap_text = chunk_text[-overlap_size:] if len(chunk_text) > overlap_size else chunk_text
                    # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸°
                    overlap_start = overlap_text.rfind('. ')
                    if overlap_start > 0:
                        previous_overlap = overlap_text[overlap_start + 2:]
                    else:
                        previous_overlap = overlap_text

                # ìƒˆ ì²­í¬ ì‹œì‘ (ì˜¤ë²„ë© í¬í•¨)
                if previous_overlap and previous_overlap not in sentence:
                    current_chunk = [previous_overlap, sentence]
                    current_length = len(previous_overlap) + sentence_length
                else:
                    current_chunk = [sentence]
                    current_length = sentence_length
            else:
                current_chunk.append(sentence)
                current_length += sentence_length

        # ë§ˆì§€ë§‰ ì²­í¬
        if current_chunk:
            chunk_text = ' '.join(current_chunk).strip()
            if chunk_text:
                chunks.append({
                    'text': chunk_text,
                    'section': f'í˜ì´ì§€ {page_num}',
                    'page': page_num,
                    'is_sectioned': False
                })

    else:
        # ì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹
        lines = text.split('\n')

        # ê° ì„¹ì…˜ë³„ë¡œ ì²­í¬ ìƒì„±
        for i, header in enumerate(headers):
            section_title = header['title']
            start_pos = header['position']
            end_pos = headers[i + 1]['position'] if i + 1 < len(headers) else len(lines)

            # ì„¹ì…˜ ë‚´ìš© ì¶”ì¶œ
            section_lines = lines[start_pos:end_pos]
            section_text = '\n'.join(section_lines).strip()

            # ì„¹ì…˜ì´ ë„ˆë¬´ í¬ë©´ ë¶„í•  (ì˜¤ë²„ë© í¬í•¨)
            if len(section_text) > max_chunk_size:
                # í° ì„¹ì…˜ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  (ì˜¤ë²„ë© í¬í•¨)
                sentences = re.split(r'([.!?]\s+)', section_text)
                full_sentences = []
                for j in range(0, len(sentences) - 1, 2):
                    if j + 1 < len(sentences):
                        full_sentences.append(sentences[j] + sentences[j + 1])
                    else:
                        full_sentences.append(sentences[j])
                if len(sentences) % 2 == 1:
                    full_sentences.append(sentences[-1])

                sub_chunks = []
                current_sub = []
                current_length = 0
                previous_overlap = ""

                for sentence in full_sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue

                    sentence_length = len(sentence)

                    if current_length + sentence_length > max_chunk_size and current_sub:
                        sub_text = ' '.join(current_sub).strip()
                        if sub_text:
                            sub_chunks.append(sub_text)

                            # ì˜¤ë²„ë© ì¤€ë¹„
                            overlap_text = sub_text[-overlap_size:] if len(sub_text) > overlap_size else sub_text
                            overlap_start = overlap_text.rfind('. ')
                            if overlap_start > 0:
                                previous_overlap = overlap_text[overlap_start + 2:]
                            else:
                                previous_overlap = overlap_text

                        # ìƒˆ ì²­í¬ ì‹œì‘ (ì˜¤ë²„ë© í¬í•¨)
                        if previous_overlap and previous_overlap not in sentence:
                            current_sub = [previous_overlap, sentence]
                            current_length = len(previous_overlap) + sentence_length
                        else:
                            current_sub = [sentence]
                            current_length = sentence_length
                    else:
                        current_sub.append(sentence)
                        current_length += sentence_length

                if current_sub:
                    sub_text = ' '.join(current_sub).strip()
                    if sub_text:
                        sub_chunks.append(sub_text)

                # ê° ì„œë¸Œ ì²­í¬ë¥¼ ì¶”ê°€
                for idx, sub_text in enumerate(sub_chunks):
                    chunks.append({
                        'text': sub_text,
                        'section': f'{section_title} (part {idx+1})',
                        'page': page_num,
                        'is_sectioned': True
                    })
            else:
                # ì„¹ì…˜ì´ ì ë‹¹í•œ í¬ê¸°ë©´ ê·¸ëŒ€ë¡œ ì²­í¬ë¡œ
                if section_text:
                    chunks.append({
                        'text': section_text,
                        'section': section_title,
                        'page': page_num,
                        'is_sectioned': True
                    })

    # ë¹ˆ ì²­í¬ ì œê±°
    chunks = [c for c in chunks if c['text'].strip()]

    return chunks


def clean_text(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ ì •ì œ

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸

    Returns:
        ì •ì œëœ í…ìŠ¤íŠ¸
    """
    # ì—°ì†ëœ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)

    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì¼ë¶€ ë³´ì¡´)
    # text = re.sub(r'[^\w\sê°€-í£.,!?()[\]{}\-:]', '', text)

    return text.strip()


def merge_short_chunks(chunks: List[str], min_length: int = 100) -> List[str]:
    """
    ì§§ì€ ì²­í¬ë¥¼ ë³‘í•©

    Args:
        chunks: ì›ë³¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        min_length: ìµœì†Œ ì²­í¬ ê¸¸ì´

    Returns:
        ë³‘í•©ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    merged = []
    buffer = ""

    for chunk in chunks:
        if len(buffer) + len(chunk) < min_length:
            buffer += " " + chunk
        else:
            if buffer:
                merged.append(buffer.strip())
            buffer = chunk

    if buffer:
        merged.append(buffer.strip())

    return merged
