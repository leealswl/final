"""
ëª©ì°¨(Table of Contents) ì¶”ì¶œ ëª¨ë“ˆ
ì œì•ˆì„œ ì–‘ì‹ ë˜ëŠ” ê³µê³ ë¬¸/ì²¨ë¶€ì„œë¥˜ì—ì„œ ëª©ì°¨ êµ¬ì¡° ì¶”ì¶œ

í•µì‹¬ ë…¸ë“œ í•¨ìˆ˜ë§Œ í¬í•¨ (ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ëŠ” toc_util.py ì°¸ì¡°)
"""

import json
import re
import unicodedata
from datetime import datetime
from typing import List, Dict

from ..state_types import BatchState
from .toc_util import (
    find_proposal_template,
    find_toc_table,
    parse_toc_table,
    extract_sections_from_symbols,
    create_default_toc,
    client  # OpenAI í´ë¼ì´ì–¸íŠ¸ëŠ” toc_util.pyì—ì„œ ì´ˆê¸°í™”
)


SUBSECTION_PATTERNS = [
    (re.compile(r'^ï¿­\s*(.+)$'), 1),
    (re.compile(r'^â–ª\s*(.+)$'), 1),
    (re.compile(r'^â–«\s*(.+)$'), 1),
    (re.compile(r'^[-â€“â€”]\s*(.+)$'), 1),
    (re.compile(r'^â—\s*(.+)$'), 1),
    (re.compile(r'^â—‹\s*(.+)$'), 1),
    (re.compile(r'^([0-9]{1,2})\)\s*(.+)$'), 2),
    (re.compile(r'^\(([0-9]{1,2})\)\s*(.+)$'), 2),
    (re.compile(r'^([ê°€-í£])\)\s*(.+)$'), 2),
    (re.compile(r'^\(([ê°€-í£])\)\s*(.+)$'), 2),
]


def extract_subsections_from_range(
    lines_block: List[str],
    parent_number: str,
    base_line_index: int,
    end_line_index: int,
    start_counter: int = 1
) -> List[Dict]:
    subsections = []
    counter = start_counter
    for offset, line in enumerate(lines_block):
        clean = line.strip()
        if not clean:
            continue
        for pattern, group_idx in SUBSECTION_PATTERNS:
            match = pattern.match(clean)
            if match:
                title = match.group(group_idx).strip()
                if len(title) < 2:
                    break
                subsection_number = f"{parent_number}.{counter}"
                absolute_index = base_line_index + offset
                subsections.append({
                    'number': subsection_number,
                    'title': title,
                    'required': True,
                    'level': 'sub',
                    'parent_number': parent_number,
                    'line_index': absolute_index
                })
                counter += 1
                break
    for idx, sub in enumerate(subsections):
        next_line = subsections[idx + 1]['line_index'] if idx + 1 < len(subsections) else end_line_index
        sub['next_line_index'] = next_line
    return subsections


def route_toc_extraction(state: BatchState) -> str:
    """
    ëª©ì°¨ ì¶”ì¶œ ë°©ë²• ê²°ì • (ì¡°ê±´ë¶€ ë¼ìš°íŒ…)
    
    LangGraphì—ì„œ ì‚¬ìš©í•˜ëŠ” ë¼ìš°íŒ… í•¨ìˆ˜ë¡œ, ì–‘ì‹ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬
    ì ì ˆí•œ ëª©ì°¨ ì¶”ì¶œ ë°©ë²•ì„ ì„ íƒí•©ë‹ˆë‹¤.
    
    ë™ì‘ ë°©ì‹:
    1. stateì—ì„œ attachment_templatesë¥¼ ê°€ì ¸ì˜´
    2. ì œì•ˆì„œ ì–‘ì‹ì´ ìˆëŠ”ì§€ í™•ì¸ (find_proposal_template)
    3. ì–‘ì‹ì´ ìˆìœ¼ë©´ â†’ "extract_toc_from_template" ë°˜í™˜
    4. ì–‘ì‹ì´ ì—†ìœ¼ë©´ â†’ "extract_toc_from_announcement_and_attachments" ë°˜í™˜
    
    Args:
        state: BatchState - í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ë°°ì¹˜ ìƒíƒœ
        
    Returns:
        str: ë‹¤ìŒì— ì‹¤í–‰í•  ë…¸ë“œ ì´ë¦„
        - "extract_toc_from_template": ì–‘ì‹ì—ì„œ ëª©ì°¨ ì¶”ì¶œ
        - "extract_toc_from_announcement_and_attachments": ê³µê³ +ì²¨ë¶€ì„œë¥˜ì—ì„œ ëª©ì°¨ ìœ ì¶”
    """
    templates = state.get('attachment_templates', [])
    proposal_template = find_proposal_template(templates)

    if proposal_template:
        return "extract_toc_from_template"
    else:
        return "extract_toc_from_announcement_and_attachments"


def extract_toc_from_template(state: BatchState) -> BatchState:
    """
    ì œì•ˆì„œ ì–‘ì‹ì—ì„œ ëª©ì°¨ ì¶”ì¶œ (ì²­í‚¹ + íŒ¨í„´ íŒíŠ¸ + LLM ë‹¨ì¼ ê²½ë¡œ)
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“‘ ì–‘ì‹ì—ì„œ ëª©ì°¨ ì¶”ì¶œ")
    print(f"{'='*60}")

    # ì–‘ì‹ ì°¾ê¸°
    templates = state.get('attachment_templates', [])
    template = find_proposal_template(templates)

    # [Fallback] detect_templates ë…¸ë“œê°€ ë†“ì¹œ ê²½ìš°ë¥¼ ìœ„í•œ ê¸´ê¸‰ ë³µêµ¬ ë¡œì§
    # ì •ìƒì ìœ¼ë¡œëŠ” detect_templatesì—ì„œ ì–‘ì‹ì„ ê°ì§€í•˜ì§€ë§Œ,
    # ì‹ ë¢°ë„ê°€ ë‚®ì•„ ëˆ„ë½ëœ ê²½ìš° íŒŒì¼ëª… í‚¤ì›Œë“œë¡œ ìµœì¢… ì‹œë„
    if not template:
        documents = state.get('documents', [])
        attachment_docs = [d for d in documents if d.get('folder') == 2]

        for att_doc in attachment_docs:
            file_name = att_doc.get('file_name', '')
            # ì‹ ì²­ì„œ, ê³„íšì„œ, ì œì•ˆì„œ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê°•ì œë¡œ ì‹œë„
            if any(kw in file_name for kw in ['ì‹ ì²­ì„œ', 'ê³„íšì„œ', 'ì œì•ˆì„œ', 'ì–‘ì‹']):
                print(f"\n  âš ï¸  ì–‘ì‹ ê°ì§€ ëˆ„ë½ â†’ íŒŒì¼ëª… ê¸°ë°˜ ë³µêµ¬ ì‹œë„: {file_name}")
                # ì„ì‹œ í…œí”Œë¦¿ ì •ë³´ ìƒì„±
                template = {
                    'file_name': file_name,
                    'tables': att_doc.get('tables', []),
                    'confidence_score': 0.5,  # ë‚®ì€ ì‹ ë¢°ë„ë¡œ í‘œì‹œ
                    'has_template': False  # ê°ì§€ëŠ” ì•ˆ ë˜ì—ˆì§€ë§Œ ì‹œë„
                }
                break

        if not template:
            print(f"\n  âš ï¸  ì–‘ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ â†’ ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©")
            state['table_of_contents'] = create_default_toc()
            state['status'] = 'toc_extracted'
            return state

    print(f"\n  ğŸ“‹ ì–‘ì‹: {template['file_name']}")

    tables = template.get('tables', [])

    # ì–‘ì‹ ë¬¸ì„œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    documents = state.get('documents', [])
    template_file_name = unicodedata.normalize('NFC', template['file_name'])
    template_doc = None
    
    for doc in documents:
        doc_file_name = unicodedata.normalize('NFC', doc.get('file_name', ''))
        if doc_file_name == template_file_name:
            template_doc = doc
            break
    
    # templateì— tablesê°€ ì—†ìœ¼ë©´ documentsì—ì„œ ê°€ì ¸ì˜¤ê¸°
    if not template.get('tables') and template_doc:
        template['tables'] = template_doc.get('tables', [])
    
    if not template_doc or not template_doc.get('full_text'):
        print(f"  âœ— ì–‘ì‹ í…ìŠ¤íŠ¸ ì—†ìŒ â†’ ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©")
        state['table_of_contents'] = create_default_toc()
        state['status'] = 'toc_extracted'
        return state

    full_text = template_doc.get('full_text', '')
    if not full_text:
        print(f"  âœ— ì–‘ì‹ í…ìŠ¤íŠ¸ ì—†ìŒ â†’ ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©")
        state['table_of_contents'] = create_default_toc()
        state['status'] = 'toc_extracted'
        return state

    print(f"  ğŸ¤– í˜ì´ì§€ ê¸°ë°˜ LLM ì¶”ì¶œ ì‹œì‘ (í…œí”Œë¦¿ ì „ìš©)")

    # [2025-11-19 ê°œì„ ] page_texts ìš°ì„  ì‚¬ìš© â†’ all_chunks ëŒ€ë¹„ êµ¬ì¡° ë³´ì¡´ ìš°ìˆ˜
    # page_textsëŠ” í˜ì´ì§€ë³„ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë³´ì¡´í•˜ë¯€ë¡œ ëª©ì°¨ íŒ¨í„´ ì¸ì‹ì— ìœ ë¦¬
    # all_chunksëŠ” ì´ë¯¸ ì„¹ì…˜ë³„ë¡œ ë¶„í• ëœ ì¡°ê°ì´ë¼ ì „ì²´ êµ¬ì¡° íŒŒì•…ì´ ì–´ë ¤ì›€
    page_texts = template_doc.get('page_texts', {})
    chunk_context = ""

    if page_texts:
        # page_texts ì‚¬ìš© (ê¶Œì¥): ì›ë³¸ í˜ì´ì§€ êµ¬ì¡° ë³´ì¡´
        print(f"    âœ… page_texts ì‚¬ìš©: {len(page_texts)}ê°œ í˜ì´ì§€")

        # í˜ì´ì§€ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ í…ìŠ¤íŠ¸ ê²°í•©
        sorted_pages = sorted(page_texts.items(), key=lambda x: x[0])
        page_context_parts = []

        # ìµœëŒ€ 20í˜ì´ì§€ê¹Œì§€ ì‚¬ìš© (í† í° ì ˆì•½)
        MAX_PAGES = 20
        for page_num, page_text in sorted_pages[:MAX_PAGES]:
            # ê° í˜ì´ì§€ë‹¹ ìµœëŒ€ 1500ìê¹Œì§€ ì‚¬ìš© (ëª©ì°¨ ì¶”ì¶œì— ì¶©ë¶„)
            text_snippet = page_text[:1500]
            page_context_parts.append(f"[í˜ì´ì§€ {page_num}]\n{text_snippet}")

        chunk_context = '\n\n'.join(page_context_parts)
        print(f"    ğŸ“„ í˜ì´ì§€ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(chunk_context):,}ì (ìµœëŒ€ {MAX_PAGES}í˜ì´ì§€)")

    # Fallback: page_textsê°€ ì—†ìœ¼ë©´ all_chunks ì‚¬ìš©
    if not chunk_context:
        print(f"    âš ï¸  page_texts ì—†ìŒ â†’ all_chunks fallback ì‹œë„")
        all_chunks = state.get('all_chunks', [])
        template_chunks = []
        template_file_name_nfc = unicodedata.normalize('NFC', template['file_name'])

        if all_chunks:
            for chunk in all_chunks:
                chunk_file = unicodedata.normalize('NFC', chunk.get('file_name', ''))
                if chunk_file == template_file_name_nfc:
                    template_chunks.append(chunk)

        # í˜ì´ì§€ ìˆœìœ¼ë¡œ ì •ë ¬
        template_chunks.sort(key=lambda c: (c.get('page', 0) or 0, c.get('chunk_id', '')))

        # ìƒìœ„ 20ê°œ ì²­í¬ë§Œ ì‚¬ìš©í•˜ì—¬ í† í° ì ˆì•½
        MAX_TEMPLATE_CHUNKS = 20
        chunk_context_parts = []
        for chunk in template_chunks[:MAX_TEMPLATE_CHUNKS]:
            page = chunk.get('page', '?')
            section = chunk.get('section', 'ì„¹ì…˜')
            text_snippet = chunk.get('text', '')[:800]
            chunk_context_parts.append(
                f"[í˜ì´ì§€ {page} | {section}]\n{text_snippet}"
            )

        chunk_context = '\n\n'.join(chunk_context_parts)
        print(f"    ğŸ“¦ ì²­í¬ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(chunk_context):,}ì ({len(template_chunks)}ê°œ ì²­í¬)")

    # ìµœì¢… fallback: full_text ì¼ë¶€ ì‚¬ìš©
    if not chunk_context:
        chunk_context = full_text[:5000]
        if chunk_context:
            print(f"    âš ï¸  ì²­í¬ ì—†ìŒ â†’ full_text ì¼ë¶€ ì‚¬ìš© (ê¸¸ì´ {len(chunk_context):,}ì)")
        else:
            print(f"    âœ— í…ìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸ í™•ë³´ ì‹¤íŒ¨ â†’ ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©")
            state['table_of_contents'] = create_default_toc()
            state['status'] = 'toc_extracted'
            return state
    
    # íŒ¨í„´ ê°ì§€ ê²°ê³¼ (LLMì— ì°¸ê³ ë¡œ ì œê³µ, ìŠ¤ì¼ˆë ˆí†¤ ìƒì„±ì—ë„ í™œìš©)
    toc_table = find_toc_table(tables) if tables else None
    table_sections = parse_toc_table(toc_table['data']) if toc_table else []
    
    # full_textì—ì„œ [í˜ì´ì§€ X] ë§ˆì»¤ ì œê±° (extract_sections_from_symbolsìš©)
    clean_full_text = re.sub(r'\[í˜ì´ì§€ \d+\]', '', template_doc['full_text'])
    symbol_sections = extract_sections_from_symbols(clean_full_text)
    full_lines = full_text.split('\n')
    
    # ë””ë²„ê¹…: symbol_sections ì¶”ì¶œ ê²°ê³¼ í™•ì¸
    print(f"    ğŸ” íŒ¨í„´ ê¸°ë°˜ ì„¹ì…˜ ì¶”ì¶œ: {len(symbol_sections)}ê°œ")
    if symbol_sections:
        print(f"    ğŸ“‹ ì¶”ì¶œëœ ì„¹ì…˜ (ì²« 5ê°œ):")
        for sec in symbol_sections[:5]:
            print(f"      â€¢ {sec.get('number', '')} {sec.get('title', '')} (level: {sec.get('level', 'unknown')})")
    else:
        print(f"    âš ï¸  íŒ¨í„´ ê¸°ë°˜ ì„¹ì…˜ ì¶”ì¶œ ì‹¤íŒ¨ - LLMì´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„")

    base_sections: List[Dict] = []
    section_contexts: List[Dict] = []
    total_lines = len(full_lines)
    main_sections = [sec for sec in symbol_sections if sec.get('level') == 'main']

    if main_sections:
        for main in main_sections:
            start_line = main.get('line_index', 0)
            end_line = main.get('next_line_index', total_lines)
            block_lines = full_lines[start_line:end_line]
            block_text = '\n'.join(block_lines).strip()

            main_entry = {
                'number': main['number'],
                'title': main['title'],
                'required': True,
                'level': 'main',
                'parent_number': None,
                'line_index': start_line,
                'next_line_index': end_line
            }
            if block_text:
                main_entry['excerpt'] = block_text[:800]
                section_contexts.append({
                    'number': main_entry['number'],
                    'title': main_entry['title'],
                    'excerpt': main_entry['excerpt']
                })
            base_sections.append(main_entry)

            subs = [
                sec for sec in symbol_sections
                if sec.get('level') == 'sub' and sec.get('parent_number') == main['number']
            ]
            if subs:
                subs.sort(key=lambda s: s.get('line_index', 0))
                for sub in subs:
                    sub_start = sub.get('line_index', start_line)
                    sub_end = sub.get('next_line_index', end_line)
                    sub_text = '\n'.join(full_lines[sub_start:sub_end]).strip()
                    sub_entry = {
                        'number': sub['number'],
                        'title': sub['title'],
                        'required': True,
                        'level': 'sub',
                        'parent_number': main_entry['number'],
                        'line_index': sub_start,
                        'next_line_index': sub_end
                    }
                    if sub_text:
                        sub_entry['excerpt'] = sub_text[:600]
                        section_contexts.append({
                            'number': sub_entry['number'],
                            'title': sub_entry['title'],
                            'excerpt': sub_entry['excerpt']
                        })
                    base_sections.append(sub_entry)
            else:
                sub_candidates = extract_subsections_from_range(
                    block_lines,
                    main['number'],
                    start_line,
                    end_line
                )
                for sub in sub_candidates:
                    sub_start = sub.get('line_index', start_line)
                    sub_end = sub.get('next_line_index', end_line)
                    sub_text = '\n'.join(full_lines[sub_start:sub_end]).strip()
                    sub_entry = {
                        'number': sub['number'],
                        'title': sub['title'],
                        'required': True,
                        'level': 'sub',
                        'parent_number': main_entry['number'],
                        'line_index': sub_start,
                        'next_line_index': sub_end
                    }
                    if sub_text:
                        sub_entry['excerpt'] = sub_text[:600]
                        section_contexts.append({
                            'number': sub_entry['number'],
                            'title': sub_entry['title'],
                            'excerpt': sub_entry['excerpt']
                        })
                    base_sections.append(sub_entry)

    skeleton_payload = [
        {'number': sec['number'], 'title': sec['title'], 'required': sec.get('required', True)}
        for sec in base_sections
    ]
    skeleton_json = json.dumps(skeleton_payload, ensure_ascii=False, indent=2) if base_sections else ""
    
    # [2025-11-19 ê°œì„ ] ì„¹ì…˜ ì¶”ì¶œ ìƒì„¸ ë””ë²„ê¹…
    print(f"\n    ğŸ” ì„¹ì…˜ ì¶”ì¶œ ê²°ê³¼ ìƒì„¸:")
    print(f"    ğŸ“Š base_sections: {len(base_sections)}ê°œ")
    print(f"    ğŸ“ section_contexts (ë³¸ë¬¸ ë°œì·Œ): {len(section_contexts)}ê°œ")

    # base_sectionsê°€ ë¹„ì–´ìˆìœ¼ë©´ ê²½ê³ 
    if not base_sections:
        print(f"    âš ï¸  ê²½ê³ : base_sectionsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. LLMì´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„ (í¼ í•„ë“œ í¬í•¨ ê°€ëŠ¥)")
    else:
        # ì²« 10ê°œ ì„¹ì…˜ ì¶œë ¥ (ê¸°ì¡´ 5ê°œ â†’ 10ê°œë¡œ í™•ëŒ€í•˜ì—¬ ë” ë§ì€ íŒ¨í„´ í™•ì¸)
        print(f"    ğŸ“‹ ì¶”ì¶œëœ ì„¹ì…˜ (ì²« 10ê°œ):")
        for idx, sec in enumerate(base_sections[:10], 1):
            level_icon = "â– " if sec.get('level') == 'main' else "  â—‹"
            print(f"      {level_icon} [{idx}] {sec.get('number', '')} {sec.get('title', '')}")

        # ì „ì²´ ì„¹ì…˜ ë ˆë²¨ ë¶„í¬ í™•ì¸
        main_count = sum(1 for sec in base_sections if sec.get('level') == 'main')
        sub_count = sum(1 for sec in base_sections if sec.get('level') == 'sub')
        print(f"    ğŸ“ˆ ë ˆë²¨ ë¶„í¬: main={main_count}ê°œ, sub={sub_count}ê°œ")

    # ì„¹ì…˜ë³„ ë³¸ë¬¸ ë°œì·Œ ìƒ˜í”Œ
    if section_contexts:
        print(f"\n    ğŸ“„ ì„¹ì…˜ ë³¸ë¬¸ ë°œì·Œ ìƒ˜í”Œ (ì²« 3ê°œ):")
        for ctx in section_contexts[:3]:
            excerpt_preview = ctx.get('excerpt', '')[:80]
            print(f"      â€¢ {ctx.get('number', '')} {ctx.get('title', '')}")
            print(f"        â””â”€ {excerpt_preview}...")
    
    def summarize_sections(sections: List[Dict], label: str, limit: int = 10) -> str:
        if not sections:
            return f"- {label}: ê°ì§€ë˜ì§€ ì•ŠìŒ"
        lines = [f"- {label} (ìƒìœ„ {min(len(sections), limit)}ê°œ)"]
        for sec in sections[:limit]:
            lines.append(f"  â€¢ {sec.get('number', '-')}: {sec.get('title', '')}")
        if len(sections) > limit:
            lines.append(f"  â€¢ ... ì™¸ {len(sections) - limit}ê°œ")
        return '\n'.join(lines)
    
    detected_outline = '\n'.join([
        summarize_sections(table_sections, "í‘œ ê¸°ë°˜ í›„ë³´"),
        summarize_sections(symbol_sections, "ê¸°í˜¸/íŒ¨í„´ ê¸°ë°˜ í›„ë³´")
    ])
    
    # ğŸ” í…ìŠ¤íŠ¸ ê¸°ë°˜ ëª©ì°¨ ì„¹ì…˜ ì¶”ì¶œ (ê¸°ì¡´ LLM í´ë°± ë¡œì§ í†µí•©)
    template_text = ''

    toc_start_keywords = [
        '< ë³¸ë¬¸', '<ë³¸ë¬¸', 'ë³¸ë¬¸>',
        'ì‘ì„± ëª©ì°¨', 'ì œì¶œì„œë¥˜ ëª©ì°¨', 'ê³„íšì„œ ëª©ì°¨',
        'ì‘ì„±í•­ëª©', 'ì œì¶œí•­ëª©', 'ê¸°ì¬ì‚¬í•­'
    ]
    toc_section_start = -1
    for keyword in toc_start_keywords:
        idx = full_text.find(keyword)
        if idx != -1:
            toc_section_start = idx
            print(f"    ğŸ“ ëª©ì°¨ ì‹œì‘ í‚¤ì›Œë“œ ë°œê²¬: '{keyword}' (ìœ„ì¹˜: {idx})")
            break

    if toc_section_start != -1:
        text_after_start = full_text[toc_section_start:]
        end_keywords = [
            '< ë³¸ë¬¸ 2', '<ë³¸ë¬¸ 2', 'ë³¸ë¬¸ 2>',
            'ì‘ì„±ìš”ë ¹', 'ì‘ì„± ìš”ë ¹', 'ì£¼ì˜ì‚¬í•­', 'ìœ ì˜ì‚¬í•­',
            'ì°¸ê³ ì‚¬í•­', 'ê¸°ì¬ìš”ë ¹', 'ì²¨ë¶€ì„œë¥˜',
            'â€» ì°¸ê³ ', 'ã€ì°¸ê³ ', '[ì°¸ê³ '
        ]
        toc_end = len(text_after_start)
        for end_kw in end_keywords:
            end_idx = text_after_start.find(end_kw)
            if end_idx != -1 and end_idx < toc_end:
                toc_end = end_idx
                print(f"    ğŸ“ ëª©ì°¨ ë í‚¤ì›Œë“œ ë°œê²¬: '{end_kw}' (ìƒëŒ€ ìœ„ì¹˜: {end_idx})")
                break
        template_text = text_after_start[:min(toc_end, 5000)]
        print(f"    âœ… ëª©ì°¨ ì„¹ì…˜ ì¶”ì¶œ ì™„ë£Œ (ê¸¸ì´: {len(template_text)}ì)")
    else:
        pattern = r'^[1-9]\.\s+[ê°€-í£]{2,}'
        lines = full_text.split('\n')
        toc_line_start = -1
        consecutive_numbered = 0
        for i, line in enumerate(lines):
            if re.search(pattern, line.strip()):
                if toc_line_start == -1:
                    toc_line_start = i
                consecutive_numbered += 1
                if consecutive_numbered >= 3:
                    toc_lines = lines[toc_line_start:toc_line_start + 100]
                    template_text = '\n'.join(toc_lines)[:5000]
                    print(f"    âœ… ë²ˆí˜¸ íŒ¨í„´ ê¸°ë°˜ ëª©ì°¨ ë°œê²¬ (ë¼ì¸: {toc_line_start}, ê¸¸ì´: {len(template_text)}ì)")
                    break
            else:
                consecutive_numbered = 0
        if not template_text:
            template_text = full_text[:15000]
            print(f"    âš ï¸  ëª©ì°¨ íŒ¨í„´ ë¯¸ë°œê²¬ â†’ ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš© (15000ì)")

    if not template_text:
        template_text = chunk_context

    system_prompt = """ë‹¹ì‹ ì€ ì •ë¶€ ì§€ì›ì‚¬ì—… ì‹ ì²­ì„œ/ì œì•ˆì„œ ì–‘ì‹ì„ ë¶„ì„í•˜ì—¬ ì‹¤ì œ ì‘ì„±í•´ì•¼ í•˜ëŠ” ë³¸ë¬¸ ëª©ì°¨ë¥¼ ì •ë¦¬í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

âš ï¸ ì¤‘ìš” ê·œì¹™:
1. **í¼ ì…ë ¥ í•„ë“œëŠ” ì ˆëŒ€ ëª©ì°¨ë¡œ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”:**
   - âŒ ì œì™¸: "ê¸°ì—…ëª…", "ëŒ€í‘œì", "ì—°ë½ì²˜", "ì£¼ì†Œ", "ì „í™”", "íŒ©ìŠ¤", "mail", "íœ´ëŒ€ì „í™”", "ìƒë…„ì›”ì¼", "ì„±ë³„", "ì§ìœ„", "ë¶€ì„œ" ë“±
   - âŒ ì œì™¸: "1)ê¸°ì—…í˜„í™©", "2)ëŒ€í‘œì", "3)ì‹¤ë¬´ì±…ì„ì" ê°™ì€ í¼ ì„¹ì…˜ ë²ˆí˜¸
   - âœ… í¬í•¨: "â–¡ ê¸°ì—…í˜„í™©", "â–¡ ëŒ€í‘œì ë° ê²½ì˜ì§„ í˜„í™©", "â–¡ ëª©í‘œ" ê°™ì€ ë³¸ë¬¸ ì‘ì„± ì„¹ì…˜

2. **ë³¸ë¬¸ ì‘ì„± í•­ëª©ë§Œ ëª©ì°¨ë¡œ ì¶”ì¶œ:**
   - â–¡, â– , ï¿­ë¡œ ì‹œì‘í•˜ëŠ” ì„¹ì…˜ë§Œ ì¶”ì¶œ
   - ê° ì„¹ì…˜ì€ ì‹¤ì œë¡œ ì„œìˆ í•´ì•¼ í•  ë‚´ìš©ì„ ìš”êµ¬í•˜ëŠ” í•­ëª©ì´ì–´ì•¼ í•¨

3. **ê³„ì¸µ êµ¬ì¡°:**
   - â–¡, â– , â—ëŠ” ì£¼ìš” ì„¹ì…˜ (1, 2, 3...)
   - ï¿­, â–ª, â–«, 1), (ê°€) ë“±ì€ í•˜ìœ„ ì„¹ì…˜ (1.1, 1.2...)

4. **JSON í˜•ì‹ì„ ë°˜ë“œì‹œ ì§€í‚¤ê³ , ì„¹ì…˜ì€ ìµœì†Œ 10ê°œ ì´ìƒ ì¶œë ¥í•˜ì„¸ìš”.**
"""
    if skeleton_json:
        system_prompt += "\n\nâš ï¸ ë§¤ìš° ì¤‘ìš”: ì œê³µëœ ìŠ¤ì¼ˆë ˆí†¤ì˜ number/title ìˆœì„œë¥¼ ë°˜ë“œì‹œ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³ , ìŠ¤ì¼ˆë ˆí†¤ì— ì—†ëŠ” ì„¹ì…˜ì€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”. ìŠ¤ì¼ˆë ˆí†¤ì— ìˆëŠ” ì„¹ì…˜ë§Œ ëª©ì°¨ë¡œ ë°˜í™˜í•˜ì„¸ìš”."
    else:
        system_prompt += "\n\nâš ï¸ ë§¤ìš° ì¤‘ìš”: ìŠ¤ì¼ˆë ˆí†¤ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ì—ì„œ â–¡, â– , â—ë¡œ ì‹œì‘í•˜ëŠ” ë³¸ë¬¸ ì‘ì„± ì„¹ì…˜ë§Œ ì¶”ì¶œí•˜ì„¸ìš”. í¼ ì…ë ¥ í•„ë“œ(ê¸°ì—…ëª…, ëŒ€í‘œì, ì—°ë½ì²˜, mail, íŒ©ìŠ¤, íœ´ëŒ€ì „í™” ë“±)ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."
    
    if section_contexts:
        system_prompt += "\n\nâš ï¸ ì¤‘ìš”: 'ì„¹ì…˜ë³„ ë³¸ë¬¸ ë°œì·Œ'ì— ì œê³µëœ í…ìŠ¤íŠ¸ëŠ” ê° â–¡ ì„¹ì…˜ì˜ ì‹œì‘ë¶€í„° ë‹¤ìŒ â–¡ ì„¹ì…˜ ì§ì „ê¹Œì§€ì˜ ì‹¤ì œ ì›ë¬¸ì…ë‹ˆë‹¤. ì´ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ëŒì²˜ëŸ¼ ì½ì–´ì„œ í•´ë‹¹ êµ¬ê°„ì—ì„œ ìš”êµ¬í•˜ëŠ” í•˜ìœ„ í•­ëª©(ï¿­, 1), (ê°€) ë“±)ì„ ì¶”ì¶œí•˜ê³ , ì„¹ì…˜ì˜ ì‹¤ì œ ë‚´ìš©ì„ íŒŒì•…í•˜ì—¬ ëª©ì°¨ë¥¼ êµ¬ì„±í•˜ì„¸ìš”."

    user_prompt_parts = [
        f"""## í…œí”Œë¦¿ ì •ë³´
- íŒŒì¼ëª…: {template['file_name']}
- í˜ì´ì§€ ìˆ˜: {template_doc.get('page_count', '?')}
- í‘œ ìˆ˜: {len(tables)}""",
        f"""## ì‚¬ì „ ê°ì§€ëœ ëª©ì°¨ í›„ë³´ (ì°¸ê³ ìš©)
{detected_outline}""",
        f"""## ëª©ì°¨ í…ìŠ¤íŠ¸ (í‚¤ì›Œë“œ/íŒ¨í„´ ê¸°ë°˜)
{template_text}""",
        f"""## ì²¨ë¶€ ì–‘ì‹ í…ìŠ¤íŠ¸ (page_texts ë˜ëŠ” ì²­í¬)
{chunk_context}"""
    ]

    if skeleton_json:
        user_prompt_parts.append(f"""## ê°•ì œ ëª©ì°¨ ìŠ¤ì¼ˆë ˆí†¤ (number/titleì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
{skeleton_json}""")

    if section_contexts:
        MAX_CONTEXT_SECTIONS = 20
        trimmed_contexts = []
        for ctx in section_contexts:
            excerpt = ctx.get('excerpt', '').strip()
            if not excerpt:
                continue
            trimmed_contexts.append({
                'number': ctx['number'],
                'title': ctx['title'],
                'excerpt': excerpt[:600]
            })
            if len(trimmed_contexts) >= MAX_CONTEXT_SECTIONS:
                break
        if trimmed_contexts:
            context_json = json.dumps(trimmed_contexts, ensure_ascii=False, indent=2)
            user_prompt_parts.append(f"""## ì„¹ì…˜ë³„ ë³¸ë¬¸ ë°œì·Œ (ìƒìœ„ {len(trimmed_contexts)}ê°œ)
âš ï¸ ì¤‘ìš”: ì•„ë˜ ê° ì„¹ì…˜ì˜ "excerpt"ëŠ” í•´ë‹¹ â–¡ ì„¹ì…˜ì˜ ì‹œì‘ë¶€í„° ë‹¤ìŒ â–¡ ì„¹ì…˜ ì§ì „ê¹Œì§€ì˜ ì‹¤ì œ ì›ë¬¸ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ì´ í…ìŠ¤íŠ¸ë¥¼ ì½ì–´ì„œ í•´ë‹¹ êµ¬ê°„ì—ì„œ ìš”êµ¬í•˜ëŠ” í•˜ìœ„ í•­ëª©(ï¿­, 1), (ê°€) ë“±)ê³¼ ì‹¤ì œ ì‘ì„± ë‚´ìš©ì„ íŒŒì•…í•˜ì„¸ìš”.

{context_json}""")

    user_prompt_parts.append("""---
ìš”êµ¬ ì‚¬í•­:
1. **í¼ ì…ë ¥ í•„ë“œëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”:**
   - âŒ ì œì™¸: "ê¸°ì—…ëª…", "ëŒ€í‘œì", "ì—°ë½ì²˜", "ì£¼ì†Œ", "ì „í™”", "íŒ©ìŠ¤", "mail", "íœ´ëŒ€ì „í™”", "ìƒë…„ì›”ì¼", "ì„±ë³„", "ì§ìœ„", "ë¶€ì„œ", "E-mail" ë“±
   - âŒ ì œì™¸: "1)ê¸°ì—…í˜„í™©", "2)ëŒ€í‘œì", "3)ì‹¤ë¬´ì±…ì„ì" ê°™ì€ í¼ ì„¹ì…˜ ë²ˆí˜¸
   - âœ… í¬í•¨: "â–¡ ê¸°ì—…í˜„í™©", "â–¡ ëŒ€í‘œì ë° ê²½ì˜ì§„ í˜„í™©", "â–¡ ëª©í‘œ" ê°™ì€ ë³¸ë¬¸ ì‘ì„± ì„¹ì…˜

2. ìƒê¸° í…ìŠ¤íŠ¸ì—ì„œ ë³¸ë¬¸ ì‘ì„± í•­ëª©ë§Œ ì¶”ì¶œí•˜ì—¬ ëª©ì°¨ë¥¼ ìƒì„±í•˜ì„¸ìš”.

3. "ì„¹ì…˜ë³„ ë³¸ë¬¸ ë°œì·Œ"ê°€ ì œê³µëœ ê²½ìš°, ê° ì„¹ì…˜ì˜ excerpt í…ìŠ¤íŠ¸ë¥¼ ë°˜ë“œì‹œ ì½ì–´ì„œ í•´ë‹¹ êµ¬ê°„ì˜ ì‹¤ì œ ë‚´ìš©ê³¼ í•˜ìœ„ í•­ëª©ì„ íŒŒì•…í•˜ì„¸ìš”.

4. ê³„ì¸µ êµ¬ì¡°ëŠ” "1 â†’ 1.1 â†’ 1.1.1" í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.

5. ê° í•­ëª©ì— 'required' ì—¬ë¶€ì™€ ê°„ë‹¨í•œ ì„¤ëª…ì„ í¬í•¨í•˜ì„¸ìš”.

6. ì¶œë ¥ í˜•ì‹ì€ ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¥´ì„¸ìš”:
{
  "sections": [
    {
      "number": "1",
      "title": "ì‚¬ì—… ê°œìš”",
      "required": true,
      "description": "ì‚¬ì—… ì¶”ì§„ ë°°ê²½ê³¼ ëª©ì "
    }
  ]
}""")

    user_prompt = "\n\n".join(user_prompt_parts)
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0
        )
        
        content = response.choices[0].message.content
        if not content:
            raise ValueError("LLM ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        
        result = json.loads(content)
        sections = result.get('sections', [])
        
        if not sections:
            raise ValueError("LLM ê²°ê³¼ì— sectionsê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        if base_sections:
            llm_map = {sec.get('number'): sec for sec in sections}
            final_sections = []
            for base in base_sections:
                llm_candidate = llm_map.get(base['number'], {})
                description = llm_candidate.get('description') or base.get('excerpt', '')
                if not isinstance(description, str):
                    description = str(description) if description is not None else ''
                merged = {
                    'number': base['number'],
                    'title': base['title'],
                    'required': llm_candidate.get('required', base.get('required', True)),
                    'description': description.strip()
                }
                final_sections.append(merged)
        else:
            # base_sectionsê°€ ë¹„ì–´ìˆì„ ë•Œ í¼ í•„ë“œ í•„í„°ë§
            # ========================================
            # [2025-11-19 ìˆ˜ì •] í¼ í•„ë“œ í‚¤ì›Œë“œ ì¤‘ë³µ ì œê±° ë° ë¡œì§ ê°œì„ 
            # - 'E-mail' ì¤‘ë³µ ì œê±°
            # - í•„í„°ë§ ë¡œì§ ë‹¨ìˆœí™” (ì›ë³¸ ì œëª© ê¸°ì¤€ìœ¼ë¡œ ì²´í¬)
            # ========================================
            form_field_keywords = ['mail', 'e-mail', 'ì´ë©”ì¼', 'íŒ©ìŠ¤', 'íœ´ëŒ€ì „í™”', 'ì „í™”', 'ì£¼ì†Œ', 'ìƒë…„ì›”ì¼', 'ì„±ë³„', 'ì§ìœ„', 'ë¶€ì„œ']
            final_sections = []
            for sec in sections:
                original_title = sec.get('title', '')
                title_lower = original_title.lower()

                # 1. í¼ í•„ë“œ í‚¤ì›Œë“œê°€ ì œëª©ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì œì™¸
                if any(keyword in title_lower for keyword in form_field_keywords):
                    continue

                # 2. â–¡, â– , â— ë“±ì˜ ë§ˆì»¤ê°€ ì œëª©ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                has_marker = any(marker in original_title for marker in ['â–¡', 'â– ', 'â—', 'â—‹'])

                # ë§ˆì»¤ê°€ ì—†ìœ¼ë©´ ì œì™¸ (í¼ í•„ë“œì¼ ê°€ëŠ¥ì„±)
                if not has_marker:
                    continue

                final_sections.append(sec)
            
            if not final_sections:
                print(f"    âš ï¸  í¼ í•„ë“œ í•„í„°ë§ í›„ ì„¹ì…˜ì´ ì—†ìŒ â†’ ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©")
                raise ValueError("ìœ íš¨í•œ ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        toc = {
            'source': 'template',
            'source_file': template['file_name'],
            'extraction_method': 'llm_template_chunks',
            'sections': final_sections,
            'total_sections': len(final_sections),
            'has_page_numbers': False,
            'extracted_at': datetime.now().isoformat()
        }
        
        state['table_of_contents'] = toc
        state['status'] = 'toc_extracted'
        
        print(f"\n  âœ… ì²­í‚¹ ê¸°ë°˜ LLM ì¶”ì¶œ ì™„ë£Œ: {len(final_sections)}ê°œ ì„¹ì…˜")
        for sec in final_sections[:5]:
            print(f"    â€¢ {sec.get('number', '')} {sec.get('title', '')}")
        if len(final_sections) > 5:
            print(f"    ... ì™¸ {len(final_sections) - 5}ê°œ")
        
        return state
    
    except Exception as e:
        print(f"  âœ— ì²­í‚¹ ê¸°ë°˜ LLM ì¶”ì¶œ ì‹¤íŒ¨: {e} â†’ ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©")
        state['table_of_contents'] = create_default_toc()
        state['status'] = 'toc_extracted'

    return state


def extract_toc_from_announcement_and_attachments(state: BatchState) -> BatchState:
    """
    ê³µê³ ë¬¸ + ëª¨ë“  ì²¨ë¶€ì„œë¥˜ì—ì„œ ëª©ì°¨ ìœ ì¶” (RAG + LLM) - LangGraph ë…¸ë“œ
    
    âš ï¸ ì–‘ì‹ íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ì‚¬ìš©ë˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    ê³µê³ ë¬¸ê³¼ ì²¨ë¶€ì„œë¥˜ë¥¼ ë¶„ì„í•˜ì—¬ ì œì¶œí•´ì•¼ í•  ê³„íšì„œì˜ ëª©ì°¨ë¥¼ ìœ ì¶”í•©ë‹ˆë‹¤.
    
    ğŸ” ì¶”ì¶œ ê³¼ì •:
    1. ì œì¶œì„œë¥˜ feature ì°¾ê¸°
       - ê³µê³ ë¬¸ì—ì„œ "ì œì¶œì„œë¥˜" ì„¹ì…˜ ì¶”ì¶œ
       - ì–´ë–¤ ì„œë¥˜ë¥¼ ì œì¶œí•´ì•¼ í•˜ëŠ”ì§€ í™•ì¸
       
    2. RAG ê²€ìƒ‰ (Retrieval-Augmented Generation)
       - ë²¡í„° DBì—ì„œ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
       - ê²€ìƒ‰ì–´: "ì œì¶œì„œë¥˜ ì‘ì„±í•­ëª© êµ¬ì„± ëª©ì°¨ ì œì•ˆì„œ ê³„íšì„œ..."
       - ê³µê³ ë¬¸ + ëª¨ë“  ì²¨ë¶€ì„œë¥˜ì—ì„œ ê²€ìƒ‰ (25ê°œ ì²­í¬)
       
    3. LLMìœ¼ë¡œ ëª©ì°¨ ìƒì„±
       - ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì™€ ê³µê³ ë¬¸ ë‚´ìš©ì„ GPTì— ì œê³µ
       - ê³µê³  ìœ í˜•ë³„ í‘œì¤€ ëª©ì°¨ êµ¬ì¡° ì°¸ê³ 
       - ì‹¤ì œ ê³µê³  ë‚´ìš©ì„ ë°˜ì˜í•œ ëª©ì°¨ ìƒì„±
       
    ğŸ“‹ ìƒì„±ë˜ëŠ” ëª©ì°¨:
    - ì—°êµ¬ê°œë°œ(R&D) ê³¼ì œ: ì—°êµ¬ê³„íšì„œ ëª©ì°¨
    - ì°½ì—…ì§€ì› ì‚¬ì—…: ì‚¬ì—…ê³„íšì„œ ëª©ì°¨
    - ì£¼ê´€ê¸°ê´€ ì„ ì •: ì£¼ê´€ê¸°ê´€ ì‚¬ì—…ê³„íšì„œ ëª©ì°¨
    - ê¸°íƒ€ ì§€ì›ì‚¬ì—…: í•´ë‹¹ ì‚¬ì—…ì˜ ê³„íšì„œ ëª©ì°¨
    
    Args:
        state: BatchState - í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ë°°ì¹˜ ìƒíƒœ
        
    Returns:
        BatchState: table_of_contents í•„ë“œê°€ ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
        - ì„±ê³µ ì‹œ: LLMì´ ìƒì„±í•œ ëª©ì°¨ êµ¬ì¡°
        - ì‹¤íŒ¨ ì‹œ: ê¸°ë³¸ í…œí”Œë¦¿ ëª©ì°¨ ì‚¬ìš©
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“‘ ê³µê³ ë¬¸ + ì²¨ë¶€ì„œë¥˜ ê¸°ë°˜ ëª©ì°¨ ìœ ì¶”")
    print(f"{'='*60}")

    all_features = state.get('extracted_features', [])
    collection = state.get('chroma_collection')
    
    if not collection:
        print(f"\n  âš ï¸  'chroma_collection' ì—†ìŒ â†’ ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©")
        state['table_of_contents'] = create_default_toc()
        state['status'] = 'toc_extracted'
        return state

    # 1ï¸âƒ£ ì œì¶œì„œë¥˜ feature ì°¾ê¸°
    submission_features = [
        f for f in all_features
        if isinstance(f, dict) and f.get('feature_code') == 'submission_docs'
    ]

    if not submission_features:
        print(f"\n  âš ï¸  'ì œì¶œì„œë¥˜' feature ì—†ìŒ â†’ ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©")
        state['table_of_contents'] = create_default_toc()
        state['status'] = 'toc_extracted'
        return state

    submission_content = '\n\n'.join([
        f.get('full_content', '') for f in submission_features
    ])

    # 2ï¸âƒ£ RAGë¡œ ëª¨ë“  ë¬¸ì„œ(ê³µê³ +ì²¨ë¶€) ê²€ìƒ‰ (ë³¼ë¥¨ ì¦ê°€ë¥¼ ìœ„í•´ ê²€ìƒ‰ ê²°ê³¼ í™•ëŒ€)
    try:
        # OpenAI APIë¡œ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (processing.pyì˜ extract_features_ragì™€ ë™ì¼í•œ ë°©ì‹)
        query_text = "ì œì¶œì„œë¥˜ ì‘ì„±í•­ëª© êµ¬ì„± ëª©ì°¨ ì œì•ˆì„œ ê³„íšì„œ ì‚¬ì—…ê³„íšì„œ ìš´ì˜ê³„íš"
        query_response = client.embeddings.create(
            model="text-embedding-3-small",
            input=[query_text]
        )
        query_embedding = [query_response.data[0].embedding]

        # âœ… ëª¨ë“  ë¬¸ì„œì—ì„œ ê²€ìƒ‰ (ATTACHMENT í•„í„° ì œê±°)
        results = collection.query(
            query_embeddings=query_embedding,
            n_results=25  # 15 â†’ 25ë¡œ ì¦ê°€: ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ í™•ë³´
            # where ì¡°ê±´ ì œê±° â†’ ê³µê³ ë¬¸ + ëª¨ë“  ì²¨ë¶€ì„œë¥˜ ê²€ìƒ‰
        )

        all_chunks = []
        if results and results.get('ids') and results['ids'][0]:
            ids = results['ids'][0]
            documents = results.get('documents', [[]])[0] if results.get('documents') else []
            metadatas = results.get('metadatas', [[]])[0] if results.get('metadatas') else []
            
            for i in range(len(ids)):
                if i < len(documents) and i < len(metadatas):
                    metadata = metadatas[i] if isinstance(metadatas[i], dict) else {}
                    all_chunks.append({
                        'text': documents[i] if i < len(documents) else '',
                        'file': metadata.get('file_name', 'UNKNOWN'),
                        'section': metadata.get('section', 'UNKNOWN'),
                        'doc_type': metadata.get('document_type', 'UNKNOWN')
                    })
            print(f"    âœ… RAG ê²€ìƒ‰ ì™„ë£Œ: {len(all_chunks)}ê°œ ì²­í¬ (ê³µê³  + ì²¨ë¶€ì„œë¥˜)")
    except Exception as e:
        print(f"    âœ— RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        all_chunks = []

    # 3ï¸âƒ£ LLMìœ¼ë¡œ ëª©ì°¨ ìƒì„±
    print(f"    ğŸ¤– LLMìœ¼ë¡œ ëª©ì°¨ êµ¬ì¡° ìƒì„± ì¤‘...")

    # ë¬¸ì„œ íƒ€ì…ë³„ë¡œ ì •ë¦¬ (ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ í™œìš©)
    document_context = '\n\n'.join([
        f"[{c['doc_type']} - {c['file']} - {c['section']}]\n{c['text']}"
        for c in all_chunks[:20]  # 10 â†’ 20ìœ¼ë¡œ ì¦ê°€: ë” í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
    ])

    system_prompt = """ë‹¹ì‹ ì€ ì •ë¶€ ì§€ì›ì‚¬ì—… ê³µê³  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ê³µê³ ë¬¸ê³¼ ì²¨ë¶€ì„œë¥˜ë¥¼ ë¶„ì„í•˜ì—¬ **ì‹ ì²­ ì‹œ ì œì¶œí•´ì•¼ í•˜ëŠ” ê³„íšì„œì˜ ì‘ì„± í•­ëª©(ëª©ì°¨)**ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

âš ï¸ ì¤‘ìš”: ê³µê³ ì˜ ì„±ê²©ì„ ë¨¼ì € íŒŒì•…í•˜ì„¸ìš”:
- ì—°êµ¬ê°œë°œ(R&D) ê³¼ì œ ê³µê³  â†’ ì—°êµ¬ê³„íšì„œ ëª©ì°¨
- ì°½ì—…ì§€ì› ì‚¬ì—… ê³µê³  â†’ ì‚¬ì—…ê³„íšì„œ ëª©ì°¨
- ì£¼ê´€ê¸°ê´€ ì„ ì • ê³µê³  â†’ ì£¼ê´€ê¸°ê´€ ì‚¬ì—…ê³„íšì„œ ëª©ì°¨
- ê¸°íƒ€ ì§€ì›ì‚¬ì—… ê³µê³  â†’ í•´ë‹¹ ì‚¬ì—…ì˜ ê³„íšì„œ ëª©ì°¨

âš ï¸ ë‹¤ìŒì„ êµ¬ë¶„í•´ì•¼ í•©ë‹ˆë‹¤:
- âŒ ì œì¶œ ì„œë¥˜ëª… (ì˜ˆ: "ì—°êµ¬ê³„íšì„œ", "ì‹ ì²­ì„œ", "ë™ì˜ì„œ") â†’ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- âœ… ì‘ì„± í•­ëª©/ëª©ì°¨ (ì˜ˆ: "ì‚¬ì—… ì¶”ì§„ê³„íš", "ìš´ì˜ ì „ëµ", "ì˜ˆì‚° í¸ì„±") â†’ ì´ê²ƒë§Œ í¬í•¨í•˜ì„¸ìš”

ğŸ“‹ ëª©ì°¨ ìƒì„± ìš”êµ¬ì‚¬í•­:
1. **ìµœì†Œ 10-15ê°œ ì´ìƒì˜ ì„¹ì…˜ì„ ìƒì„±**í•˜ì„¸ìš” (ë„ˆë¬´ ì ìœ¼ë©´ ì•ˆ ë©ë‹ˆë‹¤)
2. **ê³„ì¸µ êµ¬ì¡°ë¥¼ í¬í•¨**í•˜ì„¸ìš” (1, 1.1, 1.2, 2, 2.1, 2.2 ë“±)
3. ê³µê³ ì˜ ì„±ê²©ì— ë§ëŠ” **í‘œì¤€ ëª©ì°¨ êµ¬ì¡°**ë¥¼ ì°¸ê³ í•˜ë˜, ì‹¤ì œ ê³µê³  ë‚´ìš©ì„ ë°˜ì˜í•˜ì„¸ìš”

ğŸ“š ê³µê³  ìœ í˜•ë³„ í‘œì¤€ ëª©ì°¨ êµ¬ì¡° ì°¸ê³ :

ã€ì—°êµ¬ê°œë°œ(R&D) ê³¼ì œã€‘
1. ì—°êµ¬ê°œë°œê³¼ì œì˜ ê°œìš” (í•„ìˆ˜)
   1.1. ê³¼ì œì˜ í•„ìš”ì„±
   1.2. ê³¼ì œì˜ ëª©í‘œ
2. ì—°êµ¬ê°œë°œ ëª©í‘œ ë° ë‚´ìš© (í•„ìˆ˜)
   2.1. ì—°êµ¬ê°œë°œ ëª©í‘œ
   2.2. ì—°êµ¬ê°œë°œ ë‚´ìš©
   2.3. ê¸°ìˆ ì  í•´ê²°ê³¼ì œ
3. ì—°êµ¬ê°œë°œ ì¶”ì§„ì²´ê³„ ë° ì¼ì • (í•„ìˆ˜)
   3.1. ì¶”ì§„ì²´ê³„
   3.2. ì—°êµ¬ì¼ì •
   3.3. ì¸ë ¥ìš´ìš©ê³„íš
4. ì—°êµ¬ê°œë°œ ì„±ê³¼ í™œìš©ë°©ì•ˆ (í•„ìˆ˜)
   4.1. ê¸°ëŒ€íš¨ê³¼
   4.2. í™œìš©ë°©ì•ˆ
5. ì†Œìš”ì˜ˆì‚° ë° ìê¸ˆê³„íš (í•„ìˆ˜)
   5.1. ì˜ˆì‚°ê³„íš
   5.2. ìê¸ˆì¡°ë‹¬ê³„íš

ã€ì°½ì—…ì§€ì›/ì‚¬ì—…ê³„íšì„œã€‘
1. ì‚¬ì—… ê°œìš” (í•„ìˆ˜)
   1.1. ì‚¬ì—… ë°°ê²½ ë° í•„ìš”ì„±
   1.2. ì‚¬ì—… ëª©í‘œ
2. ì‚¬ì—… ëª¨ë¸ (í•„ìˆ˜)
   2.1. ì‚¬ì—… ì•„ì´í…œ
   2.2. ì‚¬ì—… ì „ëµ
3. ì¶”ì§„ ê³„íš (í•„ìˆ˜)
   3.1. ìš´ì˜ ê³„íš
   3.2. ë§ˆì¼€íŒ… ê³„íš
4. ì¡°ì§ ë° ì¸ë ¥ (í•„ìˆ˜)
   4.1. ì¡°ì§ êµ¬ì„±
   4.2. ì¸ë ¥ ìš´ì˜
5. ì¬ë¬´ ê³„íš (í•„ìˆ˜)
   5.1. ë§¤ì¶œ ê³„íš
   5.2. ìê¸ˆ ê³„íš

ã€ì£¼ê´€ê¸°ê´€/ìš´ì˜ê¸°ê´€ ì„ ì •ã€‘
1. ê¸°ê´€ ê°œìš” (í•„ìˆ˜)
   1.1. ê¸°ê´€ í˜„í™©
   1.2. ì¡°ì§ êµ¬ì„±
2. ìš´ì˜ ê³„íš (í•„ìˆ˜)
   2.1. í”„ë¡œê·¸ë¨ ê¸°íš
   2.2. ìš´ì˜ ì „ëµ
3. ì¶”ì§„ ì²´ê³„ (í•„ìˆ˜)
   3.1. ì¡°ì§ ì²´ê³„
   3.2. ì¸ë ¥ ìš´ìš©
4. ì˜ˆì‚° ë° ìê¸ˆ ê³„íš (í•„ìˆ˜)
   4.1. ì˜ˆì‚° í¸ì„±
   4.2. ìê¸ˆ ê´€ë¦¬

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ JSON ë°˜í™˜:
{
  "sections": [
    {
      "number": "1",
      "title": "ì‚¬ì—… ì¶”ì§„ ê°œìš”",
      "required": true,
      "description": "ì‚¬ì—…ì˜ ëª©ì ê³¼ í•„ìš”ì„±"
    },
    {
      "number": "1.1",
      "title": "ì‚¬ì—… ë°°ê²½ ë° í•„ìš”ì„±",
      "required": true,
      "description": "ì‚¬ì—…ì„ ì¶”ì§„í•˜ëŠ” ë°°ê²½ê³¼ í•„ìš”ì„±"
    },
    {
      "number": "2",
      "title": "ìš´ì˜ ê³„íš ë° ì „ëµ",
      "required": true,
      "description": "êµ¬ì²´ì ì¸ ìš´ì˜ ë°©ì•ˆê³¼ ì¶”ì§„ ì „ëµ"
    }
  ]
}

âš ï¸ ì¤‘ìš” ì£¼ì˜ì‚¬í•­:
- ì œì¶œ ì„œë¥˜ì˜ "ì´ë¦„"ì´ ì•„ë‹Œ, ì„œë¥˜ "ë‚´ë¶€ì˜ ì‘ì„± í•­ëª©"ì„ ì¶”ì¶œí•˜ì„¸ìš”
- ê³µê³ ì˜ ì‹¤ì œ ë‚´ìš©(ì—°êµ¬ê°œë°œ/ì°½ì—…ì§€ì›/ì£¼ê´€ê¸°ê´€ì„ ì • ë“±)ì„ ë°˜ì˜í•œ ëª©ì°¨ë¥¼ ìƒì„±í•˜ì„¸ìš”
- ì„¹ì…˜ ë²ˆí˜¸ëŠ” "1", "1.1", "1.2", "2", "ê°€" ë“± ê³„ì¸µ êµ¬ì¡° í˜•ì‹ ìœ ì§€
- requiredëŠ” í•„ìˆ˜ ì‘ì„± í•­ëª© ì—¬ë¶€
- **ë°˜ë“œì‹œ 10ê°œ ì´ìƒì˜ ì„¹ì…˜ì„ ìƒì„±**í•˜ë˜, ê³µê³  ë‚´ìš©ì— ê·¼ê±°í•˜ì—¬ ìƒì„±í•˜ì„¸ìš”"""

    # ê³µê³ ë¬¸ ì „ì²´ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ì²¨ë¶€íŒŒì¼ì´ ì—†ì„ ë•Œ ëŒ€ë¹„) - ì»¨í…ìŠ¤íŠ¸ í™•ëŒ€
    documents = state.get('documents', [])
    announcement_docs = [d for d in documents if d.get('document_type') == 'ANNOUNCEMENT']
    announcement_text = ''
    if announcement_docs:
        announcement_text = announcement_docs[0].get('text', '')[:5000]  # 3000 â†’ 5000ìœ¼ë¡œ ì¦ê°€

    submission_text_limit = submission_content[:3000]  # 2000 â†’ 3000ìœ¼ë¡œ ì¦ê°€
    document_context_limit = document_context[:4000] if document_context else '(ì²¨ë¶€ì„œë¥˜ ì—†ìŒ)'  # 2000 â†’ 4000ìœ¼ë¡œ ì¦ê°€

    user_prompt = f"""## ê³µê³ ë¬¸ ë‚´ìš©

{announcement_text}

## ì œì¶œì„œë¥˜ ìš”êµ¬ì‚¬í•­

{submission_text_limit}

## ì²¨ë¶€ì„œë¥˜ ê´€ë ¨ ë‚´ìš© (ì–‘ì‹/ê³„íšì„œì˜ ì‘ì„± í•­ëª©)

{document_context_limit}

---

## ğŸ“‹ ë¶„ì„ ë° ëª©ì°¨ ìƒì„± ì§€ì¹¨

### 1ë‹¨ê³„: ê³µê³  ì„±ê²© íŒŒì•…
ë‹¤ìŒ ì¤‘ í•´ë‹¹í•˜ëŠ” í•­ëª©ì„ íŒŒì•…í•˜ì„¸ìš”:
- [ ] ì—°êµ¬ê°œë°œ(R&D) ê³¼ì œ ê³µê³ 
- [ ] ì°½ì—…ì§€ì›/ë²¤ì²˜ ì§€ì› ì‚¬ì—… ê³µê³ 
- [ ] ì£¼ê´€ê¸°ê´€/ìš´ì˜ê¸°ê´€ ì„ ì • ê³µê³ 
- [ ] ê¸°íƒ€ ì§€ì›ì‚¬ì—… ê³µê³ 

### 2ë‹¨ê³„: ëª©ì°¨ êµ¬ì¡° ìƒì„±
ìœ„ì—ì„œ íŒŒì•…í•œ ê³µê³  ì„±ê²©ì— ë§ëŠ” í‘œì¤€ ëª©ì°¨ êµ¬ì¡°ë¥¼ ì°¸ê³ í•˜ë˜, **ê³µê³ ë¬¸ê³¼ ì²¨ë¶€ì„œë¥˜ì—ì„œ ì–¸ê¸‰ëœ êµ¬ì²´ì ì¸ ì‘ì„± í•­ëª©**ì„ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì„¸ìš”.

**âš ï¸ ë°˜ë“œì‹œ ì¤€ìˆ˜í•  ì‚¬í•­:**
1. **ìµœì†Œ 10-15ê°œ ì´ìƒì˜ ì„¹ì…˜ ìƒì„±** (ê³„ì¸µ êµ¬ì¡° í¬í•¨ ì‹œ 15-20ê°œ ì´ìƒ)
2. **ê³„ì¸µ êµ¬ì¡° í•„ìˆ˜ í¬í•¨**:
   - 1ì°¨ ì„¹ì…˜: "1", "2", "3" ë“±
   - 2ì°¨ ì„¹ì…˜: "1.1", "1.2", "2.1", "2.2" ë“±
   - 3ì°¨ ì„¹ì…˜ (í•„ìš”ì‹œ): "1.1.1", "1.1.2" ë“±
3. **ê³µê³  ë‚´ìš© ê¸°ë°˜ ìƒì„±**: í‘œì¤€ êµ¬ì¡°ë¥¼ ì°¸ê³ í•˜ë˜, ê³µê³ ë¬¸ê³¼ ì²¨ë¶€ì„œë¥˜ì—ì„œ ì‹¤ì œë¡œ ì–¸ê¸‰ëœ í•­ëª©ì„ ìš°ì„  ë°˜ì˜
4. **"ì„œë¥˜ëª…"ì´ ì•„ë‹Œ "ì‘ì„± í•­ëª©"ë§Œ ì¶”ì¶œ**:
   - âŒ ì˜ëª»ëœ ëª©ì°¨: ["ì‚¬ì—…ê³„íšì„œ", "ì‹ ì²­ì„œ", "ë™ì˜ì„œ", "ì²¨ë¶€ìë£Œ"]
   - âœ… ì˜¬ë°”ë¥¸ ëª©ì°¨: ["ì‚¬ì—… ì¶”ì§„ ê°œìš”", "ìš´ì˜ ê³„íš", "ì˜ˆì‚° í¸ì„±", "ì¶”ì§„ ì²´ê³„"]

### 3ë‹¨ê³„: êµ¬ì²´ì ì¸ ì˜ˆì‹œ

**ì—°êµ¬ê°œë°œ ê³¼ì œ ì˜ˆì‹œ:**
```
1. ì—°êµ¬ê°œë°œê³¼ì œì˜ ê°œìš”
   1.1. ê³¼ì œì˜ í•„ìš”ì„± ë° ë°°ê²½
   1.2. ê³¼ì œì˜ ëª©í‘œ
   1.3. ê¸°ëŒ€íš¨ê³¼
2. ì—°êµ¬ê°œë°œ ëª©í‘œ ë° ë‚´ìš©
   2.1. ì—°êµ¬ê°œë°œ ëª©í‘œ
   2.2. ì—°êµ¬ê°œë°œ ë‚´ìš©
   2.3. ê¸°ìˆ ì  í•´ê²°ê³¼ì œ
   2.4. í•µì‹¬ê¸°ìˆ  ìš”ì†Œ
3. ì—°êµ¬ê°œë°œ ì¶”ì§„ì²´ê³„ ë° ì¼ì •
   3.1. ì¶”ì§„ì²´ê³„
   3.2. ì—°êµ¬ì¼ì • (Gantt ì°¨íŠ¸ í¬í•¨)
   3.3. ì¸ë ¥ìš´ìš©ê³„íš
   3.4. ì—­í•  ë¶„ë‹´
4. ì—°êµ¬ê°œë°œ ì„±ê³¼ í™œìš©ë°©ì•ˆ
   4.1. ê¸°ëŒ€íš¨ê³¼
   4.2. í™œìš©ë°©ì•ˆ
   4.3. ì‚¬ì—…í™” ê³„íš
5. ì†Œìš”ì˜ˆì‚° ë° ìê¸ˆê³„íš
   5.1. ì˜ˆì‚°ê³„íš (ì—°ë„ë³„)
   5.2. ìê¸ˆì¡°ë‹¬ê³„íš
   5.3. ì˜ˆì‚° ì§‘í–‰ ê³„íš
```

**âš ï¸ ì£¼ì˜: ìœ„ ì˜ˆì‹œëŠ” ì°¸ê³ ìš©ì´ë©°, ì‹¤ì œ ê³µê³ ë¬¸ê³¼ ì²¨ë¶€ì„œë¥˜ì˜ ë‚´ìš©ì„ ë°˜ì˜í•˜ì—¬ ìƒì„±í•˜ì„¸ìš”.**

ìœ„ ë‚´ìš©ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì‹ ì²­ìê°€ ì‘ì„±í•´ì•¼ í•  ê³„íšì„œì˜ **ìƒì„¸í•œ ëª©ì°¨(ìµœì†Œ 10-15ê°œ ì„¹ì…˜, ê³„ì¸µ êµ¬ì¡° í¬í•¨)**ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”."""

    try:
        import json
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0
        )

        # JSON íŒŒì‹± (ì˜ˆì™¸ ì²˜ë¦¬)
        try:
            content = response.choices[0].message.content
            if not content:
                raise ValueError("LLM ì‘ë‹µ ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ")
            result = json.loads(content)
        except (json.JSONDecodeError, ValueError, AttributeError, IndexError) as e:
            print(f"\n  âœ— LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e} â†’ ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©")
            state['table_of_contents'] = create_default_toc()
            state['status'] = 'toc_extracted'
            return state

        if result.get('sections'):
            toc = {
                'source': 'announcement',
                'extraction_method': 'rag_llm',
                'inference_confidence': 0.7,  # RAG + LLM ê¸°ë°˜ì´ë¯€ë¡œ ì¤‘ê°„ ì‹ ë¢°ë„
                'sections': result['sections'],
                'total_sections': len(result['sections']),
                'extracted_at': datetime.now().isoformat()
            }

            state['table_of_contents'] = toc
            state['status'] = 'toc_extracted'

            print(f"\n  âœ… LLMìœ¼ë¡œ {len(result['sections'])}ê°œ ì„¹ì…˜ ìƒì„± ì™„ë£Œ")
            for sec in result['sections'][:5]:
                print(f"    â€¢ {sec.get('number', '')} {sec.get('title', '')}")
            if len(result['sections']) > 5:
                print(f"    ... ì™¸ {len(result['sections']) - 5}ê°œ")

            return state
        else:
            print(f"\n  âœ— LLM ê²°ê³¼ì— ì„¹ì…˜ ì—†ìŒ â†’ ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©")
            state['table_of_contents'] = create_default_toc()
            state['status'] = 'toc_extracted'
            return state

    except Exception as e:
        print(f"\n  âœ— LLM í˜¸ì¶œ ì‹¤íŒ¨: {e} â†’ ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©")
        state['table_of_contents'] = create_default_toc()
        state['status'] = 'toc_extracted'
        return state
