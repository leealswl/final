"""
í…ìŠ¤íŠ¸ ì¶”ì¶œ ë…¸ë“œ
âœ… ê°œì„ : ëª¨ë“  ë¬¸ì„œì— pdfplumber ì‚¬ìš© (í‘œ êµ¬ì¡° ë³´ì¡´)
âœ… ë©”ëª¨ë¦¬ ê¸°ë°˜ ì²˜ë¦¬: ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼ì—ì„œ ì§ì ‘ ì¶”ì¶œ (íŒŒì¼ ì €ì¥ ë¶ˆí•„ìš”)
"""

import pdfplumber
import io
import json
import re
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

from ..state_types import BatchState
from ..utils import extract_attachment_number


def _sanitize_filename(name: str) -> str:
    """íŒŒì¼ëª…ì„ ì•ˆì „í•œ í˜•íƒœë¡œ ë³€í™˜"""
    safe = "".join(c for c in name if c.isalnum() or c in (' ', '-', '_', '.')).rstrip()
    return safe.replace('.pdf', '').replace('.PDF', '')


SECTION_MARKERS = ['â–¡', 'â– ', 'â—', 'â—‹', 'â—‡', 'â—†', 'â–²', 'â–¼']
SUBSECTION_MARKERS = ['ï¿­', 'â–ª', 'â–«', 'â—', 'â—‹', '-']


def _normalize_page_text(text: str) -> str:
    """
    ì„¹ì…˜/ì†Œì„¹ì…˜ ê¸°í˜¸ ì•ë’¤ì— ê°œí–‰ì„ ì¶”ê°€í•´ ë¼ì¸ ê²½ê³„ë¥¼ ë³´ì¡´
    """
    if not text:
        return ""

    normalized = text
    for marker in SECTION_MARKERS + SUBSECTION_MARKERS:
        normalized = normalized.replace(marker, f"\n{marker}")

    # ë‹¤ì¤‘ ê³µë°± ì¤„ ì •ë¦¬
    normalized = re.sub(r'\n{3,}', '\n\n', normalized)
    return normalized.strip()


def export_documents_to_txt(documents: List[Dict[str, Any]], output_dir: str = "./extracted_texts") -> Path:
    """
    documents ë¦¬ìŠ¤íŠ¸ì˜ full_text / page_texts / tablesë¥¼ txt/jsonë¡œ ì €ì¥
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    print(f"\n{'='*60}")
    print(f"ğŸ“„ documents â†’ txt/json ì €ì¥")
    print(f"{'='*60}")

    for idx, doc in enumerate(documents, start=1):
        file_name = doc.get('file_name', f"document_{idx}")
        doc_type = doc.get('document_type', 'UNKNOWN')
        page_texts = doc.get('page_texts', {})
        tables = doc.get('tables', [])
        safe_filename = _sanitize_filename(file_name)

        print(f"\n  [{idx}/{len(documents)}] {file_name} ({doc_type})")

        full_text = doc.get('full_text')
        if full_text:
            full_text_path = output_path / f"{idx}_{safe_filename}_FULL.txt"
            with open(full_text_path, 'w', encoding='utf-8') as f:
                f.write(f"íŒŒì¼ëª…: {file_name}\n")
                f.write(f"ë¬¸ì„œ íƒ€ì…: {doc_type}\n")
                f.write(f"í˜ì´ì§€ ìˆ˜: {doc.get('page_count', 0)}\n")
                f.write(f"í‘œ ê°œìˆ˜: {len(tables)}\n")
                f.write(f"ì¶”ì¶œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("=" * 80 + "\n\n")
                f.write(full_text)
            print(f"    âœ“ ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥: {full_text_path.name}")

        if page_texts:
            pages_dir = output_path / f"{idx}_{safe_filename}_pages"
            pages_dir.mkdir(exist_ok=True)
            for page_num, page_text in page_texts.items():
                page_file = pages_dir / f"page_{page_num:03d}.txt"
                with open(page_file, 'w', encoding='utf-8') as f:
                    f.write(f"íŒŒì¼ëª…: {file_name}\n")
                    f.write(f"í˜ì´ì§€: {page_num}\n")
                    f.write(f"ë¬¸ì„œ íƒ€ì…: {doc_type}\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(page_text or "")
            print(f"    âœ“ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì €ì¥: {pages_dir.name} (ì´ {len(page_texts)}ê°œ)")

        if tables:
            tables_path = output_path / f"{idx}_{safe_filename}_tables.json"
            with open(tables_path, 'w', encoding='utf-8') as f:
                json.dump(tables, f, ensure_ascii=False, indent=2)
            print(f"    âœ“ í‘œ ë°ì´í„° ì €ì¥: {tables_path.name} ({len(tables)}ê°œ)")

    print(f"\n  âœ… ì €ì¥ ì™„ë£Œ: {output_path.resolve()}")
    return output_path


def extract_all_texts(state: BatchState) -> BatchState:
    """
    ëª¨ë“  íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í†µí•© ë°©ì‹)
    - pdfplumber ì‚¬ìš©: í…ìŠ¤íŠ¸ + í‘œ êµ¬ì¡° ì¶”ì¶œ
    - ê³µê³ ë¬¸/ì²¨ë¶€ êµ¬ë¶„ ì—†ì´ ë™ì¼í•œ í’ˆì§ˆ ë³´ì¥
    - ë©”ëª¨ë¦¬ ê¸°ë°˜: ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼ì—ì„œ ì§ì ‘ ì¶”ì¶œ (íŒŒì¼ ê²½ë¡œ ë¶ˆí•„ìš”)
    """
    files = state['files']
    documents = []

    print(f"\n{'='*60}")
    print(f"ğŸ“„ {len(files)}ê°œ íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘ (ë©”ëª¨ë¦¬ ê¸°ë°˜ pdfplumber)")
    print(f"{'='*60}")

    for file_idx, file_info in enumerate(files):
        # ë°”ì´íŠ¸ ë°ì´í„° ë˜ëŠ” íŒŒì¼ ê²½ë¡œ ì§€ì› (í•˜ìœ„ í˜¸í™˜ì„±)
        file_bytes = file_info.get('bytes')
        file_path = file_info.get('path')
        filename = file_info['filename']
        folder = file_info['folder']

        # ë¬¸ì„œ íƒ€ì… ê²°ì • (folder ê¸°ë°˜)
        doc_type = "ANNOUNCEMENT" if folder == 1 else "ATTACHMENT"

        print(f"\n  [{file_idx+1}/{len(files)}] {filename} ({doc_type})")

        try:
            doc_id = f"doc_{state['project_idx']}_{file_idx+1}"

            # ========== ëª¨ë“  ë¬¸ì„œ: pdfplumber ì‚¬ìš© (í‘œ + í…ìŠ¤íŠ¸) ==========
            if file_bytes:
                # ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼ì—ì„œ ì§ì ‘ ì—´ê¸° (ë©”ëª¨ë¦¬ ê¸°ë°˜)
                print(f"    ğŸ“Š ë°©ì‹: pdfplumber (ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë¦¼)")
                pdf_stream = io.BytesIO(file_bytes)
                pdf_file = pdfplumber.open(pdf_stream)
            elif file_path:
                # íŒŒì¼ ê²½ë¡œì—ì„œ ì—´ê¸° (í•˜ìœ„ í˜¸í™˜ì„±)
                print(f"    ğŸ“Š ë°©ì‹: pdfplumber (íŒŒì¼ ê²½ë¡œ)")
                pdf_file = pdfplumber.open(file_path)
            else:
                raise ValueError(f"íŒŒì¼ ì •ë³´ ë¶€ì¡±: bytes ë˜ëŠ” path í•„ìš”")

            with pdf_file as pdf:
                full_text = ""
                page_texts = {}
                all_tables = []

                for page_num, page in enumerate(pdf.pages):
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text = page.extract_text() or ""
                    text = _normalize_page_text(text)
                    full_text += f"\n[í˜ì´ì§€ {page_num + 1}]\n{text}"
                    page_texts[page_num + 1] = text

                    # í‘œ ì¶”ì¶œ
                    tables = page.extract_tables()
                    if tables:
                        for table_idx, table in enumerate(tables):
                            all_tables.append({
                                'page_number': page_num + 1,
                                'table_index': table_idx,
                                'data': table,
                                'rows': len(table),
                                'cols': len(table[0]) if table else 0
                            })

            documents.append({
                'document_id': doc_id,
                'file_name': filename,
                'file_path': file_path if file_path else None,  # ê²½ë¡œê°€ ìˆìœ¼ë©´ ì €ì¥, ì—†ìœ¼ë©´ None
                'document_type': doc_type,
                'folder': folder,
                'full_text': full_text,
                'page_texts': page_texts,
                'page_count': len(page_texts),
                'attachment_number': extract_attachment_number(filename),
                'tables': all_tables
            })

            print(f"    âœ“ ì¶”ì¶œ ì™„ë£Œ: {len(full_text):,}ì, {len(page_texts)}í˜ì´ì§€, {len(all_tables)}ê°œ í‘œ")

        except Exception as e:
            print(f"    âœ— ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            state['errors'].append(f"{filename} í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

    state['documents'] = documents
    state['status'] = 'texts_extracted'

    total_chars = sum(len(d['full_text']) for d in documents)
    total_pages = sum(d['page_count'] for d in documents)
    total_tables = sum(len(d.get('tables', [])) for d in documents)

    print(f"\n  âœ… ì´ {len(documents)}ê°œ ë¬¸ì„œ, {total_chars:,}ì, {total_pages}í˜ì´ì§€, {total_tables}ê°œ í‘œ")

    if documents:
        export_documents_to_txt(documents)

    return state
