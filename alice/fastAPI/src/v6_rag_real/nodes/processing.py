"""
ë¬¸ì„œ ì²˜ë¦¬ ë…¸ë“œë“¤ (ì²­í‚¹, ì„ë² ë”©, VectorDB ë“±)
ë…¸íŠ¸ë¶ì—ì„œ ì¶”ì¶œí•œ ì „ì²´ êµ¬í˜„
"""

import re
import json
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

# OpenAI
from openai import OpenAI
import os
from dotenv import load_dotenv

# ì„ë² ë”© & VectorDB
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

from ..state_types import BatchState
from ..config import FEATURES, RAG_SETTINGS, VECTOR_DB_DIR, CSV_OUTPUT_DIR
from ..utils import detect_section_headers, chunk_by_sections

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def chunk_all_documents(state: BatchState) -> BatchState:
    """
    ëª¨ë“  ë¬¸ì„œë¥¼ ì„¹ì…˜ ê¸°ë°˜ìœ¼ë¡œ ì²­í‚¹ (ê³µê³ ë¬¸ + ì²¨ë¶€ì„œë¥˜)
    """
    documents = state['documents']
    all_chunks = []
    chunk_global_id = 0
    
    print(f"\n{'='*60}")
    print(f"ğŸ“¦ ì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹ ì‹œì‘")
    print(f"{'='*60}")
    
    for doc in documents:
        print(f"\n  ğŸ“„ {doc['file_name']} ì²­í‚¹ ì¤‘...")
        
        doc_chunk_start = chunk_global_id
        
        # í˜ì´ì§€ë³„ë¡œ ì²­í‚¹
        for page_num, page_text in doc['page_texts'].items():
            page_chunks = chunk_by_sections(page_text, page_num)
            
            for chunk_data in page_chunks:
                all_chunks.append({
                    'chunk_id': f"{doc['document_id']}_chunk_{chunk_global_id}",
                    'text': chunk_data['text'],
                    
                    # ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
                    'project_idx': state['project_idx'],
                    'document_id': doc['document_id'],
                    'document_type': doc['document_type'],
                    'file_name': doc['file_name'],
                    
                    # ì„¹ì…˜ ì •ë³´
                    'section': chunk_data['section'],
                    'page': chunk_data['page'],
                    'is_sectioned': chunk_data['is_sectioned'],
                    
                    # ì²¨ë¶€ì„œë¥˜ ë²ˆí˜¸
                    'attachment_number': doc.get('attachment_number'),
                })
                chunk_global_id += 1
        
        doc_chunk_count = chunk_global_id - doc_chunk_start
        print(f"    âœ“ {doc_chunk_count}ê°œ ì²­í¬ ìƒì„±")
        
        # ë¬¸ì„œì— ì²­í¬ ë²”ìœ„ ì €ì¥
        doc['chunk_start_id'] = doc_chunk_start
        doc['chunk_end_id'] = chunk_global_id - 1
        doc['chunk_count'] = doc_chunk_count
    
    state['all_chunks'] = all_chunks
    state['status'] = 'all_chunked'
    
    print(f"\n  âœ… ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„± ({len(documents)}ê°œ ë¬¸ì„œ)")
    
    # í†µê³„ ì¶œë ¥
    sectioned_count = sum(1 for c in all_chunks if c['is_sectioned'])
    print(f"    - ì„¹ì…˜ ê¸°ë°˜ ì²­í¬: {sectioned_count}ê°œ")
    print(f"    - ê³ ì • ê¸¸ì´ ì²­í¬: {len(all_chunks) - sectioned_count}ê°œ")
    
    return state


def embed_all_chunks(state: BatchState) -> BatchState:
    """
    ëª¨ë“  ì²­í¬ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
    """
    all_chunks = state['all_chunks']
    
    print(f"\n{'='*60}")
    print(f"ğŸ§  ì„ë² ë”© ìƒì„± ì‹œì‘")
    print(f"{'='*60}")
    
    # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    print(f"\n  ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë”©...")
    model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
    print(f"    âœ“ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    # ì²­í¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    chunk_texts = [chunk['text'] for chunk in all_chunks]

    # ë°°ì¹˜ ì„ë² ë”© (ë°°ì¹˜ë³„ ì§„í–‰ ìƒí™© í‘œì‹œ)
    batch_size = 32  # 64 â†’ 32ë¡œ ì¤„ì—¬ì„œ ë” ìì£¼ ì§„í–‰ ìƒí™© í‘œì‹œ
    total_chunks = len(chunk_texts)
    total_batches = (total_chunks + batch_size - 1) // batch_size

    print(f"\n  ğŸ”¢ {total_chunks}ê°œ ì²­í¬ ì„ë² ë”© ì¤‘... (ë°°ì¹˜ í¬ê¸°: {batch_size}, ì´ {total_batches}ê°œ ë°°ì¹˜)")

    import numpy as np
    all_embeddings = []

    for i in range(0, total_chunks, batch_size):
        batch_num = i // batch_size + 1
        batch = chunk_texts[i:i+batch_size]

        print(f"    â³ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘... ({i+1}-{min(i+len(batch), total_chunks)}/{total_chunks} ì²­í¬)")

        batch_embeddings = model.encode(
            batch,
            show_progress_bar=False,  # ë°°ì¹˜ë³„ë¡œ ì§„í–‰ë°” ë„ê¸°
            convert_to_numpy=True
        )
        all_embeddings.append(batch_embeddings)

    embeddings = np.vstack(all_embeddings)

    state['all_embeddings'] = embeddings
    state['embedding_model'] = model
    state['status'] = 'all_embedded'

    print(f"\n  âœ… ì„ë² ë”© ì™„ë£Œ: {embeddings.shape}")
    if len(embeddings.shape) > 1:
        print(f"    - ì²­í¬ ìˆ˜: {embeddings.shape[0]}")
        print(f"    - ì°¨ì›: {embeddings.shape[1]}")
    else:
        print(f"    - ì²­í¬ ìˆ˜: {embeddings.shape[0] if embeddings.shape else 0}")
    
    return state


def init_and_store_vectordb(state: BatchState) -> BatchState:
    """
    Chroma VectorDB ì´ˆê¸°í™” ë° ì²­í¬ ì €ì¥
    """
    all_chunks = state['all_chunks']
    embeddings = state['all_embeddings']
    
    print(f"\n{'='*60}")
    print(f"ğŸ’¾ Chroma VectorDB ì´ˆê¸°í™” ë° ì €ì¥")
    print(f"{'='*60}")

    # TODO: FastAPI ì—°ë™ ì‹œ config.VECTOR_DB_DIR ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½ í•„ìš”
    # Chroma DB ê²½ë¡œ ì„¤ì • (í˜„ì¬ í…ŒìŠ¤íŠ¸ìš© í•˜ë“œì½”ë”©)
    db_path = Path("./chroma_db")
    db_path.mkdir(exist_ok=True)
    
    # Chroma Client ìƒì„±
    print(f"\n  ğŸ“‚ VectorDB ê²½ë¡œ: {db_path.absolute()}")
    client = chromadb.PersistentClient(path=str(db_path))
    
    # ì»¬ë ‰ì…˜ ì´ë¦„
    collection_name = f"project_{state['project_idx']}"
    
    # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ (ì¬ì‹¤í–‰ ì‹œ)
    try:
        client.delete_collection(name=collection_name)
        print(f"  ğŸ—‘ï¸  ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ: {collection_name}")
    except:
        pass
    
    # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
    collection = client.create_collection(
        name=collection_name,
        metadata={
            "description": "ê³µê³ ë¬¸ + ì²¨ë¶€ì„œë¥˜ í†µí•© RAG DB",
            "project_idx": state['project_idx'],
            "created_at": datetime.now().isoformat(),
            "hnsw:space": "cosine"  # Cosine distance for text similarity
        }
    )
    
    print(f"  âœ“ ì»¬ë ‰ì…˜ ìƒì„±: {collection_name}")
    
    # ì²­í¬ + ì„ë² ë”© ì €ì¥
    print(f"\n  ğŸ’¾ {len(all_chunks)}ê°œ ì²­í¬ ì €ì¥ ì¤‘...")
    
    collection.add(
        ids=[chunk['chunk_id'] for chunk in all_chunks],
        embeddings=embeddings.tolist(),
        documents=[chunk['text'] for chunk in all_chunks],
        metadatas=[
            {
                'document_id': chunk['document_id'],
                'document_type': chunk['document_type'],
                'file_name': chunk['file_name'],
                'section': chunk['section'],
                'page': chunk['page'],
                'attachment_number': chunk.get('attachment_number') or 0
            }
            for chunk in all_chunks
        ]
    )
    
    state['chroma_client'] = client
    state['chroma_collection'] = collection
    state['vector_db_path'] = str(db_path)
    state['status'] = 'vectordb_ready'
    
    print(f"  âœ… VectorDB ì €ì¥ ì™„ë£Œ")
    print(f"    - ì»¬ë ‰ì…˜: {collection_name}")
    print(f"    - ì²­í¬ ìˆ˜: {len(all_chunks)}")
    print(f"    - ê²½ë¡œ: {db_path.absolute()}")
    
    return state


# í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ragê²€ìƒ‰ì„í•´ì„œ llmì´ ë¶„ì„í•œë‹¤. 
def extract_features_rag(state: BatchState) -> BatchState:
    """
    RAG ê¸°ë°˜ Feature ì¶”ì¶œ (í¬ë¡œìŠ¤ ë¬¸ì„œ ê²€ìƒ‰)
    
    í”„ë¡œì„¸ìŠ¤:
    1. Feature ì¿¼ë¦¬ ì„ë² ë”©
    2. VectorDB ìœ ì‚¬ë„ ê²€ìƒ‰ (ê³µê³  + ì²¨ë¶€ í†µí•©)
    3. ìƒìœ„ Kê°œ chunkë§Œ LLMì— ì „ë‹¬
    4. LLM ë¶„ì„ ê²°ê³¼ ì €ì¥
    """
    collection = state['chroma_collection']
    model = state['embedding_model']
    documents = state['documents']
    
    print(f"\n{'='*60}")
    print(f"ğŸ¤– RAG ê¸°ë°˜ Feature ì¶”ì¶œ")
    print(f"{'='*60}")

    # ì „ì²´ í”„ë¡œì íŠ¸ì—ì„œ Feature ì¶”ì¶œ (ê³µê³  + ì²¨ë¶€ í†µí•© RAG ê²€ìƒ‰)
    # RAGëŠ” VectorDBì—ì„œ ëª¨ë“  ë¬¸ì„œë¥¼ í†µí•© ê²€ìƒ‰í•˜ë¯€ë¡œ FeatureëŠ” í”„ë¡œì íŠ¸ë‹¹ 1ë²ˆë§Œ ì¶”ì¶œ
    all_features = []

    print(f"\n  ğŸ“‹ ì „ì²´ í”„ë¡œì íŠ¸ì—ì„œ Feature ì¶”ì¶œ ì¤‘... (ì´ {len(FEATURES)}ê°œ)")

    for i, feature_def in enumerate(FEATURES):
        print(f"\n    [{i+1}/{len(FEATURES)}] {feature_def['feature_type']}...", end=" ")

        try:
            # 1ï¸âƒ£ Feature ì¿¼ë¦¬ ì„ë² ë”©
            # í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„: primary â†’ secondary â†’ related
            keywords = feature_def['keywords']
            if isinstance(keywords, dict):
                # ìƒˆë¡œìš´ êµ¬ì¡°: primary/secondary/related
                all_keywords = []
                all_keywords.extend(keywords.get('primary', []))
                all_keywords.extend(keywords.get('secondary', []))
                all_keywords.extend(keywords.get('related', []))
                keywords_str = " ".join(all_keywords[:5])  # ìƒìœ„ 5ê°œ
            else:
                # ì´ì „ êµ¬ì¡° í˜¸í™˜ (ë¦¬ìŠ¤íŠ¸)
                keywords_str = " ".join(keywords[:5])

            query_text = f"{feature_def['feature_type']} {keywords_str}"
            query_embedding = model.encode([query_text], convert_to_numpy=True)

            # 2ï¸âƒ£ VectorDB ìœ ì‚¬ë„ ê²€ìƒ‰
            results = collection.query(
                query_embeddings=query_embedding.tolist(),
                n_results=7,  # ìƒìœ„ 7ê°œ (ê³µê³  + ì²¨ë¶€ í¬í•¨)
                # where ì¡°ê±´ ì—†ìŒ â†’ ëª¨ë“  ë¬¸ì„œ ê²€ìƒ‰ (ê³µê³  + ì²¨ë¶€)
            )

            # ê²°ê³¼ ì—†ìŒ
            if not results['ids'][0]:
                print("âœ— (ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)")
                continue

            # 3ï¸âƒ£ ìœ ì‚¬ë„ ì„ê³„ê°’ ì²´í¬
            top_distance = results['distances'][0][0]

            if top_distance > 1.2:  # ChromaDB cosine: 0.0-2.0 range
                print(f"âœ— (ê±°ë¦¬ ë©€ìŒ: {top_distance:.3f})")
                continue

            # 4ï¸âƒ£ ê²€ìƒ‰ëœ chunk ì •ë¦¬
            retrieved_chunks = []
            for j in range(len(results['ids'][0])):
                retrieved_chunks.append({
                    'chunk_id': results['ids'][0][j],
                    'text': results['documents'][0][j],
                    'metadata': results['metadatas'][0][j],
                    'distance': results['distances'][0][j]
                })

            # 5ï¸âƒ£ ê³µê³  vs ì²¨ë¶€ ë¶„ë¦¬
            announcement_chunks = [c for c in retrieved_chunks if c['metadata']['document_type'] == 'ANNOUNCEMENT']
            attachment_chunks = [c for c in retrieved_chunks if c['metadata']['document_type'] == 'ATTACHMENT']

            # 6ï¸âƒ£ LLM ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_parts = []

            if announcement_chunks:
                context_parts.append("=== ğŸ“„ ê³µê³ ë¬¸ ê´€ë ¨ ì„¹ì…˜ ===")
                for chunk in announcement_chunks:
                    meta = chunk['metadata']
                    context_parts.append(
                        f"\n[ì„¹ì…˜: {meta['section']}, í˜ì´ì§€: {meta['page']}]\n{chunk['text']}"
                    )

            if attachment_chunks:
                context_parts.append("\n=== ğŸ“ ì²¨ë¶€ì„œë¥˜ ê´€ë ¨ ì„¹ì…˜ ===")
                for chunk in attachment_chunks:
                    meta = chunk['metadata']
                    context_parts.append(
                        f"\n[íŒŒì¼: {meta['file_name']}, ì„¹ì…˜: {meta['section']}, í˜ì´ì§€: {meta['page']}]\n{chunk['text']}"
                    )

            context_text = "\n\n---\n".join(context_parts)

            # 7ï¸âƒ£ LLM í˜¸ì¶œ
            system_prompt = f"""ë‹¹ì‹ ì€ ì •ë¶€ ì—°êµ¬ê°œë°œ ê³µê³ ë¬¸ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ê³µê³ ë¬¸ ë° ì²¨ë¶€ì„œë¥˜ì—ì„œ '{feature_def['feature_type']}'ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì„¤ëª…: {feature_def['description']}

ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:
{{
"found": true/false,
"title": "ì„¹ì…˜ ì œëª©",
"content": "ì¶”ì¶œëœ ë‚´ìš© (200ì ì´ë‚´ ìš”ì•½)",
"full_content": "ì „ì²´ ë‚´ìš©",
"key_points": ["ìš”ì 1", "ìš”ì 2"]
}}

í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ foundë¥¼ falseë¡œ ë°˜í™˜í•˜ì„¸ìš”."""

            user_prompt = f"""ê²€ìƒ‰ëœ ê´€ë ¨ ì„¹ì…˜:

{context_text}

'{feature_def['feature_type']}' ì •ë³´ë¥¼ ì°¾ì•„ JSONìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”."""

            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0
            )

            result = json.loads(response.choices[0].message.content)

            # 8ï¸âƒ£ ê²°ê³¼ ì €ì¥
            if result.get("found"):
                all_features.append({
                    'feature_code': feature_def['feature_key'],
                    'feature_name': feature_def['feature_type'],
                    'title': result.get('title', ''),
                    'summary': result.get('content', ''),
                    'full_content': result.get('full_content', ''),
                    'key_points': result.get('key_points', []),

                    # RAG ë©”íƒ€ë°ì´í„°
                    'chunks_used': [
                        {
                            'file': c['metadata']['file_name'],
                            'section': c['metadata']['section'],
                            'page': c['metadata']['page']
                        }
                        for c in retrieved_chunks
                    ],
                    'keywords_detected': all_keywords if isinstance(keywords, dict) else keywords,
                    'vector_similarity': float(top_distance),
                    'chunks_from_announcement': len(announcement_chunks),
                    'chunks_from_attachments': len(attachment_chunks),
                    'referenced_attachments': list(set(
                        c['metadata']['file_name'] for c in attachment_chunks
                    )),

                    # í”„ë¡œì íŠ¸ ì •ë³´
                    'project_idx': state['project_idx'],
                    'extracted_at': datetime.now().isoformat()
                })

                print(f"âœ“ (ê³µê³ :{len(announcement_chunks)} + ì²¨ë¶€:{len(attachment_chunks)}, ìœ ì‚¬ë„:{top_distance:.2f})")
            else:
                print("âœ— (LLM: found=false)")

        except Exception as e:
            print(f"âœ— (ì—ëŸ¬: {e})")
            state['errors'].append(f"Feature '{feature_def['feature_type']}' ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    state['extracted_features'] = all_features
    state['status'] = 'features_extracted'

    print(f"\n  ğŸ¯ ì´ {len(all_features)}ê°œ Feature ì¶”ì¶œ ì™„ë£Œ")

    return state


# ========================================
# ğŸ”– MVP2: ë¶„ì„ ëŒ€ì‹œë³´ë“œ (ê·¼ê±° ì¶”ì )
# ========================================
# ëª©ì : ê³µê³ ë¬¸ì—ì„œ "ë¶™ì„ 1 ì°¸ì¡°", "ë³„ì²¨ 2 ì°¸ì¡°" ë“±ì˜ ì–¸ê¸‰ì„ ê°ì§€í•˜ì—¬
#       í•´ë‹¹ ì²¨ë¶€ ë¬¸ì„œì™€ ìë™ ë§¤ì¹­
#       â†’ ë¶„ì„ ëŒ€ì‹œë³´ë“œì—ì„œ ì‚¬ìš©ìê°€ íŠ¹ì • ë‚´ìš©ì˜ ê·¼ê±°ë¥¼ í™•ì¸í•  ë•Œ í™œìš©
# 
# ì˜ˆì‹œ:
# - ê³µê³ ë¬¸: "ì œì¶œ ì„œë¥˜ëŠ” ë¶™ì„ 2 ì°¸ì¡°"
# - ì²¨ë¶€ë¬¸ì„œ: "ë¶™ì„2_ì—°êµ¬ê³„íšì„œì–‘ì‹.pdf"
# - ë§¤ì¹­ ê²°ê³¼: ê³µê³  íŠ¹ì • ì„¹ì…˜ â†” ì²¨ë¶€2 ì—°ê²° ì €ì¥
# ========================================

def match_cross_references(state: BatchState) -> BatchState:
    """
    ê³µê³ ë¬¸ â†” ì²¨ë¶€ì„œë¥˜ ì°¸ì¡° ìë™ ë§¤ì¹­
    
    ë°©ë²•:
    1. ê³µê³ ë¬¸ì—ì„œ "ë¶™ì„ 1", "ë³„ì²¨ 2" ë“± íŒ¨í„´ ê°ì§€
    2. VectorDBë¡œ í•´ë‹¹ ì²¨ë¶€íŒŒì¼ ê²€ìƒ‰
    3. ë§¤ì¹­ ê²°ê³¼ ì €ì¥
    """
    documents = state['documents']
    collection = state['chroma_collection']
    model = state['embedding_model']
    
    print(f"\n{'='*60}")
    print(f"ğŸ”— ì°¸ì¡° ìë™ ë§¤ì¹­")
    print(f"{'='*60}")
    
    cross_references = []
    
    # ê³µê³ ë¬¸ì—ì„œ ì°¸ì¡° íŒ¨í„´ ì°¾ê¸°
    announcement_docs = [d for d in documents if d['document_type'] == 'ANNOUNCEMENT']
    
    for ann_doc in announcement_docs:
        full_text = ann_doc['full_text']
        
        # ì°¸ì¡° íŒ¨í„´ ì¶”ì¶œ
        ref_patterns = re.findall(
            r'(ë¶™ì„|ë³„ì²¨|ì²¨ë¶€)\s*(\d+)[.\s:]*([ê°€-í£a-zA-Z\s]+)?',
            full_text
        )
        
        print(f"\n  ğŸ“„ {ann_doc['file_name']}: {len(ref_patterns)}ê°œ ì°¸ì¡° íŒ¨í„´ ë°œê²¬")
        
        for pattern in ref_patterns:
            ref_type = pattern[0]  # "ë¶™ì„"
            ref_number = int(pattern[1])  # 1
            ref_title = pattern[2].strip() if pattern[2] else ""  # "ì—°êµ¬ê³„íšì„œ ì–‘ì‹"
            
            print(f"\n    ğŸ” '{ref_type} {ref_number} {ref_title}' ë§¤ì¹­ ì¤‘...", end=" ")
            
            # ë°©ë²• 1: ì²¨ë¶€ë²ˆí˜¸ë¡œ ì§ì ‘ ë§¤ì¹­
            target_attachment = next(
                (d for d in documents 
                 if d['document_type'] == 'ATTACHMENT' 
                 and d.get('attachment_number') == ref_number),
                None
            )
            
            match_method = "NUMBER_MATCH"
            match_score = 1.0
            
            # ë°©ë²• 2: ì œëª©ìœ¼ë¡œ Vector ê²€ìƒ‰
            if not target_attachment and ref_title:
                query = f"{ref_type} {ref_number} {ref_title}"
                query_emb = model.encode([query], convert_to_numpy=True)
                
                results = collection.query(
                    query_embeddings=query_emb.tolist(),
                    n_results=3,
                    where={"document_type": "ATTACHMENT"}
                )
                
                if results['ids'][0] and results['distances'][0][0] < 0.5:
                    # ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ì˜ ë¬¸ì„œ ì°¾ê¸°
                    target_doc_id = results['metadatas'][0][0]['document_id']
                    target_attachment = next(
                        (d for d in documents if d['document_id'] == target_doc_id),
                        None
                    )
                    match_method = "VECTOR_SEARCH"
                    match_score = 1.0 - results['distances'][0][0]
            
            # ë§¤ì¹­ ì„±ê³µ
            if target_attachment:
                cross_references.append({
                    'source_document_id': ann_doc['document_id'],
                    'source_file_name': ann_doc['file_name'],
                    'target_document_id': target_attachment['document_id'],
                    'target_file_name': target_attachment['file_name'],
                    'reference_type': ref_type,
                    'reference_number': ref_number,
                    'reference_title': ref_title,
                    'match_method': match_method,
                    'match_score': match_score,
                    'created_at': datetime.now().isoformat()
                })
                
                print(f"âœ“ â†’ {target_attachment['file_name']} ({match_method}, {match_score:.2f})")
            else:
                print("âœ— (ë§¤ì¹­ ì‹¤íŒ¨)")
    
    state['cross_references'] = cross_references
    state['status'] = 'references_matched'
    
    print(f"\n  âœ… ì´ {len(cross_references)}ê°œ ì°¸ì¡° ë§¤ì¹­ ì™„ë£Œ")
    
    return state


def save_to_csv(state: BatchState) -> BatchState:
    """
    ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (ì˜¤ë¼í´ ì—°ê²° ì „ í…ŒìŠ¤íŠ¸ìš©)

    ì €ì¥ íŒŒì¼:
    1. ANALYSIS_RESULT_{timestamp}.csv - Feature ì¶”ì¶œ ê²°ê³¼ (RAG + LLM ë¶„ì„)
    2. ANALYSIS_RESULT_{timestamp}.json - Feature ì¶”ì¶œ ê²°ê³¼ (JSON)
    3. table_of_contents_{timestamp}.json - ëª©ì°¨ ì •ë³´ (JSON)
    """
    
    print(f"\n{'='*60}")
    print(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ (CSV + JSON)")
    print(f"{'='*60}")

    # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_folder = Path("./parsed_results/v6_rag")
    output_folder.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    project_idx = state['project_idx']

    output_paths = {}

    try:
        # ========================================
        # 1. ANALYSIS_RESULT.csv (Feature ì¶”ì¶œ ê²°ê³¼ë§Œ)
        # ========================================
        analysis_data = []
        analysis_json = []
        for idx, feature in enumerate(state['extracted_features'], start=1):
            result_id = idx
            analysis_json.append({
                'result_id': result_id,
                'project_idx': project_idx,
                'feature_code': feature['feature_code'],
                'feature_name': feature['feature_name'],
                'title': feature.get('title', ''),
                'summary': feature.get('summary', ''),
                'full_content': feature.get('full_content', ''),
                'key_points': feature.get('key_points', []),
                'vector_similarity': float(feature.get('vector_similarity', 0.0)),
                'chunks_from_announcement': int(feature.get('chunks_from_announcement', 0)),
                'chunks_from_attachments': int(feature.get('chunks_from_attachments', 0)),
                'referenced_attachments': feature.get('referenced_attachments', []),
                'extracted_at': feature.get('extracted_at', datetime.now().isoformat())
            })

            analysis_data.append({
                'result_id': result_id,
                'project_idx': project_idx,
                'feature_code': feature['feature_code'],
                'feature_name': feature['feature_name'],
                'title': feature.get('title', ''),
                'summary': feature.get('summary', ''),
                'full_content': feature.get('full_content', ''),
                'key_points': '|'.join(feature.get('key_points', [])),
                'vector_similarity': feature.get('vector_similarity', 0.0),
                'chunks_from_announcement': feature.get('chunks_from_announcement', 0),
                'chunks_from_attachments': feature.get('chunks_from_attachments', 0),
                'referenced_attachments': '|'.join(feature.get('referenced_attachments', [])),
                'extracted_at': feature.get('extracted_at', datetime.now().isoformat())
            })

        df_analysis = pd.DataFrame(analysis_data)
        csv_path = output_folder / f"ANALYSIS_RESULT_{project_idx}_{timestamp}.csv"
        df_analysis.to_csv(csv_path, index=False, encoding='utf-8-sig')
        output_paths['csv'] = str(csv_path)
        print(f"\n  âœ… ANALYSIS_RESULT.csv: {len(analysis_data)}í–‰")
        print(f"     â†’ {csv_path.name}")

        # ========================================
        # 2. ANALYSIS_RESULT.json (Feature ì¶”ì¶œ ê²°ê³¼)
        # ========================================
        json_result_path = output_folder / f"ANALYSIS_RESULT_{project_idx}_{timestamp}.json"
        with open(json_result_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_json, f, ensure_ascii=False, indent=2)
        output_paths['analysis_json'] = str(json_result_path)
        print(f"\n  âœ… ANALYSIS_RESULT.json: {len(analysis_json)}ê°œ í•­ëª©")
        print(f"     â†’ {json_result_path.name}")
        
        # ========================================
        # 3. table_of_contents.json (ëª©ì°¨ ì •ë³´)
        # ========================================
        toc = state.get('table_of_contents')
        if toc:
            json_path = output_folder / f"table_of_contents_{project_idx}_{timestamp}.json"

            # JSON ì €ì¥ (ë“¤ì—¬ì“°ê¸° í¬í•¨)
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(toc, f, ensure_ascii=False, indent=2)

            output_paths['json'] = str(json_path)
            print(f"\n  âœ… table_of_contents.json: {toc.get('total_sections', 0)}ê°œ ì„¹ì…˜")
            print(f"     â†’ {json_path.name}")
            print(f"     ì¶œì²˜: {toc.get('source', 'unknown')}")
        else:
            print(f"\n  âš ï¸  table_of_contents.json: ëª©ì°¨ ì—†ìŒ, ìƒì„± ìŠ¤í‚µ")

        # State ì—…ë°ì´íŠ¸
        state['csv_paths'] = output_paths
        state['status'] = 'csv_saved'

        print(f"\n  ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {output_folder.absolute()}")
        print(f"  ğŸ“Š ì´ {len(output_paths)}ê°œ íŒŒì¼ ìƒì„±")
        
    except Exception as e:
        error_msg = f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}"
        print(f"\n  âŒ {error_msg}")
        state['errors'].append(error_msg)

    return state
