"""
ë¬¸ì„œ ì²˜ë¦¬ ë…¸ë“œë“¤ (ì²­í‚¹, ì„ë² ë”©, VectorDB ë“±)

âœ… í•µì‹¬ ë…¸ë“œë“¤:
  1. chunk_all_documents: ì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹ (â–¡, â– , â— ë§ˆì»¤ ì¸ì‹)
  2. embed_all_chunks: OpenAI Embedding APIë¡œ ë²¡í„° ë³€í™˜
  3. init_and_store_vectordb: Chroma VectorDB ì €ì¥
  4. extract_features_rag: RAG ê¸°ë°˜ Feature ì¶”ì¶œ (LLM ë¶„ì„)
  5. save_to_csv: ë¡œì»¬ íŒŒì¼ ì €ì¥ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
"""

import json
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

# OpenAI
from openai import OpenAI
import os
from dotenv import load_dotenv

# ì„ë² ë”© & VectorDB
import chromadb
import numpy as np

from ..state_types import BatchState
from ..config import FEATURES, CSV_OUTPUT_DIR
from ..utils import chunk_by_sections

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def chunk_all_documents(state: BatchState) -> BatchState:
    """
    ëª¨ë“  ë¬¸ì„œë¥¼ ì„¹ì…˜ ê¸°ë°˜ìœ¼ë¡œ ì²­í‚¹ (ê³µê³ ë¬¸ + ì²¨ë¶€ì„œë¥˜)

    âœ… í•µì‹¬ ê¸°ëŠ¥: ë¬¸ì„œë¥¼ ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„(ì„¹ì…˜)ë¡œ ë¶„í• 
    ğŸ“Œ ì²­í‚¹ ì „ëµ:
      - ì„¹ì…˜ ë§ˆì»¤ ê°ì§€ (â–¡, â– , â— ë“±)
      - ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ê³ ì • ê¸¸ì´ ì²­í‚¹ (fallback)
      - MIN_CHUNK_LENGTH(50) ë¯¸ë§Œì€ ì œì™¸

    Returns:
        state['all_chunks']: ëª¨ë“  ì²­í¬ ë¦¬ìŠ¤íŠ¸
        - ì²­í¬ë§ˆë‹¤ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°, ì„¹ì…˜ ì •ë³´, í˜ì´ì§€ ë²ˆí˜¸ í¬í•¨
    """
    documents = state['documents']
    all_chunks = []
    chunk_global_id = 0

    print(f"\n{'='*60}")
    print(f"ğŸ“¦ ì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹ ì‹œì‘")
    print(f"{'='*60}")

    for doc in documents:
        print(f"\n  ğŸ“„ {doc['file_name']} ì²­í‚¹ ì¤‘...")

        doc_chunk_start = chunk_global_id

        # [2025-01-10 suyeon] page_texts íƒ€ì… ì²´í¬ ì¶”ê°€
        # ë³€ê²½ ì´ìœ :
        # 1. íƒ€ì… ì•ˆì •ì„±: dict/list ëª¨ë‘ ì²˜ë¦¬í•˜ì—¬ AttributeError ë°©ì§€
        # 2. ìœ ì—°ì„±: ë¬¸ì„œ íŒŒì‹± ë¡œì§ ë³€ê²½ ì‹œì—ë„ í˜¸í™˜
        # ê·¼ê±°: page_textsê°€ dict ë˜ëŠ” listë¡œ ë“¤ì–´ì˜¬ ìˆ˜ ìˆìŒ

        page_texts = doc.get('page_texts')
        if not page_texts:
            print(f"    âš ï¸  page_textsê°€ ì—†ìŒ - ê±´ë„ˆëœ€")
            continue

        # íƒ€ì…ì— ë”°ë¼ ìˆœíšŒ ë°©ì‹ ë¶„ê¸°
        if isinstance(page_texts, dict):
            page_items = page_texts.items()
        elif isinstance(page_texts, list):
            page_items = enumerate(page_texts, start=1)
        else:
            print(f"    âš ï¸  ì˜ëª»ëœ page_texts íƒ€ì…: {type(page_texts)} - ê±´ë„ˆëœ€")
            continue

        # í˜ì´ì§€ë³„ë¡œ ì²­í‚¹
        empty_page_count = 0
        for page_num, page_text in page_items:
            page_chunks = chunk_by_sections(page_text, page_num)

            # [2025-01-10 suyeon] ë¹ˆ í˜ì´ì§€ ì²˜ë¦¬ ê°œì„ 
            # ë³€ê²½ ì´ìœ :
            # 1. ê°€ì‹œì„±: ì²­í¬ ìƒì„± ì•ˆëœ í˜ì´ì§€ ëª…ì‹œì  ë¡œê¹…
            # 2. ë””ë²„ê¹…: ì™œ ì²­í¬ ìˆ˜ê°€ ì ì€ì§€ ì‚¬ìš©ìê°€ íŒŒì•… ê°€ëŠ¥
            # ê·¼ê±°: ë¹ˆ í˜ì´ì§€/ì§§ì€ í˜ì´ì§€ëŠ” MIN_CHUNK_LENGTH(50)ë¡œ í•„í„°ë§ë¨
            if not page_chunks:
                empty_page_count += 1
                continue

            for chunk_data in page_chunks:
                all_chunks.append({
                    'chunk_id': f"{doc['document_id']}_chunk_{chunk_global_id}",
                    'text': chunk_data['text'],
                    # ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
                    'project_idx': state['project_idx'],
                    'document_id': doc['document_id'],
                    'document_type': doc['document_type'],
                    'file_name': doc['file_name'],
                    # ì„¹ì…˜ ì •ë³´
                    'section': chunk_data['section'],
                    'page': chunk_data['page'],
                    'is_sectioned': chunk_data['is_sectioned'],
                    # ì²¨ë¶€ì„œë¥˜ ë²ˆí˜¸
                    'attachment_number': doc.get('attachment_number'),
                })
                chunk_global_id += 1

        doc_chunk_count = chunk_global_id - doc_chunk_start
        print(f"    âœ“ {doc_chunk_count}ê°œ ì²­í¬ ìƒì„±", end="")

        # ë¹ˆ í˜ì´ì§€ ê²½ê³  ì¶œë ¥
        if empty_page_count > 0:
            print(f" (âš ï¸  {empty_page_count}ê°œ í˜ì´ì§€ ê±´ë„ˆëœ€: ë¹ˆ í˜ì´ì§€ ë˜ëŠ” ë„ˆë¬´ ì§§ìŒ)")
        else:
            print()

        # ë¬¸ì„œì— ì²­í¬ ë²”ìœ„ ì €ì¥
        doc['chunk_start_id'] = doc_chunk_start
        doc['chunk_end_id'] = chunk_global_id - 1
        doc['chunk_count'] = doc_chunk_count

    state['all_chunks'] = all_chunks
    state['status'] = 'all_chunked'

    print(f"\n  âœ… ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„± ({len(documents)}ê°œ ë¬¸ì„œ)")

    # í†µê³„ ì¶œë ¥
    sectioned_count = sum(1 for c in all_chunks if c['is_sectioned'])
    print(f"    - ì„¹ì…˜ ê¸°ë°˜ ì²­í¬: {sectioned_count}ê°œ")
    print(f"    - ê³ ì • ê¸¸ì´ ì²­í¬: {len(all_chunks) - sectioned_count}ê°œ")
    return state


def embed_all_chunks(state: BatchState) -> BatchState:
    """
    OpenAI Embedding APIë¡œ ëª¨ë“  ì²­í¬ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜

    âœ… í•µì‹¬ ê¸°ëŠ¥: í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì˜ë¯¸ ê²€ìƒ‰ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦
    ğŸ“Œ ì‚¬ìš© ëª¨ë¸: text-embedding-3-small (1536 ì°¨ì›, $0.02/1M tokens)
    ğŸ“Œ ë°°ì¹˜ ì²˜ë¦¬: ìµœëŒ€ 2048ê°œ/ìš”ì²­ìœ¼ë¡œ íš¨ìœ¨ì  ì²˜ë¦¬

    Returns:
        state['all_embeddings']: numpy array (shape: [N, 1536])
        state['embedding_model']: 'text-embedding-3-small'
    """
    all_chunks = state['all_chunks']

    print(f"\n{'='*60}")
    print(f"ğŸ§  OpenAI ì„ë² ë”© ìƒì„± ì‹œì‘")
    print(f"{'='*60}")

    # ì²­í¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    chunk_texts = [chunk['text'] for chunk in all_chunks]

    # OpenAI API ë°°ì¹˜ ì„ë² ë”© (ìµœëŒ€ 2048ê°œ/ìš”ì²­)
    batch_size = 2048
    total_chunks = len(chunk_texts)
    total_batches = (total_chunks + batch_size - 1) // batch_size

    print(f"\n  ğŸ”¢ {total_chunks}ê°œ ì²­í¬ ì„ë² ë”© ì¤‘... (ë°°ì¹˜ í¬ê¸°: {batch_size}, ì´ {total_batches}ê°œ ë°°ì¹˜)")
    print(f"  ğŸ“¡ ëª¨ë¸: text-embedding-3-small (1536 ì°¨ì›)")

    all_embeddings = []

    for i in range(0, total_chunks, batch_size):
        batch_num = i // batch_size + 1
        batch = chunk_texts[i:i+batch_size]

        print(f"    â³ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘... ({i+1}-{min(i+len(batch), total_chunks)}/{total_chunks} ì²­í¬)")

        try:
            response = client.embeddings.create(
                model="text-embedding-3-small",  # 1536 ì°¨ì›, $0.02/1M tokens
                input=batch
            )

            # ì„ë² ë”© ì¶”ì¶œ
            batch_embeddings = [item.embedding for item in response.data]
            all_embeddings.extend(batch_embeddings)

        except Exception as e:
            print(f"    âŒ ë°°ì¹˜ {batch_num} ì„ë² ë”© ì‹¤íŒ¨: {str(e)}")
            state['errors'].append(f"ì„ë² ë”© ë°°ì¹˜ {batch_num} ì‹¤íŒ¨: {str(e)}")
            # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” 0 ë²¡í„°ë¡œ ì±„ì›€
            all_embeddings.extend([[0.0] * 1536 for _ in batch])

    embeddings = np.array(all_embeddings)

    state['all_embeddings'] = embeddings
    state['embedding_model'] = 'text-embedding-3-small'  # API ëª¨ë¸ëª… ì €ì¥
    state['status'] = 'all_embedded'

    print(f"\n  âœ… ì„ë² ë”© ì™„ë£Œ: {embeddings.shape}")
    if len(embeddings.shape) > 1:
        print(f"    - ì²­í¬ ìˆ˜: {embeddings.shape[0]}")
        print(f"    - ì°¨ì›: {embeddings.shape[1]}")
    else:
        print(f"    - ì²­í¬ ìˆ˜: {embeddings.shape[0] if embeddings.shape else 0}")
    return state


def init_and_store_vectordb(state: BatchState) -> BatchState:
    """
    Chroma VectorDB ì´ˆê¸°í™” ë° ì²­í¬ ì €ì¥

    âœ… í•µì‹¬ ê¸°ëŠ¥: RAG ê²€ìƒ‰ì„ ìœ„í•œ ë²¡í„° DB ìƒì„± ë° ì €ì¥ (í•„ìˆ˜)
    """
    all_chunks = state['all_chunks']
    embeddings = state['all_embeddings']

    print(f"\n{'='*60}")
    print(f"ğŸ’¾ Chroma VectorDB ì´ˆê¸°í™” ë° ì €ì¥")
    print(f"{'='*60}")

    # Chroma DB ê²½ë¡œ ì„¤ì •
    # TODO: ìš´ì˜ í™˜ê²½ì—ì„œëŠ” config.VECTOR_DB_DIR ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ ì‚¬ìš© ê¶Œì¥
    db_path = Path("./chroma_db")
    db_path.mkdir(exist_ok=True)

    # Chroma Client ìƒì„±
    print(f"\n  ğŸ“‚ VectorDB ê²½ë¡œ: {db_path.absolute()}")
    client = chromadb.PersistentClient(path=str(db_path))

    # ì»¬ë ‰ì…˜ ì´ë¦„
    collection_name = f"project_{state['project_idx']}"

    # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ (ì¬ì‹¤í–‰ ì‹œ ì¤‘ë³µ ë°©ì§€)
    try:
        client.delete_collection(name=collection_name)
        print(f"  ğŸ—‘ï¸  ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ: {collection_name}")
    except:
        pass

    # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
    collection = client.create_collection(
        name=collection_name,
        metadata={
            "description": "ê³µê³ ë¬¸ + ì²¨ë¶€ì„œë¥˜ í†µí•© RAG DB",
            "project_idx": state['project_idx'],
            "created_at": datetime.now().isoformat(),
            "hnsw:space": "cosine"  # Cosine distance for text similarity
        }
    )

    print(f"  âœ“ ì»¬ë ‰ì…˜ ìƒì„±: {collection_name}")

    # ì²­í¬ + ì„ë² ë”© ì €ì¥
    print(f"\n  ğŸ’¾ {len(all_chunks)}ê°œ ì²­í¬ ì €ì¥ ì¤‘...")

    collection.add(
        ids=[chunk['chunk_id'] for chunk in all_chunks],
        embeddings=embeddings.tolist(),
        documents=[chunk['text'] for chunk in all_chunks],
        metadatas=[
            {
                'document_id': chunk['document_id'],
                'document_type': chunk['document_type'],
                'file_name': chunk['file_name'],
                'section': chunk['section'],
                'page': chunk['page'],
                'attachment_number': chunk.get('attachment_number') or 0
            }
            for chunk in all_chunks
        ]
    )

    state['chroma_client'] = client
    state['chroma_collection'] = collection
    state['vector_db_path'] = str(db_path)
    state['status'] = 'vectordb_ready'

    print(f"  âœ… VectorDB ì €ì¥ ì™„ë£Œ")
    print(f"    - ì»¬ë ‰ì…˜: {collection_name}")
    print(f"    - ì²­í¬ ìˆ˜: {len(all_chunks)}")
    print(f"    - ê²½ë¡œ: {db_path.absolute()}")

    return state


def extract_features_rag(state: BatchState) -> BatchState:
    """
    RAG ê¸°ë°˜ Feature ì¶”ì¶œ (í¬ë¡œìŠ¤ ë¬¸ì„œ ê²€ìƒ‰)

    âœ… í•µì‹¬ ê¸°ëŠ¥: ê³µê³ ë¬¸ê³¼ ì²¨ë¶€ì„œë¥˜ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
    ğŸ“Œ RAG í”„ë¡œì„¸ìŠ¤:
      1. Feature í‚¤ì›Œë“œë¡œ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
      2. VectorDB ìœ ì‚¬ë„ ê²€ìƒ‰ (ê³µê³  + ì²¨ë¶€ í†µí•©, ìƒìœ„ 7ê°œ)
      3. ê²€ìƒ‰ëœ ì²­í¬ë§Œ LLMì— ì „ë‹¬ (í† í° ì ˆì•½)
      4. LLMì´ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ ë°˜í™˜

    ğŸ“‹ ì¶”ì¶œ ì •ë³´:
      - í•µì‹¬ ë‚´ìš© ìš”ì•½
      - key_points (ìš”ì  ë¦¬ìŠ¤íŠ¸)
      - writing_strategy (ì‘ì„± ì „ëµ - í‰ê°€ í¬ì¸íŠ¸, ì‘ì„± íŒ, ì£¼ì˜ì‚¬í•­)

    Returns:
        state['extracted_features']: ì¶”ì¶œëœ Feature ë¦¬ìŠ¤íŠ¸
        - feature_code, feature_name, summary, full_content
        - key_points, writing_strategy
        - RAG ë©”íƒ€ë°ì´í„° (ì‚¬ìš©ëœ ì²­í¬, ìœ ì‚¬ë„ ë“±)
    """
    collection = state['chroma_collection']
    model = state['embedding_model']
    documents = state['documents']
    
    print(f"\n{'='*60}")
    print(f"ğŸ¤– RAG ê¸°ë°˜ Feature ì¶”ì¶œ")
    print(f"{'='*60}")

    # ì „ì²´ í”„ë¡œì íŠ¸ì—ì„œ Feature ì¶”ì¶œ (ê³µê³  + ì²¨ë¶€ í†µí•© RAG ê²€ìƒ‰)
    # RAGëŠ” VectorDBì—ì„œ ëª¨ë“  ë¬¸ì„œë¥¼ í†µí•© ê²€ìƒ‰í•˜ë¯€ë¡œ FeatureëŠ” í”„ë¡œì íŠ¸ë‹¹ 1ë²ˆë§Œ ì¶”ì¶œ
    all_features = []

    print(f"\n  ğŸ“‹ ì „ì²´ í”„ë¡œì íŠ¸ì—ì„œ Feature ì¶”ì¶œ ì¤‘... (ì´ {len(FEATURES)}ê°œ)")

    for i, feature_def in enumerate(FEATURES):
        print(f"\n    [{i+1}/{len(FEATURES)}] {feature_def['feature_type']}...", end=" ")

        try:
            # 1ï¸âƒ£ Feature ì¿¼ë¦¬ ì„ë² ë”©
            # í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„: primary â†’ secondary â†’ related
            keywords = feature_def['keywords']
            if isinstance(keywords, dict):
                # ìƒˆë¡œìš´ êµ¬ì¡°: primary/secondary/related
                all_keywords = []
                all_keywords.extend(keywords.get('primary', []))
                all_keywords.extend(keywords.get('secondary', []))
                all_keywords.extend(keywords.get('related', []))
                keywords_str = " ".join(all_keywords[:5])  # ìƒìœ„ 5ê°œ
            else:
                # ì´ì „ êµ¬ì¡° í˜¸í™˜ (ë¦¬ìŠ¤íŠ¸)
                keywords_str = " ".join(keywords[:5])

            query_text = f"{feature_def['feature_type']} {keywords_str}"

            # OpenAI APIë¡œ ì¿¼ë¦¬ ì„ë² ë”©
            query_response = client.embeddings.create(
                model="text-embedding-3-small",
                input=[query_text]
            )
            query_embedding = [query_response.data[0].embedding]

            # 2ï¸âƒ£ VectorDB ìœ ì‚¬ë„ ê²€ìƒ‰
            results = collection.query(
                query_embeddings=query_embedding,
                n_results=7,  # ìƒìœ„ 7ê°œ (ê³µê³  + ì²¨ë¶€ í¬í•¨)
                # where ì¡°ê±´ ì—†ìŒ â†’ ëª¨ë“  ë¬¸ì„œ ê²€ìƒ‰ (ê³µê³  + ì²¨ë¶€)
            )

            # ê²°ê³¼ ì—†ìŒ
            if not results['ids'][0]:
                print("âœ— (ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)")
                continue

            # 3ï¸âƒ£ ìœ ì‚¬ë„ ì„ê³„ê°’ ì²´í¬
            top_distance = results['distances'][0][0]

            if top_distance > 1.2:  # ChromaDB cosine: 0.0-2.0 range
                print(f"âœ— (ê±°ë¦¬ ë©€ìŒ: {top_distance:.3f})")
                continue

            # 4ï¸âƒ£ ê²€ìƒ‰ëœ chunk ì •ë¦¬
            retrieved_chunks = []
            for j in range(len(results['ids'][0])):
                retrieved_chunks.append({
                    'chunk_id': results['ids'][0][j],
                    'text': results['documents'][0][j],
                    'metadata': results['metadatas'][0][j],
                    'distance': results['distances'][0][j]
                })

            # 5ï¸âƒ£ ê³µê³  vs ì²¨ë¶€ ë¶„ë¦¬
            announcement_chunks = [c for c in retrieved_chunks if c['metadata']['document_type'] == 'ANNOUNCEMENT']
            attachment_chunks = [c for c in retrieved_chunks if c['metadata']['document_type'] == 'ATTACHMENT']

            # 6ï¸âƒ£ LLM ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_parts = []

            if announcement_chunks:
                context_parts.append("=== ğŸ“„ ê³µê³ ë¬¸ ê´€ë ¨ ì„¹ì…˜ ===")
                for chunk in announcement_chunks:
                    meta = chunk['metadata']
                    context_parts.append(
                        f"\n[ì„¹ì…˜: {meta['section']}, í˜ì´ì§€: {meta['page']}]\n{chunk['text']}"
                    )

            if attachment_chunks:
                context_parts.append("\n=== ğŸ“ ì²¨ë¶€ì„œë¥˜ ê´€ë ¨ ì„¹ì…˜ ===")
                for chunk in attachment_chunks:
                    meta = chunk['metadata']
                    context_parts.append(
                        f"\n[íŒŒì¼: {meta['file_name']}, ì„¹ì…˜: {meta['section']}, í˜ì´ì§€: {meta['page']}]\n{chunk['text']}"
                    )

            context_text = "\n\n---\n".join(context_parts)

            # 7ï¸âƒ£ LLM í˜¸ì¶œ
            system_prompt = f"""ë‹¹ì‹ ì€ ì •ë¶€ R&D ì‚¬ì—…ê³„íšì„œ ì‘ì„± ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.
ê³µê³ ë¬¸ ë° ì²¨ë¶€ì„œë¥˜ë¥¼ ë¶„ì„í•˜ì—¬ '{feature_def['feature_type']}'ì— ëŒ€í•œ ì‹¤ì§ˆì ì¸ ì‘ì„± ì „ëµì„ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.

[ë¶„ì„ ëŒ€ìƒ]
- Feature: {feature_def['feature_type']}
- ì„¤ëª…: {feature_def['description']}

ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:
{{
  "found": true/false,
  "title": "ì„¹ì…˜ ì œëª©",
  "content": "ì¶”ì¶œëœ í•µì‹¬ ë‚´ìš© ìš”ì•½ (200ì ì´ë‚´)",
  "full_content": "ì „ì²´ ë‚´ìš©",
  "key_points": ["í•µì‹¬ ìš”ì  1", "í•µì‹¬ ìš”ì  2"],
  "writing_strategy": {{
    "overview": "ì´ ì„¹ì…˜ ì‘ì„± ì‹œ í‰ê°€ìœ„ì›ì´ ì¤‘ìš”í•˜ê²Œ ë³´ëŠ” í•µì‹¬ í¬ì¸íŠ¸ (2-3ë¬¸ì¥)",
    "writing_tips": ["íš¨ê³¼ì ì¸ ì‘ì„± íŒ 1", "íš¨ê³¼ì ì¸ ì‘ì„± íŒ 2", "íš¨ê³¼ì ì¸ ì‘ì„± íŒ 3"],
    "common_mistakes": ["ìì£¼ ë°œìƒí•˜ëŠ” ì‹¤ìˆ˜ 1", "í”¼í•´ì•¼ í•  ì˜¤ë¥˜ 2"],
    "example_phrases": ["ì¢‹ì€ ì‘ì„± ì˜ˆì‹œ ë¬¸êµ¬ 1", "ì¢‹ì€ ì‘ì„± ì˜ˆì‹œ ë¬¸êµ¬ 2"]
  }}
}}

**í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ foundë¥¼ falseë¡œ ë°˜í™˜í•˜ì„¸ìš”.**"""

            user_prompt = f"""ê²€ìƒ‰ëœ ê´€ë ¨ ì„¹ì…˜:

{context_text}

'{feature_def['feature_type']}' ì •ë³´ë¥¼ ì°¾ì•„ JSONìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”."""

            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0
            )

            result = json.loads(response.choices[0].message.content)

            # 8ï¸âƒ£ ê²°ê³¼ ì €ì¥
            if result.get("found"):
                all_features.append({
                    'feature_code': feature_def['feature_key'],
                    'feature_name': feature_def['feature_type'],
                    'title': result.get('title', ''),
                    'summary': result.get('content', ''),
                    'full_content': result.get('full_content', ''),
                    'key_points': result.get('key_points', []),
                    'writing_strategy': result.get('writing_strategy', {}),  # âœ… ì‘ì„± ì „ëµ ì¶”ê°€

                    # RAG ë©”íƒ€ë°ì´í„°
                    'chunks_used': [
                        {
                            'file': c['metadata']['file_name'],
                            'section': c['metadata']['section'],
                            'page': c['metadata']['page']
                        }
                        for c in retrieved_chunks
                    ],
                    'keywords_detected': all_keywords if isinstance(keywords, dict) else keywords,
                    'vector_similarity': float(top_distance),
                    'chunks_from_announcement': len(announcement_chunks),
                    'chunks_from_attachments': len(attachment_chunks),
                    'referenced_attachments': list(set(
                        c['metadata']['file_name'] for c in attachment_chunks
                    )),

                    # í”„ë¡œì íŠ¸ ì •ë³´
                    'project_idx': state['project_idx'],
                    'extracted_at': datetime.now().isoformat()
                })

                print(f"âœ“ (ê³µê³ :{len(announcement_chunks)} + ì²¨ë¶€:{len(attachment_chunks)}, ìœ ì‚¬ë„:{top_distance:.2f})")
            else:
                print("âœ— (LLM: found=false)")

        except Exception as e:
            print(f"âœ— (ì—ëŸ¬: {e})")
            state['errors'].append(f"Feature '{feature_def['feature_type']}' ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    state['extracted_features'] = all_features
    state['status'] = 'features_extracted'

    print(f"\n  ğŸ¯ ì´ {len(all_features)}ê°œ Feature ì¶”ì¶œ ì™„ë£Œ")

    return state


# ========================================
# [2025-01-10 suyeon] match_cross_references í•¨ìˆ˜ ì‚­ì œ
# ì‚­ì œ ì´ìœ :
# 1. í˜„ì¬ ë¯¸ì‚¬ìš©: graph.pyì—ì„œ ë…¸ë“œë¡œ ë“±ë¡ë˜ì§€ ì•ŠìŒ (ì£¼ì„ ì²˜ë¦¬ë¨)
# 2. MVP2 ì¬êµ¬í˜„ ì˜ˆì •: í˜„ì¬ ì½”ë“œëŠ” ì°¸ê³ ìš©ì´ì—ˆìœ¼ë‚˜ Git íˆìŠ¤í† ë¦¬ì— ë³´ì¡´
# 3. ì½”ë“œë² ì´ìŠ¤ ê°„ì†Œí™”: 115ì¤„ ì‚­ì œë¡œ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
# ê·¼ê±°: MVP2ì—ì„œ ë¶„ì„ ëŒ€ì‹œë³´ë“œ êµ¬í˜„ ì‹œ ìƒˆë¡œìš´ êµ¬ì¡°ë¡œ ì¬ì‘ì„± ì˜ˆì •


def save_to_csv(state: BatchState) -> BatchState:
    """
    ë¶„ì„ ê²°ê³¼ë¥¼ ë¡œì»¬ íŒŒì¼ë¡œ ì €ì¥ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)

    âš ï¸ ìš´ì˜ í™˜ê²½: Backend API í˜¸ì¶œ(build_response)ì´ Oracle DB ì €ì¥ì„ ë‹´ë‹¹
    ğŸ“ ë¡œì»¬ ì €ì¥: ê°œë°œ ì¤‘ ë””ë²„ê¹…, í…ŒìŠ¤íŠ¸ ê²°ê³¼ í™•ì¸ìš©

    ì €ì¥ íŒŒì¼:
    1. ANALYSIS_RESULT_{timestamp}.csv - Feature ì¶”ì¶œ ê²°ê³¼ (RAG + LLM ë¶„ì„)
    2. ANALYSIS_RESULT_{timestamp}.json - Feature ì¶”ì¶œ ê²°ê³¼ (JSON)
    3. table_of_contents_{timestamp}.json - ëª©ì°¨ ì •ë³´ (JSON)
    """

    print(f"\n{'='*60}")
    print(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ ë¡œì»¬ ì €ì¥ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)")
    print(f"{'='*60}")

    # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_folder = Path("./parsed_results/v6_rag")
    output_folder.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    project_idx = state['project_idx']

    output_paths = {}

    try:
        # ========================================
        # 1. ANALYSIS_RESULT.csv (Feature ì¶”ì¶œ ê²°ê³¼ë§Œ)
        # ========================================
        analysis_data = []
        analysis_json = []
        for idx, feature in enumerate(state['extracted_features'], start=1):
            result_id = idx
            analysis_json.append({
                'result_id': result_id,
                'project_idx': project_idx,
                'feature_code': feature['feature_code'],
                'feature_name': feature['feature_name'],
                'title': feature.get('title', ''),
                'summary': feature.get('summary', ''),
                'full_content': feature.get('full_content', ''),
                'key_points': feature.get('key_points', []),
                'writing_strategy': feature.get('writing_strategy', {}),  # âœ… ì‘ì„± ì „ëµ ì¶”ê°€
                'vector_similarity': float(feature.get('vector_similarity', 0.0)),
                'chunks_from_announcement': int(feature.get('chunks_from_announcement', 0)),
                'chunks_from_attachments': int(feature.get('chunks_from_attachments', 0)),
                'referenced_attachments': feature.get('referenced_attachments', []),
                'extracted_at': feature.get('extracted_at', datetime.now().isoformat())
            })

            analysis_data.append({
                'result_id': result_id,
                'project_idx': project_idx,
                'feature_code': feature['feature_code'],
                'feature_name': feature['feature_name'],
                'title': feature.get('title', ''),
                'summary': feature.get('summary', ''),
                'full_content': feature.get('full_content', ''),
                'key_points': '|'.join(feature.get('key_points', [])),
                'writing_strategy': json.dumps(feature.get('writing_strategy', {}), ensure_ascii=False),  # âœ… JSON ë¬¸ìì—´ë¡œ ì €ì¥
                'vector_similarity': feature.get('vector_similarity', 0.0),
                'chunks_from_announcement': feature.get('chunks_from_announcement', 0),
                'chunks_from_attachments': feature.get('chunks_from_attachments', 0),
                'referenced_attachments': '|'.join(feature.get('referenced_attachments', [])),
                'extracted_at': feature.get('extracted_at', datetime.now().isoformat())
            })

        df_analysis = pd.DataFrame(analysis_data)
        csv_path = output_folder / f"ANALYSIS_RESULT_{project_idx}_{timestamp}.csv"
        df_analysis.to_csv(csv_path, index=False, encoding='utf-8-sig')
        output_paths['csv'] = str(csv_path)
        print(f"\n  âœ… ANALYSIS_RESULT.csv: {len(analysis_data)}í–‰")
        print(f"     â†’ {csv_path.name}")

        # ========================================
        # 2. ANALYSIS_RESULT.json (Feature ì¶”ì¶œ ê²°ê³¼)
        # ========================================
        json_result_path = output_folder / f"ANALYSIS_RESULT_{project_idx}_{timestamp}.json"
        with open(json_result_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_json, f, ensure_ascii=False, indent=2)
        output_paths['analysis_json'] = str(json_result_path)
        print(f"\n  âœ… ANALYSIS_RESULT.json: {len(analysis_json)}ê°œ í•­ëª©")
        print(f"     â†’ {json_result_path.name}")
        
        # ========================================
        # 3. table_of_contents.json (ëª©ì°¨ ì •ë³´)
        # ========================================
        toc = state.get('table_of_contents')
        if toc:
            json_path = output_folder / f"table_of_contents_{project_idx}_{timestamp}.json"

            # JSON ì €ì¥ (ë“¤ì—¬ì“°ê¸° í¬í•¨)
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(toc, f, ensure_ascii=False, indent=2)

            output_paths['json'] = str(json_path)
            print(f"\n  âœ… table_of_contents.json: {toc.get('total_sections', 0)}ê°œ ì„¹ì…˜")
            print(f"     â†’ {json_path.name}")
            print(f"     ì¶œì²˜: {toc.get('source', 'unknown')}")
        else:
            print(f"\n  âš ï¸  table_of_contents.json: ëª©ì°¨ ì—†ìŒ, ìƒì„± ìŠ¤í‚µ")

        # State ì—…ë°ì´íŠ¸
        state['csv_paths'] = output_paths
        state['status'] = 'csv_saved'

        print(f"\n  ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {output_folder.absolute()}")
        print(f"  ğŸ“Š ì´ {len(output_paths)}ê°œ íŒŒì¼ ìƒì„±")
        
    except Exception as e:
        error_msg = f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}"
        print(f"\n  âŒ {error_msg}")
        state['errors'].append(error_msg)

    return state
