# law_rag.py
from pathlib import Path
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough

import os
from dotenv import load_dotenv
load_dotenv()

# VectorDB ë¡œë”© â†’ rag_chain export

# === ê²½ë¡œ ì„¤ì • ===
BASE_DIR = Path(__file__).resolve().parent
VECTORDB_DIR = BASE_DIR / "law_pipeline_data" / "vectordb"
LAW_COLLECTION_NAME = "law_articles"

# === 1) ì„ë² ë”© ë¡œë”(OpenAI) ===
emb = OpenAIEmbeddings(
    model="text-embedding-3-small",
    api_key=os.getenv("OPENAI_API_KEY")
)

# === 2) VectorDB ë¡œë“œ ===
db = Chroma(
    persist_directory=str(VECTORDB_DIR),
    collection_name=LAW_COLLECTION_NAME,
    embedding_function=emb
)

# === 3) Retriever ===
retriever = db.as_retriever(search_kwargs={"k": 3})

# === 4) LLM ===
model = ChatOpenAI(
    model="gpt-4o-mini",
    openai_api_key=os.getenv("OPENAI_API_KEY")
)

# === 5) Prompt ===
prompt = PromptTemplate.from_template("""
ë²•ë ¹ ê²€ìƒ‰ ê²°ê³¼:

{context}

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ê·œì¹™:
- context ì•ˆì—ì„œë§Œ ë‹µë³€í•  ê²ƒ
- ì¶”ì¸¡, ìƒì„± ê¸ˆì§€
- ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ 'ê´€ë ¨ ë²•ë ¹ ì—†ìŒ'ì´ë¼ê³  ë§í•  ê²ƒ
""")

def docs_to_text(docs):
    return "\n\n---\n\n".join([d.page_content for d in docs])

# === 6) RAG ì²´ì¸ ===
rag_chain = (
    {
        "context": retriever | docs_to_text,
        "question": RunnablePassthrough()
    }
    | prompt
    | model
)
# ============================
# ğŸ” í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# ============================
# if __name__ == "__main__":
#     print("ğŸ” ë²•ë ¹ RAG í…ŒìŠ¤íŠ¸ ì‹œì‘")

#     query = "ì—°êµ¬ê°œë°œë¹„ì˜ ì§ì ‘ë¹„ì™€ ê°„ì ‘ë¹„ ì°¨ì´ë¥¼ ì•Œë ¤ì¤˜"
#     print("ğŸ“Œ ì§ˆë¬¸:", query)

#     try:
#         result = rag_chain.invoke(query)
#         print("\n=== RAG ì‘ë‹µ ===")
#         print(result.content)
#     except Exception as e:
#         print("âŒ ì˜¤ë¥˜ ë°œìƒ:", e)