import json
import re
import os
import traceback
from pathlib import Path

from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from collections import defaultdict

load_dotenv()

# ============================
# 1. VectorDB / LLM ì„¸íŒ…
# ============================

BASE_DIR = Path(__file__).resolve().parent
VECTORDB_DIR = BASE_DIR / "law_pipeline_data" / "vectordb"
LAW_COLLECTION_NAME = "law_articles"

emb = OpenAIEmbeddings(
    model="text-embedding-3-small",
    api_key=os.getenv("OPENAI_API_KEY"),
)

db = Chroma(
    persist_directory=str(VECTORDB_DIR),
    collection_name=LAW_COLLECTION_NAME,
    embedding_function=emb,
)

retriever = db.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5},
)

model = ChatOpenAI(
    model="gpt-4o-mini",
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    temperature=0,
)


# ============================
# 2. ìœ í‹¸ í•¨ìˆ˜ë“¤
# ============================

def docs_to_text(docs, max_chars: int = 8000) -> str:
    """
    ì—¬ëŸ¬ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ë˜,
    ì „ì²´ ê¸¸ì´ê°€ max_charsë¥¼ ë„˜ì§€ ì•Šë„ë¡ ì˜ë¼ì¤€ë‹¤.
    (ë²•ë ¹ ì»¨í…ìŠ¤íŠ¸ìš©)
    """
    chunks = []
    total = 0

    for d in docs:
        if not d or not getattr(d, "page_content", None):
            continue

        content = d.page_content
        remain = max_chars - total
        if remain <= 0:
            break

        if len(content) > remain:
            content = content[:remain]

        chunks.append(content)
        total += len(content)

    if not chunks:
        return "ê´€ë ¨ ë²•ë ¹ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    return "\n\n---\n\n".join(chunks)


def build_related_laws_from_docs(docs, max_items: int = 5, max_per_law: int = 2):
    """
    - í•œ ë²•ë ¹(law_name)ì—ì„œ ë„ˆë¬´ ë§ì€ ì¡°ë¬¸ì´ ëª°ë¦¬ì§€ ì•Šê²Œ max_per_lawë¡œ ì œí•œ.
    - ì˜ˆ: max_items=5, max_per_law=2 â†’
      ì •ë³´í†µì‹ ë§ë²• 2ê°œ + ê°œì¸ì •ë³´ë³´í˜¸ë²• 2ê°œ + SWì§„í¥ë²• 1ê°œ ì´ëŸ° ì‹ìœ¼ë¡œ diversity í™•ë³´.
    """
    related = []
    seen_articles = set()
    count_by_law = defaultdict(int)

    for d in docs or []:
        meta = getattr(d, "metadata", {}) or {}
        law_name = meta.get("law_name")
        article_title = meta.get("title") or meta.get("article_title")

        if not law_name or not article_title:
            continue

        # ê°™ì€ ë²•ë ¹ì—ì„œ ë„ˆë¬´ ë§ì´ ë½‘íˆëŠ” ê²ƒ ë°©ì§€
        if count_by_law[law_name] >= max_per_law:
            continue

        key = (law_name, article_title)
        if key in seen_articles:
            continue

        seen_articles.add(key)
        count_by_law[law_name] += 1

        snippet = (getattr(d, "page_content", "") or "")[:200]

        related.append(
            {
                "law_name": law_name,
                "article_title": article_title,
                "snippet": snippet,
                "source": "rag",
            }
        )

        if len(related) >= max_items:
            break

    return related


# ğŸ”¹ focusì— ë”°ë¼ ì¿¼ë¦¬ ê°•í™” (ì˜ˆì‚°/ì„±ê³¼/ì°¸ì—¬ì œí•œ ë“± í‚¤ì›Œë“œ íŒíŠ¸)
def build_query(text: str, focus: str | None) -> str:
    if not focus:
        return text

    extra = ""
    f = focus or ""

    if "ì˜ˆì‚°" in f or "ì—°êµ¬ê°œë°œë¹„" in f:
        extra = "ì—°êµ¬ê°œë°œë¹„, ì˜ˆì‚°, ì§ì ‘ë¹„, ê°„ì ‘ë¹„, ìë¶€ë‹´"
    elif "ì„±ê³¼ì§€í‘œ" in f or "í‰ê°€" in f or "ì„±ê³¼ê´€ë¦¬" in f:
        extra = "ì„±ê³¼ì§€í‘œ, í‰ê°€ ê¸°ì¤€, ì„±ê³¼í‰ê°€, ì„±ê³¼ê´€ë¦¬ ê´€ë ¨ ë²•ë ¹"
    elif "ìˆ˜í–‰ì²´ê³„" in f or "ì±…ì„" in f or "ì°¸ì—¬ì œí•œ" in f:
        extra = "ì°¸ì—¬ì œí•œ, ì œì¬, ì±…ì„, ì œì¬ ì¡°í•­"
    elif "ê°œì¸ì •ë³´" in f or "ë³´í˜¸" in f:
        extra = "ê°œì¸ì •ë³´ ë³´í˜¸ë²•, ê°œì¸ì •ë³´ ë³´í˜¸ë²• ì‹œí–‰ë ¹"
    # í•„ìš”í•˜ë©´ ì—¬ê¸° ë‹¤ë¥¸ focus ì¼€ì´ìŠ¤ë„ ì¶”ê°€ ê°€ëŠ¥

    if extra:
        return f"{text}\n\n[ê²€ì¦ ê´€ì ]: {focus}\n[ê´€ë ¨ í‚¤ì›Œë“œ]: {extra}"
    else:
        return f"{text}\n\n[ê²€ì¦ ê´€ì ]: {focus}"


# ============================
# 3. í”„ë¡¬í”„íŠ¸
# ============================

VERIFY_PROMPT = """
ë‹¹ì‹ ì€ ì •ë¶€ ì§€ì› ì‚¬ì—…(ì¼ë°˜íšŒê³„ ë¹„R&D ì‚¬ì—… ë° R&D ì‚¬ì—…ì„ í¬í•¨)ì˜ ë²•ë ¹ ë° ì§€ì¹¨ ì¤€ìˆ˜ ê²€í†  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ë²•ë ¹ ê²€ìƒ‰ ê²°ê³¼]
{context}

[ê²€í†  ëŒ€ìƒ í…ìŠ¤íŠ¸]
{text}

[ê²€ì¦ ê´€ì ]
{focus}

ìš”êµ¬ì‚¬í•­:
- ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ í•œêµ­ì–´ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
- JSON ë°–ì— ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ ì“°ì§€ ë§ˆì„¸ìš”.
- ë²•ë ¹ ê²€ìƒ‰ ê²°ê³¼ì— ê·¼ê±°í•˜ì§€ ì•ŠëŠ” ì¶”ì¸¡ì€ í•˜ì§€ ë§ê³ , ê·¼ê±°ê°€ ì—†ìœ¼ë©´ 'ê·¼ê±° ë¶€ì¡±'ì´ë¼ê³  ì“°ì„¸ìš”.
- related_laws í•­ëª©ì— ì ëŠ” ë²•ë ¹ëª…ê³¼ ì¡°ë¬¸ ì œëª©ì€ ë°˜ë“œì‹œ [ë²•ë ¹ ê²€ìƒ‰ ê²°ê³¼]ì— ì‹¤ì œë¡œ ë“±ì¥í•œ ê²ƒë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
  ìƒˆë¡œìš´ ë²•ë ¹ëª…ì´ë‚˜ ì¡°ë¬¸ì„ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.

ê° í•„ë“œì˜ ì˜ë¯¸:
- status: ì „ë°˜ì ì¸ íŒì • (ì í•©/ë³´ì™„/ë¶€ì í•©)
- risk_level: ë¦¬ìŠ¤í¬ ìˆ˜ì¤€ (LOW/MEDIUM/HIGH)
- reason: ì™œ ì´ëŸ° íŒì •ì´ ë‚˜ì™”ëŠ”ì§€ì— ëŒ€í•œ **ìš”ì•½ ì„¤ëª…**
- missing: ê¸°íšì„œì—ì„œ **ë¶€ì¡±í•˜ê±°ë‚˜ ë¹ ì§„ ìš”ì†Œ ëª©ë¡**
- evidence: ê¸°íšì„œì—ì„œ **ì‹¤ì œë¡œ ê·¼ê±°ê°€ ë˜ëŠ” ë¬¸ì¥** ë˜ëŠ” **ë¬¸ì œë˜ëŠ” ë¬¸ì¥**
- suggestion: ì–´ë–»ê²Œ ê³ ì¹˜ë©´ ì¢‹ì„ì§€ì— ëŒ€í•œ **êµ¬ì²´ì ì¸ ë³´ì™„ ì œì•ˆ**
- violation_judgment:
  - "NO_ISSUE" : í˜„ì¬ í…ìŠ¤íŠ¸ì—ì„œ ë²•ë ¹ ìœ„ë°˜ ë¦¬ìŠ¤í¬ê°€ ëšœë ·í•˜ê²Œ ë³´ì´ì§€ ì•ŠëŠ” ê²½ìš°
  - "POTENTIAL_VIOLATION" : íŠ¹ì • ë²•ë ¹Â·ì¡°í•­ê³¼ ì¶©ëŒí•  ê°€ëŠ¥ì„±ì´ ìˆëŠ” ê²½ìš°
  - "POSSIBLE_ISSUE" : ë°”ë¡œ ìœ„ë°˜ì´ë¼ê³  ë³´ê¸´ ì–´ë µì§€ë§Œ, í•´ì„ ë˜ëŠ” ì¶”í›„ ê²€í† ê°€ í•„ìš”í•œ ì• ë§¤í•œ ë¦¬ìŠ¤í¬ê°€ ìˆëŠ” ê²½ìš°
  - "UNCLEAR" : ë²•ë ¹ ê²€ìƒ‰ ê²°ê³¼ë‚˜ ê¸°íšì„œ ë‚´ìš©ì´ ë¶€ì¡±í•´ì„œ íŒë‹¨ì´ ì–´ë ¤ìš´ ê²½ìš°
- violation_summary: ì£¼ìš” ìœ„ë°˜/ë¦¬ìŠ¤í¬ ê°€ëŠ¥ì„±ì„ í•œ ì¤„ë¡œ ìš”ì•½
- violations: ìœ„ë°˜ ë˜ëŠ” ë¦¬ìŠ¤í¬ê°€ ìˆë‹¤ê³  íŒë‹¨í•œ ë²•ë ¹Â·ì¡°í•­ë³„ ìƒì„¸ ëª©ë¡

evidence ì‘ì„± ê·œì¹™ (ì¤‘ìš”):
- evidence í•„ë“œëŠ” ë‹¤ìŒ ë‘ ê°€ì§€ ì¤‘ í•˜ë‚˜ë§Œ í—ˆìš©í•©ë‹ˆë‹¤.
  1) [ê²€í†  ëŒ€ìƒ í…ìŠ¤íŠ¸]ì—ì„œ ê·¸ëŒ€ë¡œ ë°œì·Œí•œ í•œë‘ ë¬¸ì¥
  2) ëª…í™•í•œ ê·¼ê±°ê°€ ì—†ì„ ë•Œ: ë¬¸ìì—´ ì „ì²´ë¥¼ ì •í™•íˆ 'ê·¼ê±° ë¶€ì¡±' ë„¤ ê¸€ìë¡œë§Œ ì‘ì„±
- 'ê¸°íšì„œì—ì„œ ë¬¸ì œë¡œ ì§€ì í•œ ë¶€ë¶„(ë˜ëŠ” 'ê·¼ê±° ë¶€ì¡±')' ê°™ì€ ì„¤ëª… ë¬¸ì¥ì€ evidenceì— ì ˆëŒ€ ì“°ì§€ ë§ˆì„¸ìš”.
- focus ë¬¸ì¥(ì˜ˆ: 'ì—°êµ¬ê°œë°œë¹„Â·ì˜ˆì‚° ê´€ì ì—ì„œ ì´ ì´ˆì•ˆì„ ê²€í† í•˜ë¼.')ì„ evidenceì— ë°˜ë³µí•´ì„œ ì“°ì§€ ë§ˆì„¸ìš”.

reason, suggestion ë¬¸ì²´ ê·œì¹™:
- reason, suggestion ì€ ë³´ê³ ì„œìš© ê³µì†í•œ ë¬¸ì²´ë¡œ ì‘ì„±í•˜ê³ , ë¬¸ì¥ ëì€ ë˜ë„ë¡
  "~ì¸ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.", "~ì¸ ê²ƒìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤." ì™€ ê°™ì€ í˜•íƒœë¡œ í†µì¼í•˜ì„¸ìš”.
- ë°˜ë§ ë˜ëŠ” ëª…ì‚¬í˜• ì–´ë¯¸("~í•¨")ë¡œ ëë‚˜ëŠ” í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

JSON ìŠ¤í‚¤ë§ˆ:
{{
  "status": "ì í•©" | "ë³´ì™„" | "ë¶€ì í•©",
  "risk_level": "LOW" | "MEDIUM" | "HIGH",
  "reason": "ì „ì²´ íŒë‹¨ ê·¼ê±° ìš”ì•½",
  "missing": ["ë¶€ì¡±í•œ ìš”ì†Œ1", "ë¶€ì¡±í•œ ìš”ì†Œ2"],
  "evidence": "ê¸°íšì„œì—ì„œ ë¬¸ì œë¡œ ì§€ì í•œ ì‹¤ì œ ë¬¸ì¥ ë˜ëŠ” 'ê·¼ê±° ë¶€ì¡±'",
  "suggestion": "ì–´ë–»ê²Œ ë³´ì™„í•´ì•¼ í•˜ëŠ”ì§€ êµ¬ì²´ì ì¸ ì œì•ˆ",
  "related_laws": [
    {{
      "law_name": "ë²•ë ¹ëª…",
      "article_title": "ì¡°ë¬¸ ì œëª©",
      "snippet": "ë²•ë ¹ ë‚´ìš© ìš”ì•½ ë˜ëŠ” ê´€ë ¨ ë¶€ë¶„ ë°œì·Œ"
    }}
  ],
  "violation_judgment": "NO_ISSUE" | "POTENTIAL_VIOLATION" | "POSSIBLE_ISSUE" | "UNCLEAR",
  "violation_summary": "ë²•ë ¹ ìœ„ë°˜/ë¦¬ìŠ¤í¬ì— ëŒ€í•œ í•œ ì¤„ ìš”ì•½",
  "violations": [
    {{
      "law_name": "ë²•ë ¹ëª…",
      "article_no": "ì¡°ë¬¸ ë²ˆí˜¸ (ì˜ˆ: ì œ32ì¡°)",
      "article_title": "ì¡°ë¬¸ ì œëª©",
      "violation_type": "ì–´ë–¤ ìœ í˜•ì˜ ìœ„ë°˜/ë¦¬ìŠ¤í¬ì¸ì§€ ê°„ë‹¨í•œ ì´ë¦„",
      "severity": "LOW" | "MEDIUM" | "HIGH",
      "reason": "ì™œ ì´ ë²•ë ¹ì— ìœ„ë°°ë  ê°€ëŠ¥ì„±ì´ ìˆëŠ”ì§€",
      "recommendation": "ì–´ë–»ê²Œ ë³´ì™„í•˜ë©´ ì¢‹ì„ì§€"
    }}
  ]
}}
"""


# ============================
# 4. ë©”ì¸ í•¨ìˆ˜
# ============================

def verify_law_compliance(text: str, focus: str | None = None) -> dict:
    """
    ì´ˆì•ˆì˜ ì¼ë¶€(ì˜ˆì‚°, ìˆ˜í–‰ê³„íš ë“±)ë¥¼ ë„£ìœ¼ë©´
    ë²•ë ¹ RAG ê¸°ë°˜ 'ë²•ë ¹ì¤€ìˆ˜' JSONì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.

    - text: ê²€ì¦í•  ì´ˆì•ˆ ì„¹ì…˜ í…ìŠ¤íŠ¸
    - focus: ê²€ì‚¬ ê´€ì  (ì˜ˆ: 'ì—°êµ¬ê°œë°œë¹„', 'ê¸°ê´€ìš”ê±´')
    """
    try:
        # -----------------------------
        # 1) RAGë¡œ ê´€ë ¨ ë²•ë ¹ ê²€ìƒ‰
        # -----------------------------
        query = build_query(text, focus)

        print("query: ", query)

        try:
            docs = retriever.invoke(query)

            print("docs: ", docs)

            # ğŸ” ë””ë²„ê·¸: ì–´ë–¤ ë²•ë ¹ë“¤ì´ ê±¸ë ¸ëŠ”ì§€ í™•ì¸
            print("ğŸ” [RAG ê²°ê³¼ ìš”ì•½]")
            for i, d in enumerate(docs or []):
                meta = getattr(d, "metadata", {}) or {}
                print(
                    f"  #{i+1}: law_name={meta.get('law_name')} | "
                    f"title={meta.get('title') or meta.get('article_title')}"
                )

        except Exception as e:
            print("âŒ RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜:", e)
            traceback.print_exc()
            docs = []

        # ğŸ‘‰ RAGì—ì„œ ë°”ë¡œ ì¶”ì¶œí•œ ë²•ë ¹ ëª©ë¡ (fallbackìš©, ì „ë¶€ ì‹¤ì œ ë¬¸ì„œ ê¸°ë°˜)
        source_laws = build_related_laws_from_docs(docs)

        print("source_laws: ", source_laws)

        context = docs_to_text(docs) if docs else "ê´€ë ¨ ë²•ë ¹ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        # ğŸ”¹ í…ìŠ¤íŠ¸ ì „ì²´ ì‚¬ìš© (ê¸¸ì´ ì œí•œ ì œê±°)
        text_for_prompt = text

        # -----------------------------
        # 2) í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        # -----------------------------
        prompt_text = VERIFY_PROMPT.format(
            context=context,
            text=text_for_prompt,
            focus=focus or "ë²•ë ¹ ì¤€ìˆ˜ ì „ë°˜",
        )

        # -----------------------------
        # 3) LLM í˜¸ì¶œ
        # -----------------------------
        try:
            resp = model.invoke(prompt_text)
        except Exception as e:
            print("âŒ LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜:", e)
            traceback.print_exc()
            return {
                "status": "error",
                "risk_level": "UNKNOWN",
                "reason": "LLM í˜¸ì¶œ ì‹¤íŒ¨",
                "raw": str(e),
                "related_laws": source_laws,  # ê·¸ë˜ë„ ì‹¤ì œ ê²€ìƒ‰ëœ ì¡°ë¬¸ì€ í•¨ê»˜ ë‚´ë ¤ì¤Œ
            }

        raw = resp.content or ""

        # -----------------------------
        # 4) ```json ... ``` ì½”ë“œë¸”ë¡ ê»ë°ê¸° ì œê±°
        # -----------------------------
        raw_clean = raw.strip()

        if raw_clean.startswith("```"):
            # ì²« ì¤„ì˜ ``` / ```json ì œê±°
            raw_clean = re.sub(r"^```[a-zA-Z0-9]*\s*", "", raw_clean)
            # ëë¶€ë¶„ ``` ì˜ë¼ë‚´ê¸°
            if raw_clean.endswith("```"):
                raw_clean = raw_clean[: raw_clean.rfind("```")].strip()

        # -----------------------------
        # 5) JSON íŒŒì‹±
        # -----------------------------
        try:
            parsed = json.loads(raw_clean)
        except Exception:
            print("âŒ JSON íŒŒì‹± ì‹¤íŒ¨, raw ì‘ë‹µ:", raw_clean[:500])
            traceback.print_exc()
            return {
                "status": "error",
                "risk_level": "UNKNOWN",
                "reason": "LLM JSON íŒŒì‹± ì‹¤íŒ¨",
                "raw": raw,
                "related_laws": source_laws,  # ì—¬ê¸°ì„œë„ fallback
            }

        # -----------------------------
        # 6) related_laws ë³´ì • (fallback)
        # -----------------------------
        rl = parsed.get("related_laws")
        if isinstance(rl, list) and len(rl) > 0:
            # LLMì´ ì±„ì›Œì¤€ í•­ëª©ì— source í‘œì‹œ (í”„ë¡ íŠ¸ì—ì„œ êµ¬ë¶„í•˜ê³  ì‹¶ì„ ë•Œ)
            for item in rl:
                if isinstance(item, dict) and "source" not in item:
                    item["source"] = "llm"
        else:
            # LLMì´ related_lawsë¥¼ ì•ˆ ì±„ì› ìœ¼ë©´, RAGì—ì„œ ê°€ì ¸ì˜¨ ì‹¤ì œ ì¡°ë¬¸ìœ¼ë¡œ ì„¸íŒ…
            parsed["related_laws"] = source_laws

        # -----------------------------
        # 7) violation_* í•„ë“œ ê¸°ë³¸ê°’ ë³´ì •
        # -----------------------------
        vj = parsed.get("violation_judgment")
        if vj not in ("NO_ISSUE", "POTENTIAL_VIOLATION", "POSSIBLE_ISSUE", "UNCLEAR"):
            parsed["violation_judgment"] = "UNCLEAR"

        if not isinstance(parsed.get("violation_summary"), str):
            parsed["violation_summary"] = ""

        vlist = parsed.get("violations")
        if not isinstance(vlist, list):
            parsed["violations"] = []
        else:
            cleaned = []
            for v in vlist:
                if not isinstance(v, dict):
                    continue
                # severity ê¸°ë³¸ê°’ ë³´ì •
                if v.get("severity") not in ("LOW", "MEDIUM", "HIGH"):
                    v["severity"] = "MEDIUM"
                cleaned.append(v)
            parsed["violations"] = cleaned

        return parsed

    except Exception as e:
        # ìµœìƒìœ„ ë°©ì–´ë§‰: ì–´ë–¤ ì´ìœ ë¡œë“  ì—¬ê¸°ê¹Œì§€ ì˜¤ë©´ dictë¡œ error ìƒíƒœ ë¦¬í„´
        print("âŒ verify_law_compliance ì „ì²´ì—ì„œ ì˜ˆì™¸ ë°œìƒ:", e)
        traceback.print_exc()
        return {
            "status": "error",
            "risk_level": "UNKNOWN",
            "reason": "verify_law_compliance ë‚´ë¶€ ì˜ˆì™¸ ë°œìƒ",
            "raw": str(e),
        }


# if __name__ == "__main__":
#     from pprint import pprint

#     text = "ì—°êµ¬ê°œë°œë¹„ì—ì„œ ê°„ì ‘ë¹„ì™€ ì§ì ‘ë¹„ë¥¼ ì–´ë–»ê²Œ êµ¬ë¶„í•´ì„œ í¸ì„±í•´ì•¼ í•˜ëŠ”ì§€ ì„¤ëª…í•˜ëŠ” ë¬¸ë‹¨"
#     result = verify_law_compliance(text, focus="ì—°êµ¬ê°œë°œë¹„")

#     pprint(result, width=120, compact=True)
