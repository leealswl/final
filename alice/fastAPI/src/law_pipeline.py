import os
import json
import zipfile
import xml.etree.ElementTree as ET
from pathlib import Path
import re

from docx import Document
import numpy as np
import chromadb

from dotenv import load_dotenv
load_dotenv()

from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader

# RAW â†’ JSON â†’ VectorDB ìƒì„±

# ----------------------------------------------------------
# 0) ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
# ----------------------------------------------------------

BASE_DIR = Path(__file__).resolve().parent
LAW_DIR = BASE_DIR / "law_pipeline_data"

RAW_DIR = LAW_DIR / "raw"
PARSED_DIR = LAW_DIR / "parsed"
VECTORDB_DIR = LAW_DIR / "vectordb"

RAW_DIR.mkdir(parents=True, exist_ok=True)
PARSED_DIR.mkdir(parents=True, exist_ok=True)
VECTORDB_DIR.mkdir(parents=True, exist_ok=True)

LAW_COLLECTION_NAME = "law_articles"


# ----------------------------------------------------------
# 1) í…ìŠ¤íŠ¸ ì •ë¦¬ í•¨ìˆ˜
# ----------------------------------------------------------

def clean_text(text: str) -> str:
    text = text.replace("\u3000", " ").replace("\xa0", " ")
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    return "\n".join(lines)


# ----------------------------------------------------------
# 2) ì¡°ë¬¸ ë¶„ë¦¬
# ----------------------------------------------------------

def split_articles(text: str, law_name: str):
    pattern = r"\n(ì œ\d+ì¡°(?:\([^)]*\))?)"
    parts = re.split(pattern, text)

    articles = []
    seen_main = False
    is_addendum = False

    for i in range(1, len(parts), 2):
        title = parts[i].strip()
        content = clean_text(parts[i + 1])

        # ë¶€ì¹™ êµ¬ë¶„
        if title == "ë¶€ì¹™" or content.startswith("ë¶€ì¹™"):
            is_addendum = True
            continue

        if title.startswith("ì œ1ì¡°("):
            if not seen_main:
                seen_main = True
            else:
                is_addendum = True

        articles.append({
            "title": f"{law_name} {'[ë¶€ì¹™]' if is_addendum else ''} {title}".strip(),
            "content": content,
            "is_addendum": is_addendum
        })

    return articles


# ----------------------------------------------------------
# 3) HWPX / DOCX / PDF íŒŒì„œ
# ----------------------------------------------------------

def extract_hwpx(path: Path):
    with zipfile.ZipFile(path, "r") as z:
        xml = z.read("Contents/section0.xml")
    root = ET.fromstring(xml)
    texts = [e.text for e in root.iter() if e.text]
    return clean_text("\n".join(texts))


def extract_docx(path: Path):
    doc = Document(path)
    return clean_text("\n".join([p.text for p in doc.paragraphs]))


def extract_pdf(path: Path):
    loader = PyPDFLoader(str(path))
    docs = loader.load()
    return clean_text("\n".join(d.page_content for d in docs))


# ----------------------------------------------------------
# 4) íŒŒì¼ íŒŒì‹±
# ----------------------------------------------------------

def parse_law_file(path: Path):
    law_name = re.sub(r"\(.*?\)", "", path.stem).strip()

    if path.suffix.lower() == ".hwpx":
        text = extract_hwpx(path)
    elif path.suffix.lower() == ".docx":
        text = extract_docx(path)
    elif path.suffix.lower() == ".pdf":
        text = extract_pdf(path)
    else:
        raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹: {path}")

    articles = split_articles(text, law_name)

    return {
        "law_name": law_name,
        "articles": articles
    }


# ----------------------------------------------------------
# 5) RAW â†’ JSON ë³€í™˜
# ----------------------------------------------------------

def build_parsed_json():
    print("\nğŸ”µ RAW â†’ JSON ë³€í™˜ ì‹œì‘")

    for file in RAW_DIR.iterdir():
        if file.suffix.lower() not in [".hwpx", ".docx", ".pdf"]:
            continue

        print(f"ğŸ“˜ íŒŒì‹± ì¤‘: {file.name}")
        data = parse_law_file(file)

        json_path = PARSED_DIR / f"{file.stem}.json"
        json_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")

        print(f" â†’ ì €ì¥ë¨: {json_path}")


# ----------------------------------------------------------
# 6) JSON â†’ VectorDB
# ----------------------------------------------------------

def embed_openai(texts):
    emb = OpenAIEmbeddings(model="text-embedding-3-small")
    return emb.embed_documents(texts)


def build_vectordb():
    print("\nğŸ”µ VectorDB êµ¬ì„± ì‹œì‘")

    client = chromadb.PersistentClient(path=str(VECTORDB_DIR))

    try:
        client.delete_collection(LAW_COLLECTION_NAME)
    except:
        pass

    coll = client.create_collection(LAW_COLLECTION_NAME)

    for json_file in PARSED_DIR.glob("*.json"):
        data = json.loads(json_file.read_text(encoding="utf-8"))
        law_name = data["law_name"]

        texts = []
        ids = []
        metas = []

        for i, a in enumerate(data["articles"]):
            full = f"{a['title']}\n{a['content']}"
            texts.append(full)
            ids.append(f"{law_name}_{i}")
            metas.append({
                "law_name": law_name,
                "title": a["title"],
                "is_addendum": a["is_addendum"]
            })

        embeds = embed_openai(texts)

        coll.add(
            ids=ids,
            documents=texts,
            embeddings=embeds,
            metadatas=metas
        )

        print(f" â†’ {json_file.name}: {len(texts)} ì¡°ë¬¸ ì €ì¥")

    print("âœ… VectorDB ìƒì„± ì™„ë£Œ")


# ----------------------------------------------------------
# 7) ë©”ì¸ ì‹¤í–‰
# ----------------------------------------------------------

if __name__ == "__main__":
    print("\nğŸš€ ë²•ë ¹ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")

    build_parsed_json()
    build_vectordb()

    print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ! (parsed/ + vectordb/ ìƒì„±ë¨)")


# ë²•ë ¹ rag -> python law_pipeline.py ì‹¤í–‰í•˜ë©´ law_pipeline_data/parsed/ ì™€ vectordb/ ìƒì„±ë¨